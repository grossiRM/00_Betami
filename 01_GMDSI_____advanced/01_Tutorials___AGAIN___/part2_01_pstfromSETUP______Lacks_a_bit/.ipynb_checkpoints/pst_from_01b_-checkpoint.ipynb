{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a61b60-d7ea-4cba-95f1-b7c5e2fe7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyemu\n",
    "from ..pyemu_warnings import PyemuWarning\n",
    "import copy\n",
    "import string\n",
    "\n",
    "from pyemu.utils.helpers import _try_pdcol_numeric\n",
    "# the tolerable percent difference (100 * (max - min)/mean)\n",
    "# used when checking that constant and zone type parameters are in fact constant (within\n",
    "# a given zone)\n",
    "DIRECT_PAR_PERCENT_DIFF_TOL = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f5872-a562-4903-a0f9-2b420c20f3ed",
   "metadata": {},
   "source": [
    "# Cooking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe500b0-601a-4c29-98a5-036932550c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_datetime_from_str(sdt):\n",
    "    # could be expanded if someone is feeling clever.\n",
    "    if isinstance(sdt, str):\n",
    "        PyemuWarning(\n",
    "            \"Assuming passed reference start date time is \"\n",
    "            \"year first str {0}\".format(sdt)\n",
    "        )\n",
    "        sdt = pd.to_datetime(sdt, yearfirst=True)\n",
    "    assert isinstance(sdt, pd.Timestamp), \"Error interpreting start_datetime\"\n",
    "    return sdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e456fb-5e86-4fda-b2e2-bc6e2fa43a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_var_len(var, n, fill=None):\n",
    "    if not isinstance(var, list):\n",
    "        var = [var]\n",
    "    if fill is not None:\n",
    "        if fill == \"first\":\n",
    "            fill = var[0]\n",
    "        elif fill == \"last\":\n",
    "            fill = var[-1]\n",
    "    nv = len(var)\n",
    "    if nv < n:\n",
    "        var.extend([fill for _ in range(n - nv)])\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578699d8-58b4-4e53-a8b3-a352c5cfd3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_array_get_fmt(fname, sep=None, fullfile=False, logger=None):\n",
    "    splitsep = sep  # sep for splitting string for fmt (need to count mult delim.)\n",
    "    if sep is None:  # need to split line with space and count multiple\n",
    "        splitsep = ' '\n",
    "    with open(fname, 'r') as fp:  # load file or line\n",
    "        if fullfile:\n",
    "            lines = [line for line in fp.readlines()]\n",
    "            arr = np.genfromtxt(lines, delimiter=sep, ndmin=2)\n",
    "        else:\n",
    "            lines = [fp.readline()]  # just read first line\n",
    "            if splitsep not in lines[0]:\n",
    "                return _load_array_get_fmt(fname, sep, True)\n",
    "            fp.seek(0)  # reset pointer\n",
    "            arr = np.loadtxt(fp, delimiter=sep, ndmin=2)  # read array\n",
    "    n = 0  # counter for repeat delim when sep is None\n",
    "    lens, prec = [], []  # container for fmt length and precision\n",
    "    exps = 0  # exponential counter (could be bool)\n",
    "    for s in lines:  # loop over loaded lines\n",
    "        ilens, iprec = [], []\n",
    "        for ss in s.split(splitsep):\n",
    "            ss = ss.strip('\\n').lower()\n",
    "            if not ss:\n",
    "                n += 1  # count 1 char\n",
    "            else:\n",
    "                d = len(ss) + n if sep is None else len(ss)\n",
    "                ilens.append(d)  # store width (plus n spaces if no sep)\n",
    "                if 'e' in ss:\n",
    "                    exps += 1  # count the number of exp notations\n",
    "                    ss = ss.split('e')[0]  # get left of exp\n",
    "                iprec.append(len(ss.split('.')[-1]))  # store n dec place\n",
    "                n = 0  # reset space counter\n",
    "        lens.append(ilens)\n",
    "        prec.append(iprec)\n",
    "    N = np.sum(~np.isnan(arr[:len(lines)]))\n",
    "    # try to catch if file is infact fixed format (like old mt3d files)\n",
    "    firsts = np.ravel([line.pop(0) for line in lens])  # first entry on each line\n",
    "    rest = np.array(lens).ravel()  # the len of the rest of the entries\n",
    "    # if input file is fake free-format (actually fixed) then:\n",
    "    #  0. sep will be None,\n",
    "    #  1. all entries will be the same length, and\n",
    "    #  2. the first entry will \"look\" 1 char longer.\n",
    "    #     -- the rest will lose their fake delimiting space when we split above.\n",
    "    # if sci notation (exps>0) then precision needs to be width-8 to account for\n",
    "    #  space + sign + unit + dec + 4(for exp)\n",
    "    # if float format -- we can't know the width.precision relationship that\n",
    "    # will leave us with enough space for space and sign. precision needs to be\n",
    "    # max width-3 but this wont allow for any growth of the LHS for float.\n",
    "    fmax = max(firsts)  # max of first column\n",
    "    rmax = max(rest) if len(rest) > 0 else 0  # max of rest of cols\n",
    "    width = max([fmax, rmax])  # max len to max of these\n",
    "    prec = max(np.array(prec).ravel())  # max decimal places\n",
    "\n",
    "    if sep is None and all(firsts == firsts[0]) and all(rest == firsts[0]-1):\n",
    "        # this is the situation where we think the input file is fixed format\n",
    "        # so we need to try and match that and keep the width consistent with\n",
    "        # input\n",
    "        width = width - 1  # will be manually prepreding a delim so need to sub one here\n",
    "        if exps < N:  # Not all values are exp format\n",
    "            msg = (\"\\n_load_array_get_fmt(), likely fixed format file:\\n\"\n",
    "                   f\"{Path(fname).name} appears to contain some float notation (%F) values.\\n\"\n",
    "                   \"Can't define a generic format specifier that\\n\"\n",
    "                   \"will guarantee file is readable by downstream engine.\\n\"\n",
    "                   \"Will try to use %E, but this could cause issues\\n\"\n",
    "                   \"downstream...\")\n",
    "            if logger is not None:\n",
    "                logger.warn(msg)\n",
    "            else:\n",
    "                PyemuWarning(msg)\n",
    "        # force E:\n",
    "        if exps > 0:  # if read file contains some exp notation use all\n",
    "            typ = \"E\"\n",
    "            wlim = 7  # minimum width,\n",
    "        else:\n",
    "            typ = \"F\"\n",
    "            wlim = 3\n",
    "        precmax = width - wlim  # maximum precision supported with width\n",
    "        if width < wlim:\n",
    "            # If width is too narrow we really shouldn't proceed.\n",
    "            # This should catch where max precision would make it negative.\n",
    "            raise ValueError(\"\\n_load_array_get_fmt(), likely fixed format file:\\n\"\n",
    "                             f\"notation type %{typ} but values fields are not\\n\"\n",
    "                             f\"wide enough ({width}, min width: {wlim}),\\n\"\n",
    "                             f\"file: {Path(fname).name}\")\n",
    "        if precmax < 4:\n",
    "            # If the max allowable precision is low -- better warn\n",
    "            msg = (\n",
    "                f\"\\n_load_array_get_fmt(), likely fixed format file:\\n\"\n",
    "                \"the maximum precision that be safely supported for notation\\n\"\n",
    "                f\"type (%{typ}) with width {width} is low ({precmax}<4).\\n\"\n",
    "                \"You may want to consider the formatting of your\\n\"\n",
    "                f\"input file: {Path(fname).name}.\")\n",
    "            if logger is not None:\n",
    "                logger.warn(msg)\n",
    "            else:\n",
    "                PyemuWarning(msg)\n",
    "        if prec > precmax:\n",
    "            # If the extracted precision is larger than the max allowable.\n",
    "            msg = (\n",
    "                \"\\n_load_array_get_fmt(), likely fixed format file:\\n\"\n",
    "                f\"{Path(fname).name} interpreted data value width ({width}) and precision\\n\"\n",
    "                f\"({prec}) cannot be satisfied for format type %{typ}.\\n\"\n",
    "                f\"Reducing precision term to {precmax}.\"\n",
    "            )\n",
    "            if logger is not None:\n",
    "                logger.warn(msg)\n",
    "            else:\n",
    "                PyemuWarning(msg)\n",
    "            prec = precmax\n",
    "        fmt = f\"%{width}.{prec}{typ}\"\n",
    "        # to allow us to override the delimiter in this fixed format case\n",
    "        fmt = ' ' + ' '.join([fmt]*arr.shape[1])\n",
    "    else:\n",
    "        # regular situation where file is def free format\n",
    "        if exps == N:\n",
    "            typ = \"E\"\n",
    "        elif exps > 0:\n",
    "            typ = \"G\"\n",
    "        else:\n",
    "            typ = \"F\"\n",
    "        fmt = f\"%{width}.{prec}{typ}\"\n",
    "    return arr, fmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ddb4cc-2f28-4e41-82a4-226c478f6ca5",
   "metadata": {},
   "source": [
    "# Construct high-dimensional PEST(++) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f3039-70c8-42b0-a946-f0eb8d39b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PstFrom(object):\n",
    "    \"\"\"construct high-dimensional PEST(++) interfaces with all the bells and whistles\n",
    "\n",
    "    Args:\n",
    "        original_d (`str` or Path): the path to a complete set of model input and output files\n",
    "        new_d (`str` or Path): the path to where the model files and PEST interface files will be copied/built\n",
    "        longnames (`bool`): flag to use longer-than-PEST-likes parameter and observation names.  Default is True\n",
    "        remove_existing (`bool`): flag to destroy any existing files and folders in `new_d`.  Default is False\n",
    "        spatial_reference (varies): an object that faciliates geo-locating model cells based on index.  Default is None\n",
    "        zero_based (`bool`): flag if the model uses zero-based indices, Default is True\n",
    "        start_datetime (`str` or Timestamp): a string that can be case to a datatime instance the represents the starting datetime\n",
    "            of the model\n",
    "        tpl_subfolder (`str`): option to write template files to a subfolder within ``new_d``.\n",
    "            Default is False (write template files to ``new_d``).\n",
    "\n",
    "        chunk_len (`int`): the size of each \"chunk\" of files to spawn a multiprocessing.Pool member to process.\n",
    "            On windows, beware setting this much smaller than 50 because of the overhead associated with\n",
    "            spawning the pool.  This value is added to the call to `apply_list_and_array_pars`. Default is 50\n",
    "        echo (`bool`): flag to echo logger messages to the screen.  Default is True\n",
    "\n",
    "    Note:\n",
    "        This is the way...\n",
    "\n",
    "    Example::\n",
    "\n",
    "        pf = PstFrom(\"path_to_model_files\",\"new_dir_with_pest_stuff\",start_datetime=\"1-1-2020\")\n",
    "        pf.add_parameters(\"hk.dat\")\n",
    "        pf.add_observations(\"heads.csv\")\n",
    "        pf.build_pst(\"pest.pst\")\n",
    "        pe = pf.draw(100)\n",
    "        pe.to_csv(\"prior.csv\")\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edad113-395f-41ce-a8c4-d8f6a643c1b7",
   "metadata": {},
   "source": [
    " # FIRST EXAMPLE:_____ self.insfile_obsmap  _____*.INS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861ec3b-d51f-450b-a854-931ed77e7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(\n",
    "        self,\n",
    "        original_d,\n",
    "        new_d,\n",
    "        longnames=True,\n",
    "        remove_existing=False,\n",
    "        spatial_reference=None,\n",
    "        zero_based=True,\n",
    "        start_datetime=None,\n",
    "        tpl_subfolder=None,\n",
    "        chunk_len=50,\n",
    "        echo=True,\n",
    "    ):\n",
    "        self.original_d = Path(original_d)\n",
    "        self.new_d = Path(new_d)\n",
    "        self.original_file_d = None\n",
    "        self.mult_file_d = None\n",
    "        self.tpl_d = self.new_d\n",
    "        if tpl_subfolder is not None:\n",
    "            self.tpl_d = Path(self.new_d, tpl_subfolder)\n",
    "        self.remove_existing = bool(remove_existing)\n",
    "        self.zero_based = bool(zero_based)\n",
    "        self._spatial_reference = spatial_reference\n",
    "        self._spatial_ref_xarray = None\n",
    "        self._spatial_ref_yarray = None\n",
    "        self.spatial_reference = None\n",
    "        if start_datetime is not None:\n",
    "            start_datetime = _get_datetime_from_str(start_datetime)\n",
    "        self.start_datetime = start_datetime\n",
    "        self.geostruct = None\n",
    "        self.par_struct_dict = {}\n",
    "        # self.par_struct_dict_l = {}\n",
    "\n",
    "        self.mult_files = []\n",
    "        self.org_files = []\n",
    "\n",
    "        self.par_dfs = []\n",
    "        self.unique_parnmes = set()  # set of unique parameters added so far through add_parameters method\n",
    "        self.obs_dfs = []\n",
    "        self.py_run_file = \"forward_run.py\"\n",
    "        self.mod_command = \"python {0}\".format(self.py_run_file)\n",
    "        self.pre_py_cmds = []\n",
    "        self.pre_sys_cmds = []  # a list of preprocessing commands to add to\n",
    "        # the forward_run.py script commands are executed with os.system()\n",
    "        # within forward_run.py.\n",
    "        self.mod_py_cmds = []\n",
    "        self.mod_sys_cmds = []\n",
    "        self.post_py_cmds = []\n",
    "        self.post_sys_cmds = []  # a list of post-processing commands to add to\n",
    "        # the forward_run.py script. Commands are executed with os.system()\n",
    "        # within forward_run.py.\n",
    "        self.extra_py_imports = []\n",
    "        self.tmp_files = []\n",
    "\n",
    "        self.tpl_filenames, self.input_filenames = [], []\n",
    "        self.ins_filenames, self.output_filenames = [], []\n",
    "        self.insfile_obsmap = {}\n",
    "\n",
    "        self.longnames = bool(longnames)\n",
    "        self.logger = pyemu.Logger(\"PstFrom.log\", echo=echo)\n",
    "\n",
    "        self.logger.statement(\"starting PstFrom process\")\n",
    "\n",
    "        self._prefix_count = {}\n",
    "\n",
    "        self.get_xy = None\n",
    "        self.add_pars_callcount = 0\n",
    "        self.ijwarned = {}\n",
    "        self.initialize_spatial_reference()\n",
    "\n",
    "        self._setup_dirs()\n",
    "        self._parfile_relations = []\n",
    "        self._pp_facs = {}\n",
    "        self.pst = None\n",
    "        self._function_lines_list = []  # each function is itself a list of lines\n",
    "        self.direct_org_files = []\n",
    "        self.ult_ubound_fill = 1.0e30\n",
    "        self.ult_lbound_fill = -1.0e30\n",
    "        self.chunk_len = int(chunk_len)\n",
    "        self.py_functions = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda6f5f-e28f-4c76-bcda-f9301c2b0583",
   "metadata": {},
   "source": [
    "# A container of parameter file information.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff758f55-f22a-40a5-8b07-b6f45b4a78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @property\n",
    "    def parfile_relations(self):\n",
    "        \"\"\"build up a container of parameter file information.  Called\n",
    "        programmatically...\"\"\"\n",
    "        if isinstance(self._parfile_relations, list):\n",
    "            pr = pd.concat(self._parfile_relations, ignore_index=True)\n",
    "        else:\n",
    "            pr = self._parfile_relations\n",
    "        # quick checker\n",
    "        for name, g in pr.groupby(\"model_file\"):\n",
    "            if g.sep.nunique() > 1:\n",
    "                self.logger.warn(\n",
    "                    \"separator mismatch for {0}, seps passed {1}\"\n",
    "                    \"\".format(name, [s for s in g.sep.unique()])\n",
    "                )\n",
    "            if g.fmt.nunique() > 1:\n",
    "                self.logger.warn(\n",
    "                    \"format mismatch for {0}, fmt passed {1}\"\n",
    "                    \"\".format(name, [f for f in g.fmt.unique()])\n",
    "                )\n",
    "            # if ultimate parameter bounds have been set for only one instance\n",
    "            # of the model file we need to pass this through to all\n",
    "            ubound = g.apply(\n",
    "                lambda x: pd.Series(\n",
    "                    {\n",
    "                        k: v\n",
    "                        for n, c in enumerate(x.use_cols)\n",
    "                        for k, v in [[\"ubound{0}\".format(c), x.upper_bound[n]]]\n",
    "                    }\n",
    "                )\n",
    "                if x.use_cols is not None\n",
    "                else pd.Series({k: v for k, v in [[\"ubound\", x.upper_bound]]}),\n",
    "                axis=1,\n",
    "            )\n",
    "            if ubound.nunique(0, False).gt(1).any():\n",
    "                ub_min = ubound.min().fillna(self.ult_ubound_fill).to_dict()\n",
    "                pr.loc[g.index, \"upper_bound\"] = g.use_cols.apply(\n",
    "                    lambda x: [ub_min[\"ubound{0}\".format(c)] for c in x]\n",
    "                    if x is not None\n",
    "                    else ub_min[\"ubound\"]\n",
    "                )\n",
    "            # repeat for lower bounds\n",
    "            lbound = g.apply(\n",
    "                lambda x: pd.Series(\n",
    "                    {\n",
    "                        k: v\n",
    "                        for n, c in enumerate(x.use_cols)\n",
    "                        for k, v in [[\"lbound{0}\".format(c), x.lower_bound[n]]]\n",
    "                    }\n",
    "                )\n",
    "                if x.use_cols is not None\n",
    "                else pd.Series({k: v for k, v in [[\"lbound\", x.lower_bound]]}),\n",
    "                axis=1,\n",
    "            )\n",
    "            if lbound.nunique(0, False).gt(1).any():\n",
    "                lb_max = lbound.max().fillna(self.ult_lbound_fill).to_dict()\n",
    "                pr.loc[g.index, \"lower_bound\"] = g.use_cols.apply(\n",
    "                    lambda x: [lb_max[\"lbound{0}\".format(c)] for c in x]\n",
    "                    if x is not None\n",
    "                    else lb_max[\"lbound\"]\n",
    "                )\n",
    "        pr[\"zero_based\"] = self.zero_based   # todo -- chase this out if going to file specific zero based def\n",
    "        return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b4015-987e-4585-a54e-639e5f8d01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _generic_get_xy(self, args, **kwargs):\n",
    "        i, j = self.parse_kij_args(args, kwargs)\n",
    "        return i, j\n",
    "\n",
    "    def _dict_get_xy(self, arg, **kwargs):\n",
    "        if isinstance(arg, list):\n",
    "            arg = tuple(arg)\n",
    "        xy = self._spatial_reference.get(arg, None)\n",
    "        if xy is None:\n",
    "            arg_len = None\n",
    "            try:\n",
    "                arg_len = len(arg)\n",
    "            except Exception as e:\n",
    "                self.logger.lraise(\n",
    "                    \"Pstfrom._dict_get_xy() error getting xy from arg:'{0}' - no len support\".format(\n",
    "                        arg\n",
    "                    )\n",
    "                )\n",
    "            if arg_len == 1:\n",
    "                xy = self._spatial_reference.get(arg[0], None)\n",
    "            elif arg_len == 2 and arg[0] == 0:\n",
    "                xy = self._spatial_reference.get(arg[1], None)\n",
    "            elif arg_len == 2 and arg[1] == 0:\n",
    "                xy = self._spatial_reference.get(arg[0], None)\n",
    "            else:\n",
    "                self.logger.lraise(\n",
    "                    \"Pstfrom._dict_get_xy() error getting xy from arg:'{0}'\".format(arg)\n",
    "                )\n",
    "        if xy is None:\n",
    "            self.logger.lraise(\n",
    "                \"Pstfrom._dict_get_xy() error getting xy from arg:'{0}' - still None\".format(\n",
    "                    arg\n",
    "                )\n",
    "            )\n",
    "        return xy[0], xy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9fa8d5-92ed-4272-bd1f-69e4cd80e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _flopy_sr_get_xy(self, args, **kwargs):\n",
    "        i, j = self.parse_kij_args(args, kwargs)\n",
    "        if all([ij is None for ij in [i, j]]):\n",
    "            return i, j\n",
    "        else:\n",
    "            if (hasattr(self._spatial_reference, \"grid_type\") and\n",
    "                    self._spatial_reference.grid_type == 'vertex'):\n",
    "                return (\n",
    "                    self._spatial_reference.xcentergrid[i, ],\n",
    "                    self._spatial_reference.ycentergrid[i, ],\n",
    "                )\n",
    "            else:\n",
    "                return (\n",
    "                    self._spatial_reference.xcentergrid[i, j],\n",
    "                    self._spatial_reference.ycentergrid[i, j],\n",
    "                )\n",
    "\n",
    "    def _flopy_mg_get_xy(self, args, **kwargs):\n",
    "        i, j = self.parse_kij_args(args, kwargs)\n",
    "        if all([ij is None for ij in [i, j]]):\n",
    "            return i, j\n",
    "        else:\n",
    "            if self._spatial_ref_xarray is None:\n",
    "                self._spatial_ref_xarray = self._spatial_reference.xcellcenters\n",
    "                self._spatial_ref_yarray = self._spatial_reference.ycellcenters\n",
    "            if (hasattr(self._spatial_reference, \"grid_type\") and\n",
    "                    self._spatial_reference.grid_type == 'vertex'):\n",
    "                return (self._spatial_ref_xarray[i, ],\n",
    "                        self._spatial_ref_yarray[i, ])\n",
    "\n",
    "            return (self._spatial_ref_xarray[i, j],\n",
    "                    self._spatial_ref_yarray[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098641db-f11d-430b-ab97-6020cb7fcbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def parse_kij_args(self, args, kwargs):\n",
    "        \"\"\"parse args into kij indices.  Called programmatically\"\"\"\n",
    "        if len(args) >= 2:\n",
    "            ij_id = None\n",
    "            if \"ij_id\" in kwargs:\n",
    "                ij_id = kwargs[\"ij_id\"]\n",
    "            if ij_id is not None:\n",
    "                i, j = [args[ij] for ij in ij_id]\n",
    "            else:\n",
    "                if not self.ijwarned[self.add_pars_callcount]:\n",
    "                    self.logger.warn(\n",
    "                        (\n",
    "                            \"get_xy() warning: position of i and j in index_cols \"\n",
    "                            \"not specified, assume (i,j) are final two entries in \"\n",
    "                            \"index_cols.\"\n",
    "                        )\n",
    "                    )\n",
    "                    self.ijwarned[self.add_pars_callcount] = True\n",
    "                # assume i and j are the final two entries in index_cols\n",
    "                i, j = args[-2], args[-1]\n",
    "\n",
    "                # vertex/list based i == cell number\n",
    "                if (hasattr(self._spatial_reference, \"grid_type\") and\n",
    "                    self._spatial_reference.grid_type == 'vertex'):\n",
    "                    i, l = args[-1], args[-2]\n",
    "\n",
    "        else:\n",
    "            if not self.ijwarned[self.add_pars_callcount]:\n",
    "                self.logger.warn(\n",
    "                    (\n",
    "                        \"get_xy() warning: need locational information \"\n",
    "                        \"(e.g. i,j) to generate xy, \"\n",
    "                        \"insufficient index cols passed to interpret: {}.\"\n",
    "                        \"i,j will be set to (None,None)\"\n",
    "                    ).format(str(args))\n",
    "                )\n",
    "                self.ijwarned[self.add_pars_callcount] = True\n",
    "            i, j = None, None\n",
    "        return i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a512ea4-32fd-405c-9d7b-e794f5d6bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def initialize_spatial_reference(self):\n",
    "        \"\"\"process the spatial reference argument.  Called programmatically\"\"\"\n",
    "        if self._spatial_reference is None:\n",
    "            self.get_xy = self._generic_get_xy\n",
    "        elif hasattr(self._spatial_reference, \"xcentergrid\") and hasattr(\n",
    "            self._spatial_reference, \"ycentergrid\"\n",
    "        ):\n",
    "            self.get_xy = self._flopy_sr_get_xy\n",
    "        elif hasattr(self._spatial_reference, \"xcellcenters\") and hasattr(\n",
    "            self._spatial_reference, \"ycellcenters\"\n",
    "        ):\n",
    "            # support modelgrid style cell locs\n",
    "            self._spatial_reference.xcentergrid = self._spatial_reference.xcellcenters\n",
    "            self._spatial_reference.ycentergrid = self._spatial_reference.ycellcenters\n",
    "            self.get_xy = self._flopy_mg_get_xy\n",
    "        elif isinstance(self._spatial_reference, dict):\n",
    "            self.logger.statement(\"dictionary-based spatial reference detected...\")\n",
    "            self.get_xy = self._dict_get_xy\n",
    "        else:\n",
    "            self.logger.lraise(\n",
    "                \"initialize_spatial_reference() error: \" \"unsupported spatial_reference\"\n",
    "            )\n",
    "        self.spatial_reference = self._spatial_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e0d89-4585-4489-8d48-99b7a4c05995",
   "metadata": {},
   "source": [
    "# Write the forward run script.  _______________ build_pst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd86e823-777a-4a75-b0e8-18feacad8c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def write_forward_run(self):\n",
    "        \"\"\"write the forward run script.  Called by build_pst()\"\"\"\n",
    "        # update python commands with system style commands\n",
    "        for alist, ilist in zip(\n",
    "            [self.pre_py_cmds, self.mod_py_cmds, self.post_py_cmds],\n",
    "            [self.pre_sys_cmds, self.mod_sys_cmds, self.post_sys_cmds],\n",
    "        ):\n",
    "            if ilist is None:\n",
    "                continue\n",
    "\n",
    "            if not isinstance(ilist, list):\n",
    "                ilist = [ilist]\n",
    "            for cmd in ilist:\n",
    "                new_sys_cmd = \"pyemu.os_utils.run(r'{0}')\\n\".format(cmd)\n",
    "                if new_sys_cmd in alist:\n",
    "                    self.logger.warn(\n",
    "                        \"sys_cmd '{0}' already in sys cmds, skipping...\".format(\n",
    "                            new_sys_cmd\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    self.logger.statement(\"forward_run line:{0}\".format(new_sys_cmd))\n",
    "                    alist.append(new_sys_cmd)\n",
    "\n",
    "        with open(self.new_d / self.py_run_file, \"w\") as f:\n",
    "            f.write(\n",
    "                \"import os\\nimport multiprocessing as mp\\nimport numpy as np\"\n",
    "                + \"\\nimport pandas as pd\\n\"\n",
    "            )\n",
    "            f.write(\"import pyemu\\n\")\n",
    "            for ex_imp in self.extra_py_imports:\n",
    "                f.write(\"import {0}\\n\".format(ex_imp))\n",
    "\n",
    "            for func_lines in self._function_lines_list:\n",
    "                f.write(\"\\n\")\n",
    "                f.write(\"# function added thru PstFrom.add_py_function()\\n\")\n",
    "                for func_line in func_lines:\n",
    "                    f.write(func_line)\n",
    "                f.write(\"\\n\")\n",
    "            f.write(\"def main():\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            s = \"    \"\n",
    "            for tmp_file in self.tmp_files:\n",
    "                f.write(s + \"try:\\n\")\n",
    "                f.write(s + \"   os.remove(r'{0}')\\n\".format(tmp_file))\n",
    "                f.write(s + \"except Exception as e:\\n\")\n",
    "                f.write(\n",
    "                    s + \"   print(r'error removing tmp file:{0}')\\n\".format(tmp_file)\n",
    "                )\n",
    "            for line in self.pre_py_cmds:\n",
    "                f.write(s + line + \"\\n\")\n",
    "            for line in self.mod_py_cmds:\n",
    "                f.write(s + line + \"\\n\")\n",
    "            for line in self.post_py_cmds:\n",
    "                f.write(s + line + \"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"if __name__ == '__main__':\\n\")\n",
    "            f.write(\"    mp.freeze_support()\\n    main()\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47a0ac-2192-430e-8308-8cebb7af2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _pivot_par_struct_dict(self):\n",
    "        struct_dict = {}\n",
    "        for gs, gps in self.par_struct_dict.items():\n",
    "            par_dfs = []\n",
    "            for _, l in gps.items():\n",
    "                df = pd.concat(l)\n",
    "                if \"timedelta\" in df.columns:\n",
    "                    df[\"y\"] = 0  #\n",
    "                    df[\"x\"] = df.timedelta.dt.days\n",
    "                par_dfs.append(df)\n",
    "            struct_dict[gs] = par_dfs\n",
    "        return struct_dict\n",
    "\n",
    "    def _rename_par_struct_dict(self, mapdict):\n",
    "        for gs, gps in self.par_struct_dict.items():\n",
    "            df_dict = {}\n",
    "            for k, li in gps.items():\n",
    "                tdf = []\n",
    "                for df in li:\n",
    "                    df['parnme'] = df.parnme.apply(lambda x: mapdict.get(x, x))\n",
    "                    df = df.set_index('parnme', drop=False)\n",
    "                    tdf.append(df)\n",
    "                df_dict[k] = tdf\n",
    "            self.par_struct_dict[gs] = df_dict\n",
    "\n",
    "    def build_prior(\n",
    "        self, fmt=\"ascii\", filename=None, droptol=None, chunk=None, sigma_range=6\n",
    "    ):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c49221-dc23-48cc-b604-f1071995d884",
   "metadata": {},
   "source": [
    " # Build the prior parameter covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e623742-a1da-496d-93e6-9c5d5c3b8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"Build the prior parameter covariance matrix\n",
    "\n",
    "        Args:\n",
    "            fmt (`str`): the file format to save to.  Default is \"ASCII\", can be \"binary\", \"coo\", or \"none\"\n",
    "            filename (`str`): the filename to save the cov to\n",
    "            droptol (`float`): absolute value of prior cov entries that are smaller than `droptol` are treated as\n",
    "                zero.\n",
    "            chunk (`int`): number of entries to write to binary/coo at once.  Default is None (write all elements at once\n",
    "            sigma_range (`int`): number of standard deviations represented by parameter bounds.  Default is 6 (99%\n",
    "                confidence).  4 would be approximately 95% confidence bounds\n",
    "\n",
    "        Returns:\n",
    "            `pyemu.Cov`: the prior parameter covariance matrix\n",
    "\n",
    "        Note:\n",
    "            This method processes parameters by group names\n",
    "\n",
    "            For really large numbers of parameters (>30K), this method\n",
    "            will cause memory errors.  Luckily, in most cases, users\n",
    "            only want this matrix to generate a prior parameter ensemble\n",
    "            and the `PstFrom.draw()` is a better choice...\n",
    "\n",
    "        \"\"\"\n",
    "        struct_dict = self._pivot_par_struct_dict()\n",
    "        self.logger.log(\"building prior covariance matrix\")\n",
    "        if len(struct_dict) > 0:\n",
    "            cov = pyemu.helpers.geostatistical_prior_builder(\n",
    "                self.pst, struct_dict=struct_dict, sigma_range=sigma_range\n",
    "            )\n",
    "        else:\n",
    "            cov = pyemu.Cov.from_parameter_data(self.pst, sigma_range=sigma_range)\n",
    "\n",
    "        if filename is None:\n",
    "            filename = self.pst.filename.with_suffix(\".prior.cov\")\n",
    "        if fmt != \"none\":\n",
    "            self.logger.statement(\n",
    "                \"saving prior covariance matrix to file {0}\".format(filename)\n",
    "            )\n",
    "        if fmt == \"ascii\":\n",
    "            cov.to_ascii(filename)\n",
    "        elif fmt == \"binary\":\n",
    "            cov.to_binary(filename, droptol=droptol, chunk=chunk)\n",
    "        elif fmt == \"uncfile\":\n",
    "            cov.to_uncfile(filename)\n",
    "        elif fmt == \"coo\":\n",
    "            cov.to_coo(filename, droptol=droptol, chunk=chunk)\n",
    "        self.logger.log(\"building prior covariance matrix\")\n",
    "        return cov\n",
    "\n",
    "    def draw(self, num_reals=100, sigma_range=6, use_specsim=False, scale_offset=True):\n",
    "        \"\"\"Draw a parameter ensemble from the distribution implied by the initial parameter values in the\n",
    "        control file and the prior parameter covariance matrix.\n",
    "\n",
    "        Args:\n",
    "            num_reals (`int`): the number of realizations to draw\n",
    "            sigma_range (`int`): number of standard deviations represented by parameter bounds.  Default is 6 (99%\n",
    "                confidence).  4 would be approximately 95% confidence bounds\n",
    "            use_specsim (`bool`): flag to use spectral simulation for grid-scale pars (highly recommended).\n",
    "                Default is False\n",
    "            scale_offset (`bool`): flag to apply scale and offset to parameter bounds before calculating prior variance.\n",
    "                Dfault is True.  If you are using non-default scale and/or offset and you get an exception during\n",
    "                draw, try changing this value to False.\n",
    "\n",
    "        Returns:\n",
    "            `pyemu.ParameterEnsemble`: a prior parameter ensemble\n",
    "\n",
    "        Note:\n",
    "            This method draws by parameter group\n",
    "\n",
    "            If you are using grid-style parameters, please use spectral simulation (`use_specsim=True`)\n",
    "\n",
    "        \"\"\"\n",
    "        self.logger.log(\"drawing realizations\")\n",
    "        if self.pst.npar_adj == 0:\n",
    "            self.logger.warn(\"no adjustable parameters, nothing to draw...\")\n",
    "            return\n",
    "        # precondition {geostruct:{group:df}} dict to {geostruct:[par_dfs]}\n",
    "        struct_dict = self._pivot_par_struct_dict()\n",
    "        delr = None\n",
    "        delc = None\n",
    "        try:\n",
    "            delr = self.spatial_reference.delr\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        try:\n",
    "            delc = self.spatial_reference.delc\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        # method moved to helpers\n",
    "        pe = pyemu.helpers.draw_by_group(self.pst, num_reals=num_reals, sigma_range=sigma_range,\n",
    "                                         use_specsim=use_specsim, scale_offset=scale_offset, struct_dict=struct_dict,\n",
    "                                         delr=delr, delc=delc,\n",
    "                                         logger=self.logger)\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d0150-7d08-4017-b9eb-cdb682c5e6c7",
   "metadata": {},
   "source": [
    "# Build control file from i/o files in PstFrom object. __________ return pst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a9261-5868-4af7-818b-d58bfe560abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_pst(self, filename=None, update=False, version=1):\n",
    "        \"\"\"Build control file from i/o files in PstFrom object.\n",
    "        Warning: This builds a pest control file from scratch, overwriting\n",
    "        anything already in self.pst object and anything already writen to `filename`\n",
    "\n",
    "        Args:\n",
    "            filename (`str`): the filename to save the control file to.\n",
    "                If None, the name is formed from the `PstFrom.original_d`\n",
    "                ,the orginal directory name from which the forward model\n",
    "                was extracted.  Default is None.\n",
    "                The control file is saved in the `PstFrom.new_d` directory.\n",
    "            update (`bool`) or (str): flag to add to existing Pst object and\n",
    "                rewrite. If string {'pars', 'obs'} just update respective\n",
    "                components of Pst. Default is False - build from PstFrom\n",
    "                components.\n",
    "            version (`int`): control file version to write, Default is 1.\n",
    "                If None, option to not write pst to file at pst_build() call --\n",
    "                handy when control file is huge pst object will be modified\n",
    "                again before running.\n",
    "        Note:\n",
    "            This builds a pest control file from scratch, overwriting anything already\n",
    "            in self.pst object and anything already writen to `filename`\n",
    "\n",
    "            The new pest control file is assigned an NOPTMAX value of 0\n",
    "\n",
    "        \"\"\"\n",
    "        import cProfile\n",
    "        pd.set_option('display.max_rows', 500)\n",
    "        pd.set_option('display.max_columns', 500)\n",
    "        pd.set_option('display.width', 1000)\n",
    "\n",
    "        par_data_cols = pyemu.pst_utils.pst_config[\"par_fieldnames\"]\n",
    "        obs_data_cols = pyemu.pst_utils.pst_config[\"obs_fieldnames\"]\n",
    "        if update:\n",
    "            if self.pst is None:\n",
    "                self.logger.warn(\n",
    "                    \"Can't update Pst object not initialised. \"\n",
    "                    \"Setting update to False\"\n",
    "                )\n",
    "                update = False\n",
    "            else:\n",
    "                if filename is None:\n",
    "                    filename = get_filepath(self.new_d, self.pst.filename)\n",
    "        else:\n",
    "            if filename is None:\n",
    "                filename = Path(self.new_d, self.original_d.name).with_suffix(\".pst\")\n",
    "        filename = get_filepath(self.new_d, filename)\n",
    "\n",
    "        # if os.path.dirname(filename) in [\"\", \".\"]:\n",
    "        #    filename = os.path.join(self.new_d, filename)\n",
    "\n",
    "        if update:\n",
    "            pst = self.pst\n",
    "            if update is True:\n",
    "                update = {\"pars\": False, \"obs\": False}\n",
    "            elif isinstance(update, str):\n",
    "                update = {update: True}\n",
    "            elif isinstance(update, (set, list)):\n",
    "                update = {s: True for s in update}\n",
    "            uupdate = True\n",
    "        else:\n",
    "            update = {\"pars\": False, \"obs\": False}\n",
    "            uupdate = False\n",
    "            pst = pyemu.Pst(filename, load=False)\n",
    "\n",
    "        # TODO should this be under an if:? incase updating and prior info has been set\n",
    "        pst.prior_information = pst.null_prior.merge(pd.DataFrame(\n",
    "            data=[], columns=pst.prior_fieldnames))\n",
    "\n",
    "        if \"pars\" in update.keys() or not uupdate:\n",
    "            if len(self.par_dfs) > 0:\n",
    "                # parameter data from object\n",
    "                full_par_dat = pd.concat(self.par_dfs)\n",
    "                par_data = full_par_dat.loc[:, par_data_cols]\n",
    "                # info relating parameter multiplier files to model input files\n",
    "                parfile_relations = self.parfile_relations\n",
    "                parfile_relations.to_csv(self.new_d / \"mult2model_info.csv\")\n",
    "                if not any(\n",
    "                    [\"apply_list_and_array_pars\" in s for s in self.pre_py_cmds]\n",
    "                ):\n",
    "                    self.pre_py_cmds.insert(\n",
    "                        0,\n",
    "                        \"pyemu.helpers.apply_list_and_array_pars(\"\n",
    "                        \"arr_par_file='mult2model_info.csv',chunk_len={0})\".format(\n",
    "                            self.chunk_len\n",
    "                        ),\n",
    "                    )\n",
    "            else:\n",
    "                par_data = pyemu.pst_utils._populate_dataframe(\n",
    "                    [], pst.par_fieldnames, pst.par_defaults, pst.par_dtype\n",
    "                )\n",
    "            # set parameter data\n",
    "            # some pre conditioning to support rentry here is more add_par\n",
    "            # calls are made with update/rebuild pst\n",
    "            shtmx = 0\n",
    "            gshtmx = 0\n",
    "            if pst.parameter_data is not None:\n",
    "                # copy existing par data (incase it has been edited)\n",
    "                par_data_orig = pst.parameter_data.copy()\n",
    "                if \"longname\" in par_data_orig.columns:\n",
    "                    # Support existing long names mapping\n",
    "                    par_data_orig = par_data_orig.set_index(\n",
    "                        \"longname\")\n",
    "                    # starting point for updated mapping\n",
    "                    shtmx = par_data_orig.parnme.str.strip('p').astype(int).max() + 1\n",
    "                    gshtmx = par_data_orig.pargp.str.strip('pg').astype(int).max() + 1\n",
    "                # index of new pars (might need this later)\n",
    "                new_par_data = par_data.index.difference(par_data_orig.index)\n",
    "            else:\n",
    "                new_par_data = slice(None)\n",
    "            # build or update par data\n",
    "            pst.parameter_data = pd.concat(\n",
    "                [pst.parameter_data,\n",
    "                 par_data.loc[new_par_data]], axis=0)\n",
    "            # pst.template_files = self.tpl_filenames\n",
    "            # pst.input_files = self.input_filenames\n",
    "            pst.model_input_data = pd.DataFrame(\n",
    "                {\"pest_file\": self.tpl_filenames, \"model_file\": self.input_filenames},\n",
    "                index=self.tpl_filenames,\n",
    "            )\n",
    "            # rename pars and obs in case short names are desired\n",
    "            if not self.longnames:\n",
    "                self.logger.log(\"Converting parameters to shortnames\")\n",
    "                # pull out again for shorthand access\n",
    "                par_data = pst.parameter_data\n",
    "                # new pars will not be mapped, start this mapping\n",
    "                npd = par_data.loc[new_par_data]\n",
    "                par_data.loc[npd.index, 'longname'] = npd.parnme\n",
    "                # get short names (using existing names as index starting point)\n",
    "                par_data.loc[npd.index, \"shortname\"] = [\n",
    "                    'p' + f\"{i}\" for i in range(shtmx, shtmx+len(npd))\n",
    "                ]\n",
    "                # set to dict\n",
    "                parmap = par_data.loc[npd.index, \"shortname\"].to_dict()\n",
    "                # get shortlist of pars for each tpl\n",
    "                tpl_shortlist = full_par_dat.loc[npd.index].groupby(\n",
    "                    'tpl_filename').parnme.groups\n",
    "                tpl_shortlist = {str(k): v.to_list()\n",
    "                                 for k, v in tpl_shortlist.items()}\n",
    "                # rename parnames and propagate to tpls etc.\n",
    "                self.logger.log(\"Renaming parameters for shortnames\")\n",
    "                # pr = cProfile.Profile()\n",
    "                # pr.enable()\n",
    "                pst.rename_parameters(parmap,\n",
    "                                      pst_path=self.new_d,\n",
    "                                      tplmap=tpl_shortlist)\n",
    "                # pr.disable()\n",
    "                self.logger.log(\"Renaming parameters for shortnames\")\n",
    "                # rename in struct dicts\n",
    "                self._rename_par_struct_dict(parmap)\n",
    "                # save whole shortname-longname mapping (will over write previous)\n",
    "                par_data.set_index(\"shortname\")[\"longname\"].to_csv(\n",
    "                    filename.with_name('parlongname.map'))\n",
    "                npd.index = npd.index.map(parmap)\n",
    "\n",
    "                # build group mapping df\n",
    "                pargpmap = pd.DataFrame(npd.pargp.unique(),\n",
    "                                        columns=['longname'])\n",
    "                # shortnames from using previous a starting point (if existing)\n",
    "                pargpmap[\"shortname\"] = \"pg\" + (pargpmap.index+gshtmx).astype(str)\n",
    "                pargpmap_dict = pargpmap.set_index('longname').shortname.to_dict()\n",
    "                par_data.loc[npd.index, \"pglong\"] = npd.pargp\n",
    "                par_data.loc[npd.index, 'pargp'] = npd.pargp.map(pargpmap_dict)\n",
    "                par_data.groupby('pargp').pglong.first().to_csv(\n",
    "                    filename.with_name('pglongname.map'))\n",
    "                self.logger.log(\"Converting parameters to shortnames\")\n",
    "        if \"obs\" in update.keys() or not uupdate:\n",
    "            if len(self.obs_dfs) > 0:\n",
    "                full_obs_data = pd.concat(self.obs_dfs)\n",
    "                obs_data = full_obs_data.loc[:, obs_data_cols]\n",
    "            else:\n",
    "                obs_data = pyemu.pst_utils._populate_dataframe(\n",
    "                    [], pst.obs_fieldnames, pst.obs_defaults, pst.obs_dtype\n",
    "                )\n",
    "                obs_data.loc[:, \"obsnme\"] = []\n",
    "                obs_data.index = []\n",
    "            # set observation data\n",
    "            # some pre conditioning to support rentry here is more add_obs\n",
    "            # calls are made with update/rebuild pst\n",
    "            shtmx = 0\n",
    "            gshtmx = 0\n",
    "            if pst.observation_data is not None:\n",
    "                # copy existing obs data (incase it has been edited)\n",
    "                obs_data_orig = pst.observation_data.copy()\n",
    "                if \"longname\" in obs_data_orig.columns:\n",
    "                    # Support existing long names mapping\n",
    "                    obs_data_orig = obs_data_orig.set_index(\n",
    "                        \"longname\")\n",
    "                    # starting point for updated mapping\n",
    "                    shtmx = obs_data_orig.obsnme.str.lstrip('ob').astype(int).max() + 1\n",
    "                    gshtmx = obs_data_orig.obgnme.str.lstrip('l_obg').astype(int).max() + 1\n",
    "                # index of new obs (might need this later)\n",
    "                new_obs_data = obs_data.index.difference(obs_data_orig.index)\n",
    "            else:\n",
    "                new_obs_data = slice(None)\n",
    "            # build or update obs data\n",
    "            pst.observation_data = pd.concat(\n",
    "                [pst.observation_data,\n",
    "                 obs_data.loc[new_obs_data]], axis=0)\n",
    "            # pst.instruction_files = self.ins_filenames\n",
    "            # pst.output_files = self.output_filenames\n",
    "            pst.model_output_data = pd.DataFrame(\n",
    "                {\"pest_file\": self.ins_filenames, \"model_file\": self.output_filenames},\n",
    "                index=self.ins_filenames,\n",
    "            )\n",
    "            # rename pars and obs in case short names are desired\n",
    "            if not self.longnames:\n",
    "                self.logger.log(\"Converting observations to shortnames\")\n",
    "                # pull out again for shorthand access\n",
    "                obs_data = pst.observation_data\n",
    "                # new obs will not be mapped so start this mapping\n",
    "                nod = obs_data.loc[new_obs_data]\n",
    "                obs_data.loc[nod.index, \"longname\"] = nod.obsnme\n",
    "                # get short names (using existing names as index starting point)\n",
    "                obs_data.loc[nod.index, \"shortname\"] = [\n",
    "                    'ob' + f\"{i}\" for i in range(shtmx, shtmx+len(nod))\n",
    "                ]\n",
    "                obsmap = obs_data.loc[nod.index, \"shortname\"].to_dict()\n",
    "                insmap = {str(Path(self.new_d, k)): v\n",
    "                          for k, v in self.insfile_obsmap.items()\n",
    "                          if len(nod.index.intersection(v))>0}\n",
    "                # rename obsnames and propagate to ins files\n",
    "                # pr2 = cProfile.Profile()\n",
    "                # pr2.enable()\n",
    "                self.logger.log(\"Renaming observations for shortnames\")\n",
    "                pst.rename_observations(obsmap,\n",
    "                                        pst_path=self.new_d,\n",
    "                                        insmap=insmap)\n",
    "                self.logger.log(\"Renaming observations for shortnames\")\n",
    "                # pr2.disable()\n",
    "                obs_data.set_index(\"shortname\")[\"longname\"].to_csv(\n",
    "                    filename.with_name('obslongname.map'))\n",
    "                nod.index = nod.index.map(obsmap)\n",
    "\n",
    "                # build group mapping df\n",
    "                obgpmap = pd.DataFrame(nod.obgnme.unique(),\n",
    "                                       columns=['longname'])\n",
    "                # shortnames from using previous a starting point (if existing)\n",
    "                obgpmap[\"shortname\"] = \"obg\" + (obgpmap.index+gshtmx).astype(str)\n",
    "                ltobs = obgpmap.longname.str.startswith(\n",
    "                    pyemu.pst.pst_handler.get_constraint_tags('lt')\n",
    "                )\n",
    "                obgpmap.loc[ltobs, \"shortname\"] = \"l_\" + obgpmap.loc[ltobs, \"shortname\"]\n",
    "                gtobs = obgpmap.longname.str.startswith(\n",
    "                    pyemu.pst.pst_handler.get_constraint_tags('gt')\n",
    "                )\n",
    "                obgpmap.loc[gtobs, \"shortname\"] = \"g_\" + obgpmap.loc[gtobs, \"shortname\"]\n",
    "                obgpmap_dict = obgpmap.set_index('longname').shortname.to_dict()\n",
    "                obs_data.loc[nod.index, \"oglong\"] = nod.obgnme\n",
    "                obs_data.loc[nod.index, 'obgnme'] = nod.obgnme.map(obgpmap_dict)\n",
    "                obs_data.groupby('obgnme').oglong.first().to_csv(\n",
    "                    filename.with_name('oglongname.map'))\n",
    "                self.logger.log(\"Converting observations to shortnames\")\n",
    "                # pr.disable()\n",
    "                # pr.print_stats(sort=\"cumtime\")\n",
    "            # obs_data.sort_index(inplace=True)  #TODO\n",
    "\n",
    "        if not uupdate:\n",
    "            pst.model_command = self.mod_command\n",
    "\n",
    "        pst.control_data.noptmax = 0\n",
    "        self.pst = pst\n",
    "        if version is not None:\n",
    "            self.pst.write(filename, version=version)\n",
    "        self.write_forward_run()\n",
    "        pst.try_parse_name_metadata()\n",
    "        # pr.print_stats(sort=\"cumtime\")\n",
    "        # pr2.print_stats(sort=\"cumtime\")\n",
    "        return pst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae11113-d7f9-46ef-84af-2c41d5ebd0d3",
   "metadata": {},
   "source": [
    "## setting up dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69bba0a-c614-4af6-afb6-258bf5f126d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _setup_dirs(self):\n",
    "        self.logger.log(\"setting up dirs\")\n",
    "        if not os.path.exists(self.original_d):\n",
    "            self.logger.lraise(f\"original_d '{self.original_d}' not found\")\n",
    "        if not os.path.isdir(self.original_d):\n",
    "            self.logger.lraise(f\"original_d '{self.original_d}' is not a directory\")\n",
    "        if self.new_d.exists():\n",
    "            if self.remove_existing:\n",
    "                self.logger.log(f\"removing existing new_d '{self.new_d}'\")\n",
    "                pyemu.os_utils._try_remove_existing(self.new_d)\n",
    "                self.logger.log(f\"removing existing new_d '{self.new_d}'\")\n",
    "            else:\n",
    "                self.logger.lraise(\n",
    "                    f\"new_d '{self.new_d}' already exists \" \"- use remove_existing=True\"\n",
    "                )\n",
    "\n",
    "        self.logger.log(\n",
    "            f\"copying original_d '{self.original_d}' to new_d '{self.new_d}'\"\n",
    "        )\n",
    "        pyemu.os_utils._try_copy_dir(self.original_d, self.new_d)\n",
    "        self.logger.log(\n",
    "            f\"copying original_d '{self.original_d}' to new_d '{self.new_d}'\"\n",
    "        )\n",
    "\n",
    "        self.original_file_d = self.new_d / \"org\"\n",
    "        if self.original_file_d.exists():\n",
    "            self.logger.lraise(f\"'org' subdir already exists in new_d '{self.new_d}'\")\n",
    "        self.original_file_d.mkdir(exist_ok=True)\n",
    "\n",
    "        self.mult_file_d = self.new_d / \"mult\"\n",
    "        if self.mult_file_d.exists():\n",
    "            self.logger.lraise(f\"'mult' subdir already exists in new_d '{self.new_d}'\")\n",
    "        self.mult_file_d.mkdir(exist_ok=True)\n",
    "\n",
    "        self.tpl_d.mkdir(exist_ok=True)\n",
    "\n",
    "        self.logger.log(\"setting up dirs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb540914-db65-438f-9dab-2b0fdde4ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _par_prep(\n",
    "        self,\n",
    "        filenames,\n",
    "        index_cols,\n",
    "        use_cols,\n",
    "        fmts=None,\n",
    "        seps=None,\n",
    "        skip_rows=None,\n",
    "        c_char=None,\n",
    "    ):\n",
    "\n",
    "        # todo: cast str column names, index_cols and use_cols to lower if str?\n",
    "        # todo: check that all index_cols and use_cols are the same type\n",
    "        file_dict = {}\n",
    "        fmt_dict = {}\n",
    "        sep_dict = {}\n",
    "        skip_dict = {}\n",
    "        (\n",
    "            filenames,\n",
    "            fmts,\n",
    "            seps,\n",
    "            skip_rows,\n",
    "            index_cols,\n",
    "            use_cols,\n",
    "        ) = self._prep_arg_list_lengths(\n",
    "            filenames, fmts, seps, skip_rows, index_cols, use_cols\n",
    "        )\n",
    "        storehead = None\n",
    "        if index_cols is not None:\n",
    "            for filename, sep, fmt, skip in zip(filenames, seps, fmts, skip_rows):\n",
    "                # cast to pathlib.Path instance\n",
    "                # input file path may or may not include original_d\n",
    "                # input_filepath = get_filepath(self.original_d, filename)\n",
    "                rel_filepath = get_relative_filepath(self.original_d, filename)\n",
    "                dest_filepath = self.new_d / rel_filepath\n",
    "\n",
    "                # data file in dest_ws/org/ folder\n",
    "                org_file = self.original_file_d / rel_filepath.name\n",
    "\n",
    "                self.logger.log(f\"loading list-style {dest_filepath}\")\n",
    "                df, storehead, _ = self._load_listtype_file(\n",
    "                    rel_filepath, index_cols, use_cols, fmt, sep, skip, c_char\n",
    "                )\n",
    "                # Currently just passing through comments in header (i.e. before the table data)\n",
    "                stkeys = np.array(\n",
    "                    sorted(storehead.keys())\n",
    "                )  # comments line numbers as sorted array\n",
    "                if (\n",
    "                    stkeys.size > 0 and stkeys.min() == 0\n",
    "                ):  # TODO pass comment_char through to par_file_rel so mid-table comments can be preserved\n",
    "                    skip = 1 + np.sum(np.diff(stkeys) == 1)\n",
    "                # # looping over model input filenames\n",
    "                if fmt.lower() == \"free\":\n",
    "                    if sep is None:\n",
    "                        sep = \" \"\n",
    "                        if rel_filepath.suffix.lower() == \".csv\":\n",
    "                            sep = \",\"\n",
    "                if pd.api.types.is_integer_dtype(df.columns):  # df.columns.is_integer(): # really!???\n",
    "                    hheader = False\n",
    "                else:\n",
    "                    hheader = df.columns\n",
    "\n",
    "                self.logger.statement(\n",
    "                    f\"loaded list-style '{dest_filepath}' of shape {df.shape}\"\n",
    "                )\n",
    "                # TODO BH: do we need to be careful of the format of the model\n",
    "                #  files? -- probs not necessary for the version in\n",
    "                #  original_file_d - but for the eventual product model file,\n",
    "                #  it might be format sensitive - yuck\n",
    "                # Update, BH: I think the `original files` saved can always\n",
    "                # be comma delim --they are never directly used\n",
    "                # as model inputs-- as long as we pass the required model\n",
    "                # input file format (and sep), right?\n",
    "                # write orig version of input file to `org` (e.g.) dir\n",
    "\n",
    "                # make any subfolders if they don't exist\n",
    "                # org_path = Path(self.original_file_d, rel_file_path.parent)\n",
    "                # org_path.mkdir(exist_ok=True)\n",
    "\n",
    "                if len(storehead) != 0:\n",
    "                    kwargs = {}\n",
    "                    if \"win\" in platform.platform().lower():\n",
    "                        kwargs = {\"lineterminator\": \"\\n\"}\n",
    "                    with open(org_file, \"w\") as fp:\n",
    "                        lc = 0\n",
    "                        fr = 0\n",
    "                        for key in sorted(storehead.keys()):\n",
    "                            if key > lc:\n",
    "                                self.logger.warn(\n",
    "                                    \"Detected mid-table comment \"\n",
    "                                    f\"on line {key + 1} tabular model file, \"\n",
    "                                    \"comment will be lost\"\n",
    "                                )\n",
    "                                lc += 1\n",
    "                                continue\n",
    "                                # TODO if we want to preserve mid-table comments,\n",
    "                                #  these lines might help - will also need to\n",
    "                                #  pass comment_char through so it can be\n",
    "                                #  used by the apply methods\n",
    "                                # to = key - lc\n",
    "                                # df.iloc[fr:to].to_csv(\n",
    "                                #     fp, sep=',', mode='a', header=hheader, # todo - presence of header may cause an issue with this\n",
    "                                #     **kwargs)\n",
    "                                # lc += to - fr\n",
    "                                # fr = to\n",
    "                            fp.write(storehead[key])\n",
    "                            fp.flush()\n",
    "                            lc += 1\n",
    "                        # if lc < len(df):  # finish off remaining table (todo: when supporting mid-table comments...)\n",
    "                        df.iloc[fr:].to_csv(\n",
    "                            fp,\n",
    "                            sep=\",\",\n",
    "                            mode=\"a\",\n",
    "                            header=hheader,\n",
    "                            index=False,\n",
    "                            **kwargs,\n",
    "                        )\n",
    "                else:\n",
    "                    df.to_csv(\n",
    "                        org_file,\n",
    "                        index=False,\n",
    "                        sep=\",\",\n",
    "                        header=hheader,\n",
    "                    )\n",
    "                file_dict[rel_filepath] = df.apply(_try_pdcol_numeric)  # make sure numeric (if reasonable)\n",
    "                fmt_dict[rel_filepath] = fmt\n",
    "                sep_dict[rel_filepath] = sep\n",
    "                skip_dict[rel_filepath] = skip\n",
    "                self.logger.log(f\"loading list-style {dest_filepath}\")\n",
    "\n",
    "            # check for compatibility\n",
    "            fnames = list(file_dict.keys())\n",
    "            for i in range(len(fnames)):\n",
    "                for j in range(i + 1, len(fnames)):\n",
    "                    if file_dict[fnames[i]].shape[1] != file_dict[fnames[j]].shape[1]:\n",
    "                        self.logger.lraise(\n",
    "                            f\"shape mismatch for array style, '{fnames[i]}' \"\n",
    "                            f\"shape {file_dict[fnames[i]].shape[1]} != \"\n",
    "                            f\"'{fnames[j]}' \"\n",
    "                            f\"shape {file_dict[fnames[j]].shape[1]}\"\n",
    "                        )\n",
    "        else:  # load array type files\n",
    "            # loop over model input files\n",
    "            for input_filena, sep, fmt, skip in zip(filenames, seps, fmts, skip_rows):\n",
    "                # cast to pathlib.Path instance\n",
    "                # input file path may or may not include original_d\n",
    "                input_filena = get_filepath(self.original_d, input_filena)\n",
    "                if fmt.lower() == \"free\":\n",
    "                    # cast to string to work with pathlib objects\n",
    "                    if input_filena.suffix.lower() == \".csv\":\n",
    "                        if sep is None:\n",
    "                            sep = \",\"\n",
    "                else:\n",
    "                    # TODO - or not?\n",
    "                    raise NotImplementedError(\n",
    "                        \"Only free format array par files currently supported\"\n",
    "                    )\n",
    "                # file path relative to model workspace\n",
    "                rel_filepath = input_filena.relative_to(self.original_d)\n",
    "                dest_filepath = self.new_d / rel_filepath\n",
    "                self.logger.log(f\"loading array {dest_filepath}\")\n",
    "                if not dest_filepath.exists():\n",
    "                    self.logger.lraise(f\"par filename '{dest_filepath}' not found \")\n",
    "                # read array type input file\n",
    "                arr, infmt = _load_array_get_fmt(dest_filepath, sep=sep,\n",
    "                                                 logger=self.logger)\n",
    "                # arr = np.loadtxt(dest_filepath, delimiter=sep, ndmin=2)\n",
    "                self.logger.log(f\"loading array {dest_filepath}\")\n",
    "                self.logger.statement(\n",
    "                    f\"loaded array '{input_filena}' of shape {arr.shape}\"\n",
    "                )\n",
    "                # save copy of input file to `org` dir\n",
    "                # make any subfolders if they don't exist\n",
    "                # this will be python auto precision\n",
    "                np.savetxt(self.original_file_d / rel_filepath.name, arr)\n",
    "                file_dict[rel_filepath] = arr\n",
    "                fmt_dict[rel_filepath] = infmt\n",
    "                sep_dict[rel_filepath] = sep\n",
    "                skip_dict[rel_filepath] = skip\n",
    "            # check for compatibility\n",
    "            fnames = list(file_dict.keys())\n",
    "            for i in range(len(fnames)):\n",
    "                for j in range(i + 1, len(fnames)):\n",
    "                    if file_dict[fnames[i]].shape != file_dict[fnames[j]].shape:\n",
    "                        self.logger.lraise(\n",
    "                            f\"shape mismatch for array style, '{fnames[i]}' \"\n",
    "                            f\"shape {file_dict[fnames[i]].shape[1]} != \"\n",
    "                            f\"'{fnames[j]}' \"\n",
    "                            f\"shape {file_dict[fnames[j]].shape[1]}\"\n",
    "                        )\n",
    "        return (\n",
    "            index_cols,\n",
    "            use_cols,\n",
    "            file_dict,\n",
    "            fmt_dict,\n",
    "            sep_dict,\n",
    "            skip_dict,\n",
    "            storehead,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9576da28-5682-4330-934b-ef4a7caa0dc1",
   "metadata": {},
   "source": [
    "# Forward run script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fd870-b316-4cb1-9503-35537f233aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _next_count(self, prefix):\n",
    "        if prefix not in self._prefix_count:\n",
    "            self._prefix_count[prefix] = 0\n",
    "        else:\n",
    "            self._prefix_count[prefix] += 1\n",
    "\n",
    "        return self._prefix_count[prefix]\n",
    "\n",
    "    def add_py_function(\n",
    "            self, file_name, call_str=None, is_pre_cmd=True,\n",
    "            function_name=None\n",
    "    ):\n",
    "        \"\"\"add a python function to the forward run script\n",
    "\n",
    "        Args:\n",
    "            file_name (`str`): a python source file\n",
    "            call_str (`str`): the call string for python function in\n",
    "                `file_name`.\n",
    "                `call_str` will be added to the forward run script, as is.\n",
    "            is_pre_cmd (`bool` or `None`): flag to include `call_str` in\n",
    "                PstFrom.pre_py_cmds.  If False, `call_str` is\n",
    "                added to PstFrom.post_py_cmds instead. If passed as `None`,\n",
    "                then the function `call_str` is added to the forward run\n",
    "                script but is not called.  Default is True.\n",
    "            function_name (`str`): DEPRECATED, used `call_str`\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Note:\n",
    "            `call_str` is expected to reference standalone a function\n",
    "            that contains all the imports it needs or these imports\n",
    "            should have been added to the forward run script through the\n",
    "            `PstFrom.extra_py_imports` list.\n",
    "\n",
    "            This function adds the `call_str` call to the forward\n",
    "            run script (either as a pre or post command or function not \n",
    "            directly called by main). It is up to users\n",
    "            to make sure `call_str` is a valid python function call\n",
    "            that includes the parentheses and requisite arguments\n",
    "\n",
    "            This function expects \"def \" + `function_name` to be flushed left\n",
    "            at the outer most indentation level\n",
    "\n",
    "        Example::\n",
    "\n",
    "            pf = PstFrom()\n",
    "            # add the function \"mult_well_function\" from the script file \"preprocess.py\" as a\n",
    "            # command to run before the model is run\n",
    "            pf.add_py_function(\"preprocess.py\",\n",
    "                               \"mult_well_function(arg1='userarg')\",\n",
    "                               is_pre_cmd = True)\n",
    "            # add the post processor function \"made_it_good\" from the script file \"post_processors.py\"\n",
    "            pf.add_py_function(\"post_processors.py\",\"make_it_good()\",is_pre_cmd=False)\n",
    "            # add the function \"another_func\" from the script file \"utils.py\" as a\n",
    "            # function not called by main\n",
    "            pf.add_py_function(\"utils.py\",\"another_func()\",is_pre_cmd=None)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        if function_name is not None:\n",
    "            warnings.warn(\n",
    "                \"add_py_function(): 'function_name' argument is deprecated, \"\n",
    "                \"use 'call_str' instead\",\n",
    "                DeprecationWarning,\n",
    "            )\n",
    "            if call_str is None:\n",
    "                call_str = function_name\n",
    "        if call_str is None:\n",
    "            self.logger.lraise(\n",
    "                \"add_py_function(): No function call string passed in arg \" \"'call_str'\"\n",
    "            )\n",
    "        if not os.path.exists(file_name):\n",
    "            self.logger.lraise(\n",
    "                \"add_py_function(): couldnt find python source file '{0}'\".format(\n",
    "                    file_name\n",
    "                )\n",
    "            )\n",
    "        if \"(\" not in call_str or \")\" not in call_str:\n",
    "            self.logger.lraise(\n",
    "                \"add_py_function(): call_str '{0}' missing paretheses\".format(call_str)\n",
    "            )\n",
    "        function_name = call_str[\n",
    "            : call_str.find(\"(\")\n",
    "        ]  # strip to first occurance of '('\n",
    "        if function_name in self.py_functions:\n",
    "            # todo: could add more duplication options here: override, increment\n",
    "            warnings.warn(\n",
    "                f\"add_py_function(): {function_name} already \"\n",
    "                f\"in forward run python functions, not overriding here, \"\n",
    "                f\"original will be maintained\",\n",
    "                PyemuWarning,\n",
    "            )\n",
    "        else:\n",
    "            func_lines = []\n",
    "            search_str = \"def \" + function_name + \"(\"\n",
    "            abet_set = set(string.ascii_uppercase)\n",
    "            abet_set.update(set(string.ascii_lowercase))\n",
    "            with open(file_name, \"r\") as f:\n",
    "                while True:\n",
    "                    line = f.readline()\n",
    "                    if line == \"\":\n",
    "                        self.logger.lraise(\n",
    "                            \"add_py_function(): EOF while searching for function '{0}'\".format(\n",
    "                                search_str\n",
    "                            )\n",
    "                        )\n",
    "                    if line.startswith(\n",
    "                        search_str\n",
    "                    ):  # case sens and no strip since 'def' should be flushed left\n",
    "                        func_lines.append(line)\n",
    "                        while True:\n",
    "                            line = f.readline()\n",
    "                            if line == \"\":\n",
    "                                break\n",
    "                            if line[0] in abet_set:\n",
    "                                break\n",
    "                            func_lines.append(line)\n",
    "                        break\n",
    "\n",
    "            self._function_lines_list.append(func_lines)\n",
    "        if is_pre_cmd is True:\n",
    "            self.pre_py_cmds.append(call_str)\n",
    "        elif is_pre_cmd is False:\n",
    "            self.post_py_cmds.append(call_str)\n",
    "        else:\n",
    "            self.logger.warn(\n",
    "                \"add_py_function() command: {0} is not being called directly\".format(\n",
    "                    call_str\n",
    "                )\n",
    "            )\n",
    "        self.py_functions.update({function_name})\n",
    "\n",
    "    def _process_array_obs(\n",
    "        self,\n",
    "        out_filename,\n",
    "        ins_filename,\n",
    "        prefix,\n",
    "        ofile_sep,\n",
    "        ofile_skip,\n",
    "        zone_array,\n",
    "    ):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1b5e0-845a-4371-a14c-2ead2d7d87c1",
   "metadata": {},
   "source": [
    "# Setup observations for an array-style file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4b9e0-d448-4bd2-8efa-057861607bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"private method to setup observations for an array-style file\n",
    "\n",
    "        Args:\n",
    "            out_filename (`str`): the output array file\n",
    "            ins_filename (`str`): the instruction file to create\n",
    "            prefix (`str`): the prefix to add to the observation names and also to use as the\n",
    "                observation group name.\n",
    "            ofile_sep (`str`): the separator in the output file.  This is currently just a\n",
    "                placeholder arg, only whitespace-delimited files are supported\n",
    "            ofile_skip (`int`): number of header and/or comment lines to skip at the\n",
    "                top of the output file\n",
    "            zone_array (numpy.ndarray): an integer array used to identify positions to skip in the\n",
    "                output array\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Note:\n",
    "            This method is called programmatically by `PstFrom.add_observations()`\n",
    "\n",
    "        \"\"\"\n",
    "        if ofile_sep is not None:\n",
    "            self.logger.lraise(\n",
    "                \"array obs are currently only supported for whitespace delim\"\n",
    "            )\n",
    "        if not os.path.exists(self.new_d / out_filename):\n",
    "            self.logger.lraise(\n",
    "                \"array obs output file '{0}' not found\".format(out_filename)\n",
    "            )\n",
    "        if len(prefix) == 0:\n",
    "            prefix = Path(out_filename).stem\n",
    "        f_out = open(self.new_d / out_filename, \"r\")\n",
    "        f_ins = open(self.new_d / ins_filename, \"w\")\n",
    "        f_ins.write(\"pif ~\\n\")\n",
    "        iline = 0\n",
    "        if ofile_skip is not None:\n",
    "            ofile_skip = int(ofile_skip)\n",
    "            f_ins.write(\"l{0}\\n\".format(ofile_skip))\n",
    "            # read the output file forward\n",
    "            for _ in range(ofile_skip):\n",
    "                f_out.readline()\n",
    "                iline += 1\n",
    "        onames, ovals = [], []\n",
    "        iidx = 0\n",
    "        for line in f_out:\n",
    "            raw = line.split()\n",
    "            f_ins.write(\"l1 \")\n",
    "            for jr, r in enumerate(raw):\n",
    "\n",
    "                try:\n",
    "                    fr = float(r)\n",
    "                except Exception as e:\n",
    "                    self.logger.lraise(\n",
    "                        \"array obs error casting: '{0}' on line {1} to a float: {2}\".format(\n",
    "                            r, iline, str(e)\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                zval = None\n",
    "                if zone_array is not None:\n",
    "                    try:\n",
    "                        zval = zone_array[iidx, jr]\n",
    "                    except Exception as e:\n",
    "                        self.logger.lraise(\n",
    "                            \"array obs error getting zone value for i,j {0},{1} in line {2}: {3}\".format(\n",
    "                                iidx, jr, iline, str(e)\n",
    "                            )\n",
    "                        )\n",
    "                    if zval <= 0:\n",
    "                        f_ins.write(\" !dum! \")\n",
    "                        if jr < len(raw) - 1:\n",
    "                            f_ins.write(\" w \")\n",
    "                        continue\n",
    "\n",
    "                oname = \"oname:{0}_otype:arr_i:{1}_j:{2}\".format(prefix, iidx, jr)\n",
    "                if zval is not None:\n",
    "                    oname += \"_zone:{0}\".format(zval)\n",
    "                f_ins.write(\" !{0}! \".format(oname))\n",
    "                if jr < len(raw) - 1:\n",
    "                    f_ins.write(\" w \")\n",
    "            f_ins.write(\"\\n\")\n",
    "            iline += 1\n",
    "            iidx += 1\n",
    "\n",
    "    def add_observations(\n",
    "        self,\n",
    "        filename,\n",
    "        insfile=None,\n",
    "        index_cols=None,\n",
    "        use_cols=None,\n",
    "        use_rows=None,\n",
    "        prefix=\"\",\n",
    "        ofile_skip=None,\n",
    "        ofile_sep=None,\n",
    "        rebuild_pst=False,\n",
    "        obsgp=None,\n",
    "        zone_array=None,\n",
    "        includes_header=True,\n",
    "    ):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da08697-94a3-4be5-9344-7f20a4b2f246",
   "metadata": {},
   "source": [
    "# Add values in output files as observations to PstFrom object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e36307-2c0a-4148-9e46-1216480a7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"\n",
    "        Add values in output files as observations to PstFrom object\n",
    "\n",
    "        Args:\n",
    "            filename (`str`): model output file name(s) to set up\n",
    "                as observations. By default filename should give relative\n",
    "                loction from top level of pest template directory\n",
    "                (`new_d` as passed to `PstFrom()`).\n",
    "            insfile (`str`): desired instructions file filename\n",
    "            index_cols (`list`-like or `int`): columns to denote are indices for obs\n",
    "            use_cols (`list`-like or `int`): columns to set up as obs. If None,\n",
    "                and `index_cols` is not None (i.e list-style obs assumed),\n",
    "                observations will be set up for all columns in `filename` that\n",
    "                are not in `index_cols`.\n",
    "            use_rows (`list`-like or `int`): select only specific row of file for obs\n",
    "            prefix (`str`): prefix for obsnmes\n",
    "            ofile_skip (`int`): number of lines to skip in model output file\n",
    "            ofile_sep (`str`): delimiter in output file.\n",
    "                If `None`, the delimiter is eventually governed by the file\n",
    "                extension (`,` for .csv).\n",
    "            rebuild_pst (`bool`): (Re)Construct PstFrom.pst object after adding\n",
    "                new obs\n",
    "            obsgp (`str` of `list`-like): observation group name(s). If type\n",
    "                `str` (or list of len == 1) and `use_cols` is None (i.e. all\n",
    "                non-index cols are to  be set up as obs), the same group name\n",
    "                will be mapped to all obs in call. If None the obs group name\n",
    "                will be derived from the base of the constructed observation\n",
    "                name. If passed as `list` (and len(`list`) = `n` > 1), the\n",
    "                entries in obsgp will be interpreted to explicitly define the\n",
    "                grouped for the first `n` cols in `use_cols`, any remaining\n",
    "                columns will default to None and the base of the observation\n",
    "                name will be used. Default is None.\n",
    "            zone_array (`np.ndarray`): array defining spatial limits or zones\n",
    "                for array-style observations. Default is None\n",
    "            includes_header (`bool`): flag indicating that the list-style file\n",
    "                includes a header row.  Default is True.\n",
    "\n",
    "        Returns:\n",
    "            `Pandas.DataFrame`: dataframe with info for new observations\n",
    "\n",
    "        Note:\n",
    "            This is the main entry for adding observations to the pest interface\n",
    "\n",
    "            If `index_cols` and `use_cols` are both None, then it is assumed that\n",
    "            array-style observations are being requested.  In this case,\n",
    "            `filenames` must be only one filename.\n",
    "\n",
    "            `zone_array` is only used for array-style observations.  Zone values\n",
    "            less than or equal to zero are skipped (using the \"dum\" option)\n",
    "\n",
    "\n",
    "        Example::\n",
    "\n",
    "            # setup observations for the 2nd thru 5th columns of the csv file\n",
    "            # using the first column as the index\n",
    "            df = pf.add_observations(\"heads.csv\",index_col=0,use_cols=[1,2,3,4],\n",
    "                                     ofile_sep=\",\")\n",
    "            # add array-style observations, skipping model cells with an ibound\n",
    "            # value less than or equal to zero\n",
    "            df = pf.add_observations(\"conce_array.dat,index_col=None,use_cols=None,\n",
    "                                     zone_array=ibound)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        use_cols_psd = copy.copy(use_cols)  # store passed use_cols argument\n",
    "        if insfile is None:  # setup instruction file name\n",
    "            insfile = \"{0}.ins\".format(filename)\n",
    "        self.logger.log(\"adding observations from output file \" \"{0}\".format(filename))\n",
    "        # precondition arguments\n",
    "        (\n",
    "            filenames,\n",
    "            fmts,\n",
    "            seps,\n",
    "            skip_rows,\n",
    "            index_cols,\n",
    "            use_cols,\n",
    "        ) = self._prep_arg_list_lengths(\n",
    "            filename,\n",
    "            index_cols=index_cols,\n",
    "            use_cols=use_cols,\n",
    "            fmts=None,\n",
    "            seps=ofile_sep,\n",
    "            skip_rows=ofile_skip,\n",
    "        )\n",
    "        # array style obs, if both index_cols and use_cols are None (default)\n",
    "        if index_cols is None and use_cols is None:\n",
    "            if not isinstance(filenames, str):\n",
    "                if len(filenames) > 1:\n",
    "                    self.logger.lraise(\n",
    "                        \"only a single filename can be used for array-style observations\"\n",
    "                    )\n",
    "                filenames = filenames[0]\n",
    "            self.logger.log(\n",
    "                \"adding observations from array output file '{0}'\".format(filenames)\n",
    "            )\n",
    "            # Setup obs for array style output, build and write instruction file\n",
    "            self._process_array_obs(\n",
    "                filenames,\n",
    "                insfile,\n",
    "                prefix,\n",
    "                ofile_sep,\n",
    "                ofile_skip,\n",
    "                zone_array,\n",
    "            )\n",
    "\n",
    "            # Add obs from ins file written by _process_array_obs()\n",
    "            new_obs = self.add_observations_from_ins(\n",
    "                ins_file=self.new_d / insfile, out_file=self.new_d / filename\n",
    "            )\n",
    "            # Try to add an observation group name -- should default to `obgnme`\n",
    "            # TODO: note list style default to base of obs name, here array default to `obgnme`\n",
    "            if obsgp is not None:  # if a group name is passed\n",
    "                new_obs.loc[:, \"obgnme\"] = obsgp\n",
    "            elif prefix is not None and len(prefix) != 0:  # if prefix is passed\n",
    "                new_obs.loc[:, \"obgnme\"] = prefix\n",
    "            else:\n",
    "                new_obs.loc[:, \"obgnme\"] = \"oname:{0}_otype:arr\".format(filenames)\n",
    "            # else will default to `obgnme`\n",
    "            self.logger.log(\n",
    "                \"adding observations from array output file '{0}'\".format(filenames)\n",
    "            )\n",
    "        else:\n",
    "            # list style obs\n",
    "            self.logger.log(\n",
    "                \"adding observations from tabular output file \" \"'{0}'\".format(filenames)\n",
    "            )\n",
    "            # -- will end up here if either of index_cols or use_cols is not None\n",
    "            df, storehead, inssep = self._load_listtype_file(\n",
    "                filenames, index_cols, use_cols, fmts, seps, skip_rows\n",
    "            )\n",
    "            # parse to numeric (read as dtype object to preserve mixed types)\n",
    "            # df = df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "            df = df.apply(_try_pdcol_numeric)\n",
    "            if inssep != \",\":\n",
    "                inssep = seps\n",
    "            else:\n",
    "                inssep = [inssep]\n",
    "            # rectify df?\n",
    "            # if iloc[0] are strings and index_cols are ints,\n",
    "            #   can we assume that there were infact column headers?\n",
    "            if all(isinstance(c, str) for c in df.iloc[0]) and all(\n",
    "                isinstance(a, (int, np.integer)) for a in index_cols\n",
    "            ):\n",
    "                index_cols = df.iloc[0][index_cols].to_list()  # redefine index_cols\n",
    "                if use_cols is not None:\n",
    "                    use_cols = df.iloc[0][use_cols].to_list()  # redefine use_cols\n",
    "                df = df.rename(\n",
    "                    columns=df.iloc[0].to_dict()\n",
    "                ).drop(0).reset_index(drop=True)\n",
    "                df = df.apply(_try_pdcol_numeric)\n",
    "            # Select all non index cols if use_cols is None\n",
    "            if use_cols is None:\n",
    "                use_cols = df.columns.drop(index_cols).tolist()\n",
    "            # Currently just passing through comments in header (i.e. before the table data)\n",
    "            lenhead = 0\n",
    "            stkeys = np.array(\n",
    "                sorted(storehead.keys())\n",
    "            )  # comments line numbers as sorted array\n",
    "            if stkeys.size > 0 and stkeys.min() == 0:\n",
    "                lenhead += 1 + np.sum(np.diff(stkeys) == 1)\n",
    "            new_obs_l = []\n",
    "            for filename, sep in zip(\n",
    "                filenames, inssep\n",
    "            ):  # should only ever be one but hey...\n",
    "                self.logger.log(\n",
    "                    \"building insfile for tabular output file {0}\" \"\".format(filename)\n",
    "                )\n",
    "                # Build dataframe from output file df for use in insfile\n",
    "                df_temp = _get_tpl_or_ins_df(\n",
    "                    df,\n",
    "                    prefix,\n",
    "                    typ=\"obs\",\n",
    "                    index_cols=index_cols,\n",
    "                    use_cols=use_cols,\n",
    "                )\n",
    "                df.loc[:, \"idx_str\"] = df_temp.idx_strs\n",
    "                # Select only certain rows if requested\n",
    "                if use_rows is not None:\n",
    "                    if isinstance(use_rows, str):\n",
    "                        if use_rows not in df.idx_str:\n",
    "                            self.logger.warn(\n",
    "                                \"can't find {0} in generated observation idx_str. \"\n",
    "                                \"setting up obs for all rows instead\"\n",
    "                                \"\".format(use_rows)\n",
    "                            )\n",
    "                            use_rows = None\n",
    "                    elif isinstance(use_rows, (int, np.integer)):\n",
    "                        use_rows = [use_rows]\n",
    "                    use_rows = [r for r in use_rows if r <= len(df)]\n",
    "                    use_rows = df.iloc[use_rows].idx_str.unique()\n",
    "\n",
    "                # Construct ins_file from df\n",
    "                # first rectify group name with number of columns\n",
    "                ncol = len(use_cols)\n",
    "                fill = True  # default fill=True means that the groupname will be\n",
    "                # derived from the base of the observation name\n",
    "                # if passed group name is a string or list with len < ncol\n",
    "                # and passed use_cols was None or of len > len(obsgp)\n",
    "                if obsgp is not None:\n",
    "                    if use_cols_psd is None:  # no use_cols defined (all are setup)\n",
    "                        if len([obsgp] if isinstance(obsgp, str) else obsgp) == 1:\n",
    "                            # only 1 group provided, assume passed obsgp applys\n",
    "                            # to all use_cols\n",
    "                            fill = \"first\"\n",
    "                        else:\n",
    "                            # many obs groups passed, assume last will fill if < ncol\n",
    "                            fill = \"last\"\n",
    "                    # else fill will be set to True (base of obs name will be used)\n",
    "                else:\n",
    "                    obsgp = True  # will use base of col\n",
    "                obsgp = _check_var_len(obsgp, ncol, fill=fill)\n",
    "                nprefix = prefix\n",
    "\n",
    "                if len(nprefix) == 0:\n",
    "                    nprefix = filenames[0]\n",
    "                nprefix = \"oname:{0}_otype:lst\".format(nprefix.lower())\n",
    "                df_ins = pyemu.pst_utils.csv_to_ins_file(\n",
    "                    df.set_index(\"idx_str\"),\n",
    "                    ins_filename=self.new_d / insfile,\n",
    "                    only_cols=use_cols,\n",
    "                    only_rows=use_rows,\n",
    "                    marker=\"~\",\n",
    "                    includes_header=includes_header,\n",
    "                    includes_index=False,\n",
    "                    prefix=nprefix,\n",
    "                    head_lines_len=lenhead,\n",
    "                    sep=sep,\n",
    "                    gpname=obsgp,\n",
    "                )\n",
    "                self.logger.log(\n",
    "                    \"building insfile for tabular output file {0}\" \"\".format(filename)\n",
    "                )\n",
    "                new_obs = self.add_observations_from_ins(\n",
    "                    ins_file=self.new_d / insfile, out_file=self.new_d / filename\n",
    "                )\n",
    "                if \"obgnme\" in df_ins.columns:\n",
    "                    new_obs.loc[:, \"obgnme\"] = df_ins.loc[new_obs.index, \"obgnme\"]\n",
    "                new_obs_l.append(new_obs)\n",
    "            new_obs = pd.concat(new_obs_l)\n",
    "            self.logger.log(\n",
    "                \"adding observations from tabular output file \" \"'{0}'\".format(filenames)\n",
    "            )\n",
    "        self.insfile_obsmap[insfile] = new_obs.obsnme.to_list()\n",
    "        self.logger.log(\"adding observations from output file \" \"{0}\".format(filename))\n",
    "        if rebuild_pst:\n",
    "            if self.pst is not None:\n",
    "                self.logger.log(\"Adding obs to control file \" \"and rewriting pst\")\n",
    "                self.build_pst(filename=self.pst.filename, update=\"obs\")\n",
    "                self.logger.log(\"Adding obs to control file \" \"and rewriting pst\")\n",
    "            else:\n",
    "                pstname = Path(self.new_d, self.original_d.name)\n",
    "                self.logger.warn(\n",
    "                    \"pst object not available, \"\n",
    "                    f\"new control file will be written with filename {pstname}\"\n",
    "                )\n",
    "                self.build_pst(filename=None, update=False)\n",
    "\n",
    "        return new_obs\n",
    "\n",
    "    def add_observations_from_ins(\n",
    "        self, ins_file, out_file=None, pst_path=None, inschek=True\n",
    "    ):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf81bd5f-2b85-49b7-bb5f-1163e9078915",
   "metadata": {},
   "source": [
    "# add new observations to a control file from an existing instruction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209dcf0-3cd4-4c16-9d92-6001ed592271",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"add new observations to a control file from an existing instruction file\n",
    "\n",
    "        Args:\n",
    "            ins_file (`str`): instruction file with exclusively new\n",
    "               observation names. N.B. if `ins_file` just contains base\n",
    "               filename string (i.e. no directory name), the path to PEST\n",
    "               directory will be automatically appended.\n",
    "            out_file (`str`): model output file.  If None, then\n",
    "               ins_file.replace(\".ins\",\"\") is used. Default is None.\n",
    "               If `out_file` just contains base filename string\n",
    "               (i.e. no directory name), the path to PEST directory will be\n",
    "               automatically appended.\n",
    "            pst_path (`str`): the path to append to the instruction file and\n",
    "               out file in the control file.  If not None, then any existing\n",
    "               path in front of the template or ins file is split off and\n",
    "               pst_path is prepended.  If python is being run in a directory\n",
    "               other than where the control file will reside, it is useful\n",
    "               to pass `pst_path` as `.`. Default is None\n",
    "            inschek (`bool`): flag to try to process the existing output file\n",
    "               using the `pyemu.InstructionFile` class.  If successful,\n",
    "               processed outputs are used as obsvals\n",
    "\n",
    "        Returns:\n",
    "            `pandas.DataFrame`: the data for the new observations that were\n",
    "               added\n",
    "\n",
    "        Note:\n",
    "            populates the new observation information with default values\n",
    "\n",
    "        Example::\n",
    "\n",
    "            pf = pyemu.PstFrom(\"temp\",\"template\")\n",
    "            pf.add_observations_from_ins(os.path.join(\"template\",\"new_obs.dat.ins\"),\n",
    "                                 pst_path=\".\")\n",
    "\n",
    "        \"\"\"\n",
    "        # lifted almost completely from `Pst().add_observation()`\n",
    "        if os.path.dirname(ins_file) in [\"\", \".\"]:\n",
    "            # if insfile is passed as just a filename,\n",
    "            # append pest directory name\n",
    "            ins_file = self.new_d / ins_file\n",
    "            pst_path = \".\"  # reset and new assumed pst_path\n",
    "        # else:\n",
    "        # assuming that passed insfile is the full path to file from current location\n",
    "        if not os.path.exists(ins_file):\n",
    "            self.logger.lraise(\n",
    "                \"ins file not found: {0}, {1}\" \"\".format(os.getcwd(), ins_file)\n",
    "            )\n",
    "        if out_file is None:\n",
    "            out_file = str(ins_file).replace(\".ins\", \"\")\n",
    "        elif os.path.dirname(out_file) in [\"\", \".\"]:\n",
    "            out_file = self.new_d / out_file\n",
    "        if ins_file == out_file:\n",
    "            self.logger.lraise(\"ins_file == out_file, doh!\")\n",
    "\n",
    "        # get the obs names in the instructions file\n",
    "        self.logger.log(\n",
    "            \"adding observation from instruction file '{0}'\".format(ins_file)\n",
    "        )\n",
    "        obsnme = pyemu.pst_utils.parse_ins_file(ins_file)\n",
    "\n",
    "        sobsnme = set(obsnme)\n",
    "        if len(self.obs_dfs) > 0:\n",
    "            sexist = pd.concat(self.obs_dfs).obsnme\n",
    "        else:\n",
    "            sexist = []\n",
    "        sexist = set(sexist)  # todo need to check this here?\n",
    "        sint = sobsnme.intersection(sexist)\n",
    "        if len(sint) > 0:\n",
    "            self.logger.lraise(\n",
    "                \"the following obs instruction file {0} are already in the \"\n",
    "                \"control file:{1}\".format(ins_file, \",\".join(sint))\n",
    "            )\n",
    "\n",
    "        # find \"new\" obs that are not already in the control file\n",
    "        new_obsnme = sobsnme - sexist\n",
    "        if len(new_obsnme) == 0:\n",
    "            self.logger.lraise(\n",
    "                \"no new observations found in instruction file {0}\".format(ins_file)\n",
    "            )\n",
    "\n",
    "        # extend observation_data\n",
    "        new_obsnme = np.sort(list(new_obsnme))\n",
    "        new_obs_data = pyemu.pst_utils._populate_dataframe(\n",
    "            new_obsnme,\n",
    "            pyemu.pst_utils.pst_config[\"obs_fieldnames\"],\n",
    "            pyemu.pst_utils.pst_config[\"obs_defaults\"],\n",
    "            pyemu.pst_utils.pst_config[\"obs_dtype\"],\n",
    "        )\n",
    "        new_obs_data.loc[new_obsnme, \"obsnme\"] = new_obsnme\n",
    "        new_obs_data.index = new_obsnme\n",
    "\n",
    "        # need path relative to where control file\n",
    "        ins_file_pstrel = Path(ins_file).relative_to(self.new_d)\n",
    "        out_file_pstrel = Path(out_file).relative_to(self.new_d)\n",
    "        if pst_path is not None:\n",
    "            ins_file_pstrel = pst_path / ins_file_pstrel\n",
    "            out_file_pstrel = pst_path / out_file_pstrel\n",
    "        self.ins_filenames.append(ins_file_pstrel)\n",
    "        self.output_filenames.append(out_file_pstrel)\n",
    "        # add to temporary files to be removed at start of forward run\n",
    "        self.tmp_files.append(out_file_pstrel)\n",
    "        df = None\n",
    "        if inschek:\n",
    "            # df = pst_utils._try_run_inschek(ins_file,out_file,cwd=cwd)\n",
    "            # ins_file = os.path.join(cwd, ins_file)\n",
    "            # out_file = os.path.join(cwd, out_file)\n",
    "            df = pyemu.pst_utils.try_process_output_file(\n",
    "                ins_file=ins_file, output_file=out_file\n",
    "            )\n",
    "        if df is not None:\n",
    "            # print(self.observation_data.index,df.index)\n",
    "            new_obs_data.loc[df.index, \"obsval\"] = df.obsval\n",
    "        self.obs_dfs.append(new_obs_data)\n",
    "        self.logger.log(\n",
    "            \"adding observation from instruction file '{0}'\".format(ins_file)\n",
    "        )\n",
    "        return new_obs_data\n",
    "\n",
    "    def add_parameters(\n",
    "        self,\n",
    "        filenames,\n",
    "        par_type,\n",
    "        zone_array=None,\n",
    "        dist_type=\"gaussian\",\n",
    "        sigma_range=4.0,\n",
    "        upper_bound=None,\n",
    "        lower_bound=None,\n",
    "        transform=None,\n",
    "        par_name_base=\"p\",\n",
    "        index_cols=None,\n",
    "        use_cols=None,\n",
    "        use_rows=None,\n",
    "        pargp=None,\n",
    "        pp_space=10,\n",
    "        use_pp_zones=False,\n",
    "        num_eig_kl=100,\n",
    "        spatial_reference=None,\n",
    "        geostruct=None,\n",
    "        datetime=None,\n",
    "        mfile_fmt=\"free\",\n",
    "        mfile_skip=None,\n",
    "        mfile_sep=None,\n",
    "        ult_ubound=None,\n",
    "        ult_lbound=None,\n",
    "        rebuild_pst=False,\n",
    "        alt_inst_str=\"inst\",\n",
    "        comment_char=None,\n",
    "        par_style=\"multiplier\",\n",
    "        initial_value=None,\n",
    "    ):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f4361-4f58-4dcc-a9a7-59e52726bd40",
   "metadata": {},
   "source": [
    "# Add list or array style model input files to PstFrom object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d9381-4b69-49c2-9302-163923dc788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"\n",
    "        Add list or array style model input files to PstFrom object.\n",
    "        This method is the main entry point for adding parameters to the\n",
    "        pest interface\n",
    "\n",
    "        Args:\n",
    "            filenames (`str`): Model input filenames to parameterize. By default filename should give relative\n",
    "                loction from top level of pest template directory\n",
    "                (`new_d` as passed to `PstFrom()`).\n",
    "            par_type (`str`): One of `grid` - for every element, `constant` - for single\n",
    "                parameter applied to every element, `zone` - for zone-based\n",
    "                parameterization or `pilotpoint` - for\n",
    "                pilot-point base parameterization of array style input files.\n",
    "                Note `kl` not yet implemented # TODO\n",
    "            zone_array (`np.ndarray`): array defining spatial limits or zones\n",
    "                for parameterization.\n",
    "            dist_type: not yet implemented # TODO\n",
    "            sigma_range: not yet implemented # TODO\n",
    "            upper_bound (`float`): PEST parameter upper bound.  If `None`, then 1.0e+10 is used.  Default is `None` #\n",
    "            lower_bound (`float`): PEST parameter lower bound.  If `None` and `transform` is \"log\", then 1.0e-10 is used.\n",
    "                Otherwise, if `None`, -1.0e+10 is used.  Default is `None`\n",
    "            transform (`str`): PEST parameter transformation.  Must be either \"log\",\"none\" or \"fixed.  The \"tied\" transform\n",
    "                must be used after calling `PstFrom.build_pst()`.\n",
    "            par_name_base (`str` or `list`-like): basename for parameters that\n",
    "                are set up. If parameter file is tabular list-style file\n",
    "                (`index_cols` is not None) then :\n",
    "                len(par_name_base) must equal len(use_cols)\n",
    "            index_cols (`list`-like): if not None, will attempt to parameterize\n",
    "                expecting a tabular-style model input file. `index_cols`\n",
    "                defines the unique columns used to set up pars. If passed as a\n",
    "                list of `str`, stings are expected to denote the columns\n",
    "                headers in tabular-style parameter files; if `i` and `j` in\n",
    "                list, these columns will be used to define spatial position for\n",
    "                spatial correlations (if required). WARNING: If passed as list\n",
    "                of `int`, `i` and `j` will be assumed to be in last two entries\n",
    "                in the list. Can be passed as a dictionary using the keys\n",
    "                `i` and `j` to explicitly speficy the columns that relate to\n",
    "                model rows and columns to be identified and processed to x,y.\n",
    "            use_cols (`list`-like or `int`): for tabular-style model input file,\n",
    "                defines the columns to be parameterised\n",
    "            use_rows (`list` or `tuple`): Setup parameters for\n",
    "                only specific rows in list-style model input file.\n",
    "                Action is dependent on the the dimensions of use_rows.\n",
    "                If ndim(use_rows) < 2: use_rows is assumed to represent the row number, index slicer (equiv df.iloc),\n",
    "                for all passed files (after headers stripped). So use_rows=[0,3,5], will parameterise the\n",
    "                1st, 4th and 6th rows of each passed list-like file.\n",
    "                If ndim(use_rows) = 2: use_rows represent the index value to paramterise according to index_cols.\n",
    "                e.g. [(3,5,6)] or [[3,5,6]] would attempt to set parameters where the model file\n",
    "                values for 3 `index_cols` are 3,5,6. N.B. values in tuple are the actual\n",
    "                model file entry values.\n",
    "                If no rows in the model input file match `use_rows`, parameters\n",
    "                will be set up for all rows. Only valid/effective if index_cols is not None.\n",
    "                Default is None -- setup parameters for all rows.\n",
    "            pargp (`str`): Parameter group to assign pars to. This is PESTs\n",
    "                pargp but is also used to gather correlated parameters set up\n",
    "                using multiple `add_parameters()` calls (e.g. temporal pars)\n",
    "                with common geostructs.\n",
    "            pp_space (`float`, `int`,`str` or `pd.DataFrame`): Spatial pilot point information.\n",
    "                If `float` or `int`, AND `spatial_reference` is of type VertexGrid, it is the spacing in model length untis between pilot points.\n",
    "                If `int` it is the spacing in rows and cols of where to place pilot points.\n",
    "                If `pd.DataFrame`, then this arg is treated as a prefined set of pilot points\n",
    "                and in this case, the dataframe must have \"name\", \"x\", \"y\", and optionally \"zone\" columns.\n",
    "                If `str`, then an attempt is made to load a dataframe from a csv file (if `pp_space` ends with \".csv\"),\n",
    "                shapefile (if `pp_space` ends with \".shp\") or from a pilot points file.  If `pp_space` is None,\n",
    "                an integer spacing of 10 is used.  Default is None\n",
    "            use_pp_zones (`bool`): a flag to use the greater-than-zero values\n",
    "                in the zone_array as pilot point zones.\n",
    "                If False, zone_array values greater than zero are treated as a\n",
    "                single zone.  This argument is only used if `pp_space` is None\n",
    "                or `int`. Default is False.\n",
    "            num_eig_kl: TODO - impliment with KL pars\n",
    "            spatial_reference (`pyemu.helpers.SpatialReference`): If different\n",
    "                spatial reference required for pilotpoint setup.\n",
    "                If None spatial reference passed to `PstFrom()` will be used\n",
    "                for pilot-points\n",
    "            geostruct (`pyemu.geostats.GeoStruct()`): For specifying correlation\n",
    "                geostruct for pilot-points and par covariance.\n",
    "            datetime (`str`): optional %Y%m%d string or datetime object for\n",
    "                setting up temporally correlated pars. Where datetime is passed\n",
    "                correlation axis for pars will be set to timedelta.\n",
    "            mfile_fmt (`str`): format of model input file - this will be preserved\n",
    "            mfile_skip (`int` or `str`): header in model input file to skip\n",
    "                when reading and reapply when writing. Can optionally be `str` in which case `mf_skip` will be treated\n",
    "                as a `comment_char`.\n",
    "            mfile_sep (`str`): separator/delimiter in model input file.\n",
    "                If None, separator will be interpretted from file name extension.\n",
    "                `.csv` is assumed to be comma separator. Default is None\n",
    "            ult_ubound (`float`): Ultimate upper bound for model input\n",
    "                parameter once all mults are applied - ensure physical model par vals. If not passed,\n",
    "                it is set to 1.0e+30\n",
    "            ult_lbound (`float`): Ultimate lower bound for model input\n",
    "                parameter once all mults are applied.  If not passed, it is set to\n",
    "                1.0e-30 for log transform and -1.0e+30 for non-log transform\n",
    "            rebuild_pst (`bool`): (Re)Construct PstFrom.pst object after adding\n",
    "                new parameters\n",
    "            alt_inst_str (`str`): Alternative to default `inst` string in\n",
    "                parameter names. Specify ``None`` or ``\"\"`` to exclude the instance\n",
    "                information from parameter names. For example, if parameters\n",
    "                that apply to more than one input/template file are desired.\n",
    "            comment_char (`str`): option to skip comment lines in model file.\n",
    "                This is not additive with `mfile_skip` option.\n",
    "                Warning: currently comment lines within list-style tabular data\n",
    "                will be lost.\n",
    "            par_style (`str`): either \"m\"/\"mult\"/\"multiplier\", \"a\"/\"add\"/\"addend\", or \"d\"/\"direct\" where the former sets up\n",
    "                up a multiplier and addend parameters process against the existing model input\n",
    "                array and the former sets up a template file to write the model\n",
    "                input file directly.  Default is \"multiplier\".\n",
    "\n",
    "            initial_value (`float`): the value to set for the `parval1` value in the control file\n",
    "                Default is 1.0\n",
    "        Returns:\n",
    "            `pandas.DataFrame`: dataframe with info for new parameters\n",
    "\n",
    "        Example::\n",
    "\n",
    "            # setup grid-scale direct parameters for an array of numbers\n",
    "            df = pf.add_parameters(\"hk.dat\",par_type=\"grid\",par_style=\"direct\")\n",
    "            # setup pilot point multiplier parameters for an array of numbers\n",
    "            # with a pilot point being set in every 5th active model cell\n",
    "            df = pf.add_parameters(\"recharge.dat\",par_type=\"pilotpoint\",pp_space=5,\n",
    "                                   zone_array=\"ibound.dat\")\n",
    "            # setup a single multiplier parameter for the 4th column\n",
    "            # of a column format (list/tabular type) file\n",
    "            df = pf.add_parameters(\"wel_list_1.dat\",par_type=\"constant\",\n",
    "                                   index_cols=[0,1,2],use_cols=[3])\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO need more support for temporal pars?\n",
    "        #  - As another partype using index_cols or an additional time_cols\n",
    "\n",
    "        # TODO support passing par_file (i,j)/(x,y) directly where information\n",
    "        #  is not contained in model parameter file - e.g. no i,j columns\n",
    "        self.add_pars_callcount += 1\n",
    "        self.ijwarned[self.add_pars_callcount] = False\n",
    "\n",
    "        if transform is None:\n",
    "            if par_style in [\"a\", \"add\", \"addend\"]:\n",
    "                transform = 'none'\n",
    "                self.logger.statement(\n",
    "                    \"par_style is 'add' and transform was not passed, setting tranform to 'none'\"\n",
    "                )\n",
    "            else:\n",
    "                transform = 'log'\n",
    "                self.logger.statement(\n",
    "                    \"transform was not passed, setting default tranform to 'log'\"\n",
    "                )\n",
    "        if transform.lower().strip() not in [\"none\", \"log\", \"fixed\"]:\n",
    "            self.logger.lraise(\n",
    "                \"unrecognized transform ('{0}'), should be in ['none','log','fixed']\".format(\n",
    "                    transform\n",
    "                )\n",
    "            )\n",
    "        if transform == \"fixed\" and geostruct is not None:\n",
    "            self.logger.lraise(\n",
    "                \"geostruct is not 'None', cant draw values for fixed pars\"\n",
    "            )\n",
    "\n",
    "        # some checks for direct parameters\n",
    "        par_style = par_style.lower()\n",
    "        if len(par_style) > 1:\n",
    "            par_style = par_style[0]\n",
    "        if par_style not in [\"m\", \"d\", \"a\"]:\n",
    "            self.logger.lraise(\n",
    "                \"add_parameters(): unrecognized 'style': {0}, \"\n",
    "                \"should be either 'm'/'mult'/'multiplier', \"\n",
    "                \"'a'/'add'/'addend' or 'd'/'direct'\".format(\n",
    "                    par_style\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if initial_value is None:\n",
    "            if par_style == \"m\":\n",
    "                initial_value = 1.0\n",
    "            elif par_style == \"a\":\n",
    "                initial_value = 0.0\n",
    "            elif par_style == \"d\":\n",
    "                initial_value = 1.0  # ?\n",
    "\n",
    "        if upper_bound is None:\n",
    "            upper_bound = 1.0e10\n",
    "\n",
    "        if lower_bound is None:\n",
    "            if transform.lower() == \"log\":\n",
    "                lower_bound = 1.0e-10\n",
    "            else:\n",
    "                lower_bound = -1.0e10\n",
    "\n",
    "        if isinstance(filenames, str) or isinstance(filenames, Path):\n",
    "            filenames = [filenames]\n",
    "        # data file paths relative to the pest parent directory\n",
    "        filenames = [\n",
    "            get_relative_filepath(self.original_d, filename) for filename in filenames\n",
    "        ]\n",
    "        if len(filenames) == 0:\n",
    "            self.logger.lraise(\"add_parameters(): filenames is empty\")\n",
    "        if par_style == \"d\":\n",
    "            if len(filenames) != 1:\n",
    "                self.logger.lraise(\n",
    "                    \"add_parameters(): 'filenames' arg for 'direct' style \"\n",
    "                    \"must contain one and only one filename, \"\n",
    "                    f\"not {len(filenames)} files\"\n",
    "                )\n",
    "            if filenames[0] in self.direct_org_files:\n",
    "                self.logger.lraise(\n",
    "                    f\"add_parameters(): original model input file \"\n",
    "                    f\"'{filenames[0]}' already used for 'direct' parameterization\"\n",
    "                )\n",
    "            else:\n",
    "                self.direct_org_files.append(filenames[0])\n",
    "        # Default par data columns used for pst\n",
    "        par_data_cols = pyemu.pst_utils.pst_config[\"par_fieldnames\"]\n",
    "        self.logger.log(\n",
    "            f\"adding {par_type} type {par_style} style parameters for file(s) \"\n",
    "            f\"{[str(f) for f in filenames]}\"\n",
    "        )\n",
    "        if geostruct is not None:\n",
    "            self.logger.log(\"using geostruct:{0}\".format(str(geostruct)))\n",
    "            if geostruct.sill != 1.0:  #  and par_style != \"multiplier\": #TODO !=?\n",
    "                self.logger.warn(\n",
    "                    \"geostruct sill != 1.0\"  # for 'multiplier' style parameters\"\n",
    "                )\n",
    "            if geostruct.transform != transform:\n",
    "                self.logger.warn(\n",
    "                    \"0) Inconsistency between \" \"geostruct transform and partrans.\"\n",
    "                )\n",
    "                self.logger.warn(f\"1) Setting geostruct transform to {transform}\")\n",
    "                if geostruct not in self.par_struct_dict.keys():\n",
    "                    # safe to just reset transform\n",
    "                    geostruct.transform = transform\n",
    "                else:\n",
    "                    self.logger.warn(\"2) This will create a new copy of geostruct\")\n",
    "                    # to avoid flip flopping transform need to make a new geostruct\n",
    "                    geostruct = copy.copy(geostruct)\n",
    "                    geostruct.transform = transform\n",
    "                self.logger.warn(\n",
    "                    \"-) Better to pass an appropriately transformed geostruct\"\n",
    "                )\n",
    "        # big sr and zone dependancy checker here: todo - tidy?\n",
    "        checker = (\n",
    "                self._spatial_reference is not None\n",
    "                and not isinstance(self._spatial_reference, dict)\n",
    "                and self._spatial_reference.grid_type == 'vertex'\n",
    "                and zone_array is not None\n",
    "                and len(zone_array.shape) == 1\n",
    "        )\n",
    "        if checker:\n",
    "            zone_array = np.reshape(zone_array, (zone_array.shape[0], 1))\n",
    "\n",
    "        # Get useful variables from arguments passed\n",
    "        # if index_cols passed as a dictionary that maps i,j information\n",
    "        idx_cols = index_cols\n",
    "        ij_in_idx = None\n",
    "        xy_in_idx = None\n",
    "        if isinstance(index_cols, dict):\n",
    "            # only useful if i and j are in keys .... or xy?\n",
    "            # TODO: or datetime str?\n",
    "            keys = np.array([k.lower() for k in index_cols.keys()])\n",
    "            idx_cols = [index_cols[k] for k in keys]\n",
    "            if any(all(a in keys for a in aa) for aa in [[\"i\", \"j\"], [\"x\", \"y\"]]):\n",
    "                if all(ij in keys for ij in [\"i\", \"j\"]):\n",
    "                    o_idx = np.argsort(keys)\n",
    "                    ij_in_idx = o_idx[np.searchsorted(keys[o_idx], [\"i\", \"j\"])]\n",
    "                if all(xy in keys for xy in [\"x\", \"y\"]):\n",
    "                    o_idx = np.argsort(keys)\n",
    "                    xy_in_idx = o_idx[np.searchsorted(keys[o_idx], [\"x\", \"y\"])]\n",
    "            else:\n",
    "                self.logger.lraise(\n",
    "                    \"If passing `index_cols` as type == dict, \"\n",
    "                    \"keys need to contain [`i` and `j`] or \"\n",
    "                    \"[`x` and `y`]\"\n",
    "                )\n",
    "\n",
    "        (\n",
    "            index_cols,\n",
    "            use_cols,\n",
    "            file_dict,\n",
    "            fmt_dict,\n",
    "            sep_dict,\n",
    "            skip_dict,\n",
    "            headerlines,  # needed for direct pars (need to be in tpl)\n",
    "        ) = self._par_prep(\n",
    "            filenames,\n",
    "            idx_cols,\n",
    "            use_cols,\n",
    "            fmts=mfile_fmt,\n",
    "            skip_rows=mfile_skip,\n",
    "            c_char=comment_char,\n",
    "            seps=mfile_sep,\n",
    "        )\n",
    "        if datetime is not None:  # convert and check datetime\n",
    "            # TODO: something needed here to allow a different relative point.\n",
    "            datetime = _get_datetime_from_str(datetime)\n",
    "            if self.start_datetime is None:\n",
    "                self.logger.warn(\n",
    "                    \"NO START_DATEIME PROVIDED, ASSUMING PAR \"\n",
    "                    \"DATETIME IS START {}\".format(datetime)\n",
    "                )\n",
    "                self.start_datetime = datetime\n",
    "            assert (\n",
    "                datetime >= self.start_datetime\n",
    "            ), \"passed datetime is earlier than start_datetime {0}, {1}\".format(\n",
    "                datetime, self.start_datetime\n",
    "            )\n",
    "            t_offest = datetime - self.start_datetime\n",
    "\n",
    "        # Pull out and construct name-base for parameters\n",
    "        if isinstance(par_name_base, str):\n",
    "            par_name_base = [par_name_base]\n",
    "        # if `use_cols` is passed check number of base names is the same as cols\n",
    "        if use_cols is None and len(par_name_base) == 1:\n",
    "            pass\n",
    "        elif use_cols is not None and len(par_name_base) == len(use_cols):\n",
    "            pass\n",
    "        else:\n",
    "            self.logger.lraise(\n",
    "                \"par_name_base should be a string, \"\n",
    "                \"single-element container, or container of \"\n",
    "                f\"len use_cols, not '{str(par_name_base)}'\"\n",
    "            )\n",
    "\n",
    "        # otherewise, things get tripped up in the ensemble/cov stuff\n",
    "        if pargp is not None:\n",
    "            if isinstance(pargp, list):\n",
    "                pargp = [pg.lower() for pg in pargp]\n",
    "            else:\n",
    "                pargp = pargp.lower()\n",
    "        par_name_base = [pnb.lower() for pnb in par_name_base]\n",
    "\n",
    "\n",
    "        fmt = \"_{0}\".format(alt_inst_str) + \":{0}\"\n",
    "        chk_prefix = \"_{0}\".format(alt_inst_str)  # add `instance` identifier\n",
    "\n",
    "        # increment name base if already passed\n",
    "        for i in range(len(par_name_base)):\n",
    "            # multiplier file name will be taken first par group, if passed\n",
    "            # (the same multipliers will apply to all pars passed in this call)\n",
    "            # Remove `:` for filenames\n",
    "            # multiplier file needs instance number \n",
    "            # regardless of whether instance is to be included \n",
    "            # in the parameter names\n",
    "            if i == 0:\n",
    "                inst = self._next_count(par_name_base[i] +\\\n",
    "                                        chk_prefix)\n",
    "                par_name_store = (par_name_base[0] + \n",
    "                                  fmt.format(inst)).replace(\":\", \"\")\n",
    "                # if instance is to be included in the parameter names\n",
    "                # add the instance suffix to the parameter name base\n",
    "                if alt_inst_str is not None and len(alt_inst_str) > 0:\n",
    "                    par_name_base[0] += fmt.format(inst)\n",
    "            # if instance is to be included in the parameter names\n",
    "            # add the instance suffix to the parameter name base\n",
    "            elif alt_inst_str is not None and len(alt_inst_str) > 0:\n",
    "                par_name_base[i] += fmt.format(\n",
    "                    self._next_count(par_name_base[i] + chk_prefix)\n",
    "                )\n",
    "\n",
    "        # multiplier file name will be taken first par group, if passed\n",
    "        # (the same multipliers will apply to all pars passed in this call)\n",
    "        # Remove `:` for filenames\n",
    "        #par_name_store = par_name_base[0].replace(\":\", \"\")  # for os filename\n",
    "\n",
    "        # Define requisite filenames\n",
    "        if par_style in [\"m\", \"a\"]:\n",
    "            mlt_filename = \"{0}_{1}.csv\".format(par_name_store, par_type)\n",
    "            # pst input file (for tpl->in pair) is multfile (in mult dir)\n",
    "            in_fileabs = self.mult_file_d / mlt_filename\n",
    "            # pst input file (for tpl->in pair) is multfile (in mult dir)\n",
    "            in_filepst = in_fileabs.relative_to(self.new_d)\n",
    "            tpl_filename = self.tpl_d / (mlt_filename + \".tpl\")\n",
    "        else:\n",
    "            mlt_filename = np.NaN\n",
    "            # absolute path to org/datafile\n",
    "            in_fileabs = self.original_file_d / filenames[0].name\n",
    "            # pst input file (for tpl->in pair) is orgfile (in org dir)\n",
    "            # relative path to org/datafile (relative to dest model workspace):\n",
    "            in_filepst = in_fileabs.relative_to(self.new_d)\n",
    "            tpl_filename = self.tpl_d / (filenames[0].name + \".tpl\")\n",
    "\n",
    "        # this keeps denormal values for creeping into the model input arrays\n",
    "        ubfill = None\n",
    "        lbfill = None\n",
    "        if ult_ubound is None:\n",
    "            # no ultimate bounds are passed default to class set bounds\n",
    "            ult_ubound = self.ult_ubound_fill\n",
    "            ubfill = \"first\"  # will fill for all use_cols\n",
    "        if ult_lbound is None:\n",
    "            ult_lbound = self.ult_lbound_fill\n",
    "            lbfill = \"first\"\n",
    "\n",
    "        pp_filename = None  # setup placeholder variables\n",
    "        fac_filename = None\n",
    "        nxs = None\n",
    "        # Process model parameter files to produce appropriate pest pars\n",
    "        if index_cols is not None:  # Assume list/tabular type input files\n",
    "            # ensure inputs are provided for all required cols\n",
    "            ncol = len(use_cols)\n",
    "            ult_lbound = _check_var_len(ult_lbound, ncol, fill=ubfill)\n",
    "            ult_ubound = _check_var_len(ult_ubound, ncol, fill=lbfill)\n",
    "            pargp = _check_var_len(pargp, ncol)\n",
    "            lower_bound = _check_var_len(lower_bound, ncol, fill=\"first\")\n",
    "            upper_bound = _check_var_len(upper_bound, ncol, fill=\"first\")\n",
    "            if len(use_cols) != len(ult_lbound) != len(ult_ubound):\n",
    "                self.logger.lraise(\n",
    "                    \"mismatch in number of columns to use {0} \"\n",
    "                    \"and number of ultimate lower {0} or upper \"\n",
    "                    \"{1} par bounds defined\"\n",
    "                    \"\".format(len(use_cols), len(ult_lbound), len(ult_ubound))\n",
    "                )\n",
    "\n",
    "            self.logger.log(\n",
    "                \"writing list-style template file '{0}'\".format(tpl_filename)\n",
    "            )\n",
    "            # Generate tabular type template - also returns par data\n",
    "            # relative file paths are in file_dict as Path instances (kludgey)\n",
    "            dfs = [file_dict[Path(filename)] for filename in filenames]\n",
    "            get_xy = None\n",
    "            if (\n",
    "                par_type.startswith(\"grid\") or par_type.startswith(\"p\")\n",
    "            ) and geostruct is not None:\n",
    "                get_xy = self.get_xy\n",
    "            df, nxs = write_list_tpl(\n",
    "                filenames,\n",
    "                dfs,\n",
    "                par_name_base,\n",
    "                tpl_filename=tpl_filename,\n",
    "                par_type=par_type,\n",
    "                suffix=\"\",\n",
    "                index_cols=index_cols,\n",
    "                use_cols=use_cols,\n",
    "                use_rows=use_rows,\n",
    "                zone_array=zone_array,\n",
    "                gpname=pargp,\n",
    "                ij_in_idx=ij_in_idx,\n",
    "                xy_in_idx=xy_in_idx,\n",
    "                get_xy=get_xy,\n",
    "                zero_based=self.zero_based,\n",
    "                input_filename=in_fileabs,\n",
    "                par_style=par_style,\n",
    "                headerlines=headerlines,\n",
    "                fill_value=initial_value,\n",
    "                logger=self.logger,\n",
    "            )\n",
    "            nxs = {fname: nx for fname, nx in zip(filenames, nxs)}\n",
    "            assert (\n",
    "                np.mod(len(df), len(use_cols)) == 0.0\n",
    "            ), \"Parameter dataframe wrong shape for number of cols {0}\" \"\".format(\n",
    "                use_cols\n",
    "            )\n",
    "            # variables need to be passed to each row in df\n",
    "            lower_bound = np.tile(lower_bound, int(len(df) / ncol))\n",
    "            upper_bound = np.tile(upper_bound, int(len(df) / ncol))\n",
    "            self.logger.log(\n",
    "                \"writing list-style template file '{0}'\".format(tpl_filename)\n",
    "            )\n",
    "        else:  # Assume array type parameter file\n",
    "            self.logger.log(\n",
    "                \"writing array-style template file '{0}'\".format(tpl_filename)\n",
    "            )\n",
    "            if pargp is None:\n",
    "                pargp = par_name_base[0]\n",
    "            shp = file_dict[list(file_dict.keys())[0]].shape\n",
    "            # ARRAY constant, zones or grid (cell-by-cell)\n",
    "            if par_type in {\"constant\", \"zone\", \"grid\"}:\n",
    "                self.logger.log(\n",
    "                    \"writing template file \"\n",
    "                    \"{0} for {1}\".format(tpl_filename, par_name_base)\n",
    "                )\n",
    "                # Generate array type template - also returns par data\n",
    "                df = write_array_tpl(\n",
    "                    name=par_name_base[0],\n",
    "                    tpl_filename=tpl_filename,\n",
    "                    suffix=\"\",\n",
    "                    par_type=par_type,\n",
    "                    zone_array=zone_array,\n",
    "                    shape=shp,\n",
    "                    get_xy=self.get_xy,\n",
    "                    fill_value=initial_value if initial_value is not None else 1.0,\n",
    "                    gpname=pargp,\n",
    "                    input_filename=in_fileabs,\n",
    "                    par_style=par_style,\n",
    "                )\n",
    "                self.logger.log(\n",
    "                    \"writing template file\"\n",
    "                    \" {0} for {1}\".format(tpl_filename, par_name_base)\n",
    "                )\n",
    "            # ARRAY PILOTPOINT setup\n",
    "            elif par_type in {\n",
    "                \"pilotpoints\",\n",
    "                \"pilot_points\",\n",
    "                \"pilotpoint\",\n",
    "                \"pilot_point\",\n",
    "                \"pilot-point\",\n",
    "                \"pilot-points\",\n",
    "                \"pp\"\n",
    "            }:\n",
    "                if par_style == \"d\":\n",
    "                    self.logger.lraise(\n",
    "                        \"pilot points not supported for 'direct' par_style\"\n",
    "                    )\n",
    "                # Setup pilotpoints for array type par files\n",
    "                self.logger.log(\"setting up pilot point parameters\")\n",
    "                # finding spatial references for for setting up pilot points\n",
    "                if spatial_reference is None:\n",
    "                    # if none passed with add_pars call\n",
    "                    self.logger.statement(\n",
    "                        \"No spatial reference \" \"(containing cell spacing) passed.\"\n",
    "                    )\n",
    "                    if self.spatial_reference is not None:\n",
    "                        # using global sr on PstFrom object\n",
    "                        self.logger.statement(\n",
    "                            \"OK - using spatial reference \" \"in parent object.\"\n",
    "                        )\n",
    "                        spatial_reference = self.spatial_reference\n",
    "                        spatial_reference_type = spatial_reference.grid_type\n",
    "                    else:\n",
    "                        # uhoh\n",
    "                        self.logger.lraise(\n",
    "                            \"No spatial reference in parent \"\n",
    "                            \"object either. \"\n",
    "                            \"Can't set-up pilotpoints\"\n",
    "                        )\n",
    "                # check that spatial reference lines up with the original array dimensions\n",
    "                structured = False\n",
    "                if not isinstance(spatial_reference, dict):\n",
    "                    structured = True\n",
    "                    for mod_file, ar in file_dict.items():\n",
    "                        orgdata = ar.shape\n",
    "                        if spatial_reference_type == 'vertex':\n",
    "                            assert orgdata[0] == spatial_reference.ncpl, (\n",
    "                                \"Spatial reference ncpl not equal to original data ncpl for\\n\"\n",
    "                                + os.path.join(\n",
    "                                    *os.path.split(self.original_file_d)[1:], mod_file\n",
    "                                )\n",
    "                            )\n",
    "                        else:\n",
    "                            assert orgdata[0] == spatial_reference.nrow, (\n",
    "                                \"Spatial reference nrow not equal to original data nrow for\\n\"\n",
    "                                + os.path.join(\n",
    "                                    *os.path.split(self.original_file_d)[1:], mod_file\n",
    "                                )\n",
    "                            )\n",
    "                            assert orgdata[1] == spatial_reference.ncol, (\n",
    "                                \"Spatial reference ncol not equal to original data ncol for\\n\"\n",
    "                                + os.path.join(\n",
    "                                    *os.path.split(self.original_file_d)[1:], mod_file\n",
    "                                )\n",
    "                            )\n",
    "                # (stolen from helpers.PstFromFlopyModel()._pp_prep())\n",
    "                # but only settting up one set of pps at a time\n",
    "                pnb = par_name_base[0]\n",
    "                pnb = \"pname:{1}_ptype:pp_pstyle:{0}\".format(par_style, pnb)\n",
    "                pp_dict = {0: pnb}\n",
    "                pp_filename = \"{0}pp.dat\".format(par_name_store)\n",
    "                # pst inputfile (for tpl->in pair) is\n",
    "                # par_name_storepp.dat table (in pst ws)\n",
    "                in_filepst = pp_filename\n",
    "                pp_filename_dict = {pnb: in_filepst}\n",
    "                tpl_filename = self.tpl_d / (pp_filename + \".tpl\")\n",
    "                # tpl_filename = get_relative_filepath(self.new_d, tpl_filename)\n",
    "                pp_locs = None\n",
    "                if pp_space is None:  # default spacing if not passed\n",
    "                    self.logger.warn(\"pp_space is None, using 10...\\n\")\n",
    "                    pp_space = 10\n",
    "                else:\n",
    "                    if not use_pp_zones and (isinstance(pp_space, (int, np.integer))):\n",
    "                        # if not using pp zones will set up pp for just one\n",
    "                        # zone (all non zero) -- for active domain...\n",
    "                        if zone_array is None:\n",
    "                            nr, nc = file_dict[list(file_dict.keys())[0]].shape\n",
    "                            zone_array = np.ones((nr,nc))                        \n",
    "                        zone_array[zone_array > 0] = 1  # so can set all\n",
    "                        # gt-zero to 1\n",
    "                    if isinstance(pp_space, float):\n",
    "                        pp_space = int(pp_space)\n",
    "                    elif isinstance(pp_space, (int, np.integer)):\n",
    "                        pass\n",
    "                    elif isinstance(pp_space, str):\n",
    "                        if pp_space.lower().strip().endswith(\".csv\"):\n",
    "                            self.logger.statement(\n",
    "                                \"trying to load pilot point location info from csv file '{0}'\".format(\n",
    "                                    self.new_d / Path(pp_space)\n",
    "                                )\n",
    "                            )\n",
    "                            pp_locs = pd.read_csv(self.new_d / pp_space)\n",
    "\n",
    "                        elif pp_space.lower().strip().endswith(\".shp\"):\n",
    "                            self.logger.statement(\n",
    "                                \"trying to load pilot point location info from shapefile '{0}'\".format(\n",
    "                                    self.new_d / Path(pp_space)\n",
    "                                )\n",
    "                            )\n",
    "                            pp_locs = pyemu.pp_utils.pilot_points_from_shapefile(\n",
    "                                str(self.new_d / Path(pp_space))\n",
    "                            )\n",
    "                        else:\n",
    "                            self.logger.statement(\n",
    "                                \"trying to load pilot point location info from pilot point file '{0}'\".format(\n",
    "                                    self.new_d / Path(pp_space)\n",
    "                                )\n",
    "                            )\n",
    "                            pp_locs = pyemu.pp_utils.pp_file_to_dataframe(\n",
    "                                self.new_d / pp_space\n",
    "                            )\n",
    "                        self.logger.statement(\n",
    "                            \"pilot points found in file '{0}' will be transferred to '{1}' for parameterization\".format(\n",
    "                                pp_space, pp_filename\n",
    "                            )\n",
    "                        )\n",
    "                    elif isinstance(pp_space, pd.DataFrame):\n",
    "                        pp_locs = pp_space\n",
    "                    else:\n",
    "                        self.logger.lraise(\n",
    "                            \"unrecognized 'pp_space' value, should be int, csv file, pp file or dataframe, not '{0}'\".format(\n",
    "                                type(pp_space)\n",
    "                            )\n",
    "                        )\n",
    "                    if pp_locs is not None:\n",
    "                        cols = pp_locs.columns.tolist()\n",
    "                        if \"name\" not in cols:\n",
    "                            self.logger.lraise(\"'name' col not found in pp dataframe\")\n",
    "                        if \"x\" not in cols:\n",
    "                            self.logger.lraise(\"'x' col not found in pp dataframe\")\n",
    "                        if \"y\" not in cols:\n",
    "                            self.logger.lraise(\"'y' col not found in pp dataframe\")\n",
    "                        if \"zone\" not in cols:\n",
    "                            self.logger.warn(\n",
    "                                \"'zone' col not found in pp dataframe, adding generic zone\"\n",
    "                            )\n",
    "                            pp_locs.loc[:, \"zone\"] = 1\n",
    "                        elif zone_array is not None:\n",
    "                            # check that all the zones in the pp df are in the zone array\n",
    "                            missing = []\n",
    "                            for uz in pp_locs.zone.unique():\n",
    "                                if int(uz) not in zone_array:\n",
    "                                    missing.append(str(uz))\n",
    "                            if len(missing) > 0:\n",
    "                                self.logger.lraise(\n",
    "                                    \"the following pp zone values were not found in the zone array: {0}\".format(\n",
    "                                        \",\".join(missing)\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "                            for uz in np.unique(zone_array):\n",
    "                                if uz < 1:\n",
    "                                    continue\n",
    "                                if uz not in pp_locs.zone.values:\n",
    "\n",
    "                                    missing.append(str(uz))\n",
    "                            if len(missing) > 0:\n",
    "                                self.logger.warn(\n",
    "                                    \"the following zones don't have any pilot points:{0}\".format(\n",
    "                                        \",\".join(missing)\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "                if not structured and pp_locs is None:\n",
    "                    self.logger.lraise(\n",
    "                        \"pilot point type parameters with an unstructured grid requires 'pp_space' \"\n",
    "                        \"contain explict pilot point information\"\n",
    "                    )\n",
    "\n",
    "                if not structured and zone_array is not None:\n",
    "                    # self.logger.lraise(\"'zone_array' not supported for unstructured grids and pilot points\")\n",
    "                    if \"zone\" not in pp_locs.columns:\n",
    "                        self.logger.lraise(\n",
    "                            \"'zone' not found in pp info dataframe and 'zone_array' passed\"\n",
    "                        )\n",
    "                    uvals = np.unique(zone_array)\n",
    "                    zvals = set([int(z) for z in pp_locs.zone.tolist()])\n",
    "                    missing = []\n",
    "                    for uval in uvals:\n",
    "                        if int(uval) not in zvals and int(uval) != 0:\n",
    "                            missing.append(str(int(uval)))\n",
    "                    if len(missing) > 0:\n",
    "                        self.logger.warn(\n",
    "                            \"the following values in the zone array were not found in the pp info: {0}\".format(\n",
    "                                \",\".join(missing)\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                if geostruct is None:  # need a geostruct for pilotpoints\n",
    "\n",
    "                    # can use model default, if provided\n",
    "                    if self.geostruct is None:  # but if no geostruct passed...\n",
    "                        if not structured:\n",
    "                            self.logger.lraise(\n",
    "                                \"pilot point type parameters with an unstructured grid requires an\"\n",
    "                                \" explicit `geostruct` arg be passed to either PstFrom or add_parameters()\"\n",
    "                            )\n",
    "                        self.logger.warn(\n",
    "                            \"pp_geostruct is None,\"\n",
    "                            \"using ExpVario with contribution=1 \"\n",
    "                            \"and a=(pp_space*max(delr,delc))\"\n",
    "                        )\n",
    "                        # set up a default - could probably do something better if pp locs are passed\n",
    "                        if not isinstance(pp_space, (int, np.integer)):\n",
    "                            space = 10\n",
    "                        else:\n",
    "                            space = pp_space\n",
    "                        pp_dist = space * float(\n",
    "                            max(\n",
    "                                spatial_reference.delr.max(),\n",
    "                                spatial_reference.delc.max(),\n",
    "                            )\n",
    "                        )\n",
    "                        v = pyemu.geostats.ExpVario(contribution=1.0, a=pp_dist)\n",
    "                        pp_geostruct = pyemu.geostats.GeoStruct(\n",
    "                            variograms=v, name=\"pp_geostruct\", transform=transform\n",
    "                        )\n",
    "                    else:\n",
    "                        pp_geostruct = self.geostruct\n",
    "                        if pp_geostruct.transform != transform:\n",
    "                            self.logger.warn(\n",
    "                                \"0) Inconsistency between \"\n",
    "                                \"pp_geostruct transform and \"\n",
    "                                \"partrans.\"\n",
    "                            )\n",
    "                            self.logger.warn(\n",
    "                                \"1) Setting pp_geostruct transform \"\n",
    "                                \"to {0}\".format(transform)\n",
    "                            )\n",
    "                            self.logger.warn(\n",
    "                                \"2) This will create a new copy of \" \"pp_geostruct\"\n",
    "                            )\n",
    "                            self.logger.warn(\n",
    "                                \"3) Better to pass an appropriately \"\n",
    "                                \"transformed geostruct\"\n",
    "                            )\n",
    "                            pp_geostruct = copy.copy(pp_geostruct)\n",
    "                            pp_geostruct.transform = transform\n",
    "                else:\n",
    "                    pp_geostruct = geostruct\n",
    "\n",
    "                if pp_locs is None:\n",
    "                    # Set up pilot points\n",
    "\n",
    "                    df = pyemu.pp_utils.setup_pilotpoints_grid(\n",
    "                        sr=spatial_reference,\n",
    "                        ibound=zone_array,\n",
    "                        use_ibound_zones=use_pp_zones,\n",
    "                        prefix_dict=pp_dict,\n",
    "                        every_n_cell=pp_space,\n",
    "                        pp_dir=self.new_d,\n",
    "                        tpl_dir=self.tpl_d,\n",
    "                        shapename=str(self.new_d / \"{0}.shp\".format(par_name_store)),\n",
    "                        pp_filename_dict=pp_filename_dict,\n",
    "                    )\n",
    "                else:\n",
    "\n",
    "                    df = pyemu.pp_utils.pilot_points_to_tpl(\n",
    "                        pp_locs,\n",
    "                        tpl_filename,\n",
    "                        pnb,\n",
    "                    )\n",
    "                df.loc[:, \"pargp\"] = pargp\n",
    "                df.set_index(\"parnme\", drop=False, inplace=True)\n",
    "                # df includes most of the par info for par_dfs and also for\n",
    "                # relate_parfiles\n",
    "                self.logger.statement(\n",
    "                    \"{0} pilot point parameters created\".format(df.shape[0])\n",
    "                )\n",
    "                # should be only one group at a time\n",
    "                pargp = df.pargp.unique()\n",
    "                self.logger.statement(\"pilot point 'pargp':{0}\".format(\",\".join(pargp)))\n",
    "                self.logger.log(\"setting up pilot point parameters\")\n",
    "\n",
    "                # Calculating pp factors\n",
    "                pg = pargp[0]\n",
    "                # this reletvively quick\n",
    "                ok_pp = pyemu.geostats.OrdinaryKrige(pp_geostruct, df)\n",
    "                # build krige reference information on the fly - used to help\n",
    "                # prevent unnecessary krig factor calculation\n",
    "                pp_info_dict = {\n",
    "                    \"pp_data\": ok_pp.point_data.loc[:, [\"x\", \"y\", \"zone\"]],\n",
    "                    \"cov\": ok_pp.point_cov_df,\n",
    "                    \"zn_ar\": zone_array,\n",
    "                    \"sr\": spatial_reference,\n",
    "                    \"pstyle\":par_style,\n",
    "                    \"transform\":transform\n",
    "                }\n",
    "                fac_processed = False\n",
    "                for facfile, info in self._pp_facs.items():  # check against\n",
    "                    # factors already calculated\n",
    "                    if (\n",
    "                        info[\"pp_data\"].equals(pp_info_dict[\"pp_data\"])\n",
    "                        and info[\"cov\"].equals(pp_info_dict[\"cov\"])\n",
    "                        and np.array_equal(info[\"zn_ar\"], pp_info_dict[\"zn_ar\"])\n",
    "                        and pp_info_dict[\"pstyle\"] == info[\"pstyle\"]\n",
    "                        and pp_info_dict[\"transform\"] == info[\"transform\"]\n",
    "\n",
    "                    ):\n",
    "                        if type(info[\"sr\"]) == type(spatial_reference):\n",
    "                            if isinstance(spatial_reference, dict):\n",
    "                                if len(info[\"sr\"]) != len(spatial_reference):\n",
    "                                    continue\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                        fac_processed = True  # don't need to re-calc same factors\n",
    "                        fac_filename = facfile  # relate to existing fac file\n",
    "                        self.logger.statement(\"reusing factors\")\n",
    "                        break\n",
    "                if not fac_processed:\n",
    "                    # TODO need better way of naming sequential fac_files?\n",
    "                    self.logger.log(\"calculating factors for pargp={0}\".format(pg))\n",
    "                    fac_filename = self.new_d / \"{0}pp.fac\".format(par_name_store)\n",
    "                    var_filename = fac_filename.with_suffix(\".var.dat\")\n",
    "                    self.logger.statement(\n",
    "                        \"saving krige variance file:{0}\".format(var_filename)\n",
    "                    )\n",
    "                    self.logger.statement(\n",
    "                        \"saving krige factors file:{0}\".format(fac_filename)\n",
    "                    )\n",
    "                    # store info on pilotpoints\n",
    "                    self._pp_facs[fac_filename] = pp_info_dict\n",
    "                    # this is slow (esp on windows) so only want to do this\n",
    "                    # when required\n",
    "                    if structured:\n",
    "                        ok_pp.calc_factors_grid(\n",
    "                            spatial_reference,\n",
    "                            var_filename=var_filename,\n",
    "                            zone_array=zone_array,\n",
    "                            num_threads=10,\n",
    "                        )\n",
    "                        ok_pp.to_grid_factors_file(fac_filename)\n",
    "                    else:\n",
    "                        # put the sr dict info into a df\n",
    "                        # but we only want to use the n\n",
    "                        if zone_array is not None:\n",
    "                            for zone in np.unique(zone_array):\n",
    "                                if int(zone) == 0:\n",
    "                                    continue\n",
    "\n",
    "                                data = []\n",
    "                                for node, (x, y) in spatial_reference.items():\n",
    "                                    if zone_array[0, node] == zone:\n",
    "                                        data.append([node, x, y])\n",
    "                                if len(data) == 0:\n",
    "                                    continue\n",
    "                                node_df = pd.DataFrame(data, columns=[\"node\", \"x\", \"y\"])\n",
    "                                ok_pp.calc_factors(\n",
    "                                    node_df.x,\n",
    "                                    node_df.y,\n",
    "                                    num_threads=1,\n",
    "                                    pt_zone=zone,\n",
    "                                    idx_vals=node_df.node.astype(int),\n",
    "                                )\n",
    "                            ok_pp.to_grid_factors_file(\n",
    "                                fac_filename, ncol=len(spatial_reference)\n",
    "                            )\n",
    "                        else:\n",
    "                            data = []\n",
    "                            for node, (x, y) in spatial_reference.items():\n",
    "                                data.append([node, x, y])\n",
    "                            node_df = pd.DataFrame(data, columns=[\"node\", \"x\", \"y\"])\n",
    "                            ok_pp.calc_factors(node_df.x, node_df.y, num_threads=10)\n",
    "                            ok_pp.to_grid_factors_file(\n",
    "                                fac_filename, ncol=node_df.shape[0]\n",
    "                            )\n",
    "\n",
    "                    self.logger.log(\"calculating factors for pargp={0}\".format(pg))\n",
    "            # TODO - other par types - JTW?\n",
    "            elif par_type == \"kl\":\n",
    "                self.logger.lraise(\"array type 'kl' not implemented\")\n",
    "            else:\n",
    "                self.logger.lraise(\n",
    "                    \"unrecognized 'par_type': '{0}', \"\n",
    "                    \"should be in \"\n",
    "                    \"['constant','zone','grid','pilotpoints',\"\n",
    "                    \"'kl'\"\n",
    "                )\n",
    "            self.logger.log(\n",
    "                \"writing array-based template file \" \"'{0}'\".format(tpl_filename)\n",
    "            )\n",
    "\n",
    "        if datetime is not None:\n",
    "            # add time info to par_dfs\n",
    "            df[\"datetime\"] = datetime\n",
    "            df[\"timedelta\"] = t_offest\n",
    "        # accumulate information that relates mult_files (set-up here and\n",
    "        # eventually filled by PEST) to actual model files so that actual\n",
    "        # model input file can be generated\n",
    "        # (using helpers.apply_list_and_array_pars())\n",
    "        zone_filename = None\n",
    "        if zone_array is not None and zone_array.ndim < 3:\n",
    "            # zone_filename = tpl_filename.replace(\".tpl\",\".zone\")\n",
    "            zone_filename = Path(str(tpl_filename).replace(\".tpl\", \".zone\"))\n",
    "            self.logger.statement(\n",
    "                \"saving zone array {0} for tpl file {1}\".format(\n",
    "                    zone_filename, tpl_filename\n",
    "                )\n",
    "            )\n",
    "            np.savetxt(zone_filename, zone_array, fmt=\"%4d\")\n",
    "            zone_filename = zone_filename.name\n",
    "\n",
    "        relate_parfiles = []\n",
    "        for mod_file, pdf in file_dict.items():\n",
    "            mult_dict = {\n",
    "                \"org_file\": Path(self.original_file_d.name, mod_file.name),\n",
    "                \"model_file\": mod_file,\n",
    "                \"use_cols\": use_cols,\n",
    "                \"index_cols\": index_cols,\n",
    "                \"fmt\": fmt_dict[mod_file],\n",
    "                \"sep\": sep_dict[mod_file],\n",
    "                \"head_rows\": skip_dict[mod_file],\n",
    "                \"upper_bound\": ult_ubound,\n",
    "                \"lower_bound\": ult_lbound,\n",
    "                \"operator\": par_style,\n",
    "            }\n",
    "            if nxs:\n",
    "                mult_dict[\"chkpar\"] = nxs[mod_file]\n",
    "            if par_style in [\"m\", \"a\"]:\n",
    "                mult_dict[\"mlt_file\"] = Path(self.mult_file_d.name, mlt_filename)\n",
    "\n",
    "            if pp_filename is not None:\n",
    "                # if pilotpoint need to store more info\n",
    "                assert fac_filename is not None, \"missing pilot-point input filename\"\n",
    "                mult_dict[\"fac_file\"] = os.path.relpath(fac_filename, self.new_d)\n",
    "                mult_dict[\"pp_file\"] = pp_filename\n",
    "                if transform == \"log\":\n",
    "                    mult_dict[\"pp_fill_value\"] = 1.0\n",
    "                    mult_dict[\"pp_lower_limit\"] = 1.0e-30\n",
    "                    mult_dict[\"pp_upper_limit\"] = 1.0e30\n",
    "                else:\n",
    "                    mult_dict[\"pp_fill_value\"] = 0.0\n",
    "                    mult_dict[\"pp_lower_limit\"] = -1.0e30\n",
    "                    mult_dict[\"pp_upper_limit\"] = 1.0e30\n",
    "            if zone_filename is not None:\n",
    "                mult_dict[\"zone_file\"] = zone_filename\n",
    "            relate_parfiles.append(mult_dict)\n",
    "        relate_pars_df = pd.DataFrame(relate_parfiles)\n",
    "        # store on self for use in pest build etc\n",
    "        self._parfile_relations.append(relate_pars_df)\n",
    "\n",
    "        # add cols required for pst.parameter_data\n",
    "        df.loc[:, \"partype\"] = par_type\n",
    "        df.loc[:, \"partrans\"] = transform\n",
    "        df.loc[:, \"parubnd\"] = upper_bound\n",
    "        df.loc[:, \"parlbnd\"] = lower_bound\n",
    "        if par_style != \"d\":\n",
    "            df.loc[:, \"parval1\"] = initial_value\n",
    "        # df.loc[:,\"tpl_filename\"] = tpl_filename\n",
    "\n",
    "        # store tpl --> in filename pair\n",
    "        self.tpl_filenames.append(get_relative_filepath(self.new_d, tpl_filename))\n",
    "        self.input_filenames.append(in_filepst)\n",
    "        for file_name in file_dict.keys():\n",
    "            # store mult --> original file pairs\n",
    "            self.org_files.append(file_name)\n",
    "            self.mult_files.append(mlt_filename)\n",
    "\n",
    "        # add pars to par_data list BH: is this what we want?\n",
    "        # - BH: think we can get away with dropping duplicates?\n",
    "        missing = set(par_data_cols) - set(df.columns)\n",
    "        for field in missing:  # fill missing pst.parameter_data cols with defaults\n",
    "            df[field] = pyemu.pst_utils.pst_config[\"par_defaults\"][field]\n",
    "        df = df.drop_duplicates(subset=\"parnme\")  # drop pars that appear multiple times\n",
    "        # df = df.loc[:, par_data_cols]  # just storing pst required cols\n",
    "        # - need to store more for cov builder (e.g. x,y)\n",
    "        # TODO - check when self.par_dfs gets used\n",
    "        #  if constructing struct_dict here....\n",
    "        #  - possibly not necessary to store\n",
    "        # only add pardata for new parameters\n",
    "        # some parameters might already be in a par_df if they occur\n",
    "        # in more than one instance (layer, for example)\n",
    "        new_parnmes = set(df['parnme']).difference(self.unique_parnmes)\n",
    "        df = df.loc[df['parnme'].isin(new_parnmes)].copy()\n",
    "        self.par_dfs.append(df)\n",
    "        self.unique_parnmes.update(new_parnmes)\n",
    "        # pivot df to list of df per par group in this call\n",
    "        # (all groups will be related to same geostruct)\n",
    "        # TODO maybe use different marker to denote a relationship between pars\n",
    "        #  at the moment relating pars using common geostruct and pargp but may\n",
    "        #  want to reserve pargp for just PEST\n",
    "        if \"covgp\" not in df.columns:\n",
    "            gp_dict = {g: [d] for g, d in df.groupby(\"pargp\")}\n",
    "        else:\n",
    "            gp_dict = {g: [d] for g, d in df.groupby(\"covgp\")}\n",
    "        # df_list = [d for g, d in df.groupby('pargp')]\n",
    "        if geostruct is not None and (\n",
    "            par_type.lower() not in [\"constant\", \"zone\"] or datetime is not None\n",
    "        ):\n",
    "            # relating pars to geostruct....\n",
    "            if geostruct not in self.par_struct_dict.keys():\n",
    "                # add new geostruct\n",
    "                self.par_struct_dict[geostruct] = gp_dict\n",
    "                # self.par_struct_dict_l[geostruct] = list(gp_dict.values())\n",
    "            else:\n",
    "                # append group to appropriate key associated with this geostruct\n",
    "                # this is how pars setup with different calls are collected\n",
    "                # so their correlations can be tracked\n",
    "                for gp, gppars in gp_dict.items():\n",
    "                    # if group not already set up\n",
    "                    if gp not in self.par_struct_dict[geostruct].keys():\n",
    "                        # update dict entry with new {key:par} pair\n",
    "                        self.par_struct_dict[geostruct].update({gp: gppars})\n",
    "                    else:\n",
    "                        # if gp already assigned to this geostruct append par\n",
    "                        # list to approprate group key\n",
    "                        self.par_struct_dict[geostruct][gp].extend(gppars)\n",
    "                # self.par_struct_dict_l[geostruct].extend(list(gp_dict.values()))\n",
    "        else:  # TODO some rules for if geostruct is not passed....\n",
    "            if \"x\" in df.columns:\n",
    "                pass\n",
    "                #  TODO warn that it looks like spatial pars but no geostruct?\n",
    "            # if self.geostruct is not None:\n",
    "            #     geostruct = self.geostruct\n",
    "            # elif pp_geostruct is not None:\n",
    "            #     geostruct = pp_geostruct\n",
    "            # else:\n",
    "            #     TODO - do we need an error or warning and define a default?\n",
    "            #     options:\n",
    "            # if spatial_reference is None:\n",
    "            #     spatial_reference = self.spatial_reference  # TODO placeholder for now. but this needs improving, sr and self.sr might be None\n",
    "            # dist = 10 * float(\n",
    "            #             max(spatial_reference.delr.max(),\n",
    "            #                 spatial_reference.delc.max()))\n",
    "            # v = pyemu.geostats.ExpVario(contribution=1.0, a=dist)\n",
    "            # geostruct = pyemu.geostats.GeoStruct(\n",
    "            #     variograms=v)\n",
    "            # temporal default:\n",
    "            # v = pyemu.geostats.ExpVario(contribution=1.0, a=180.0)  # 180 correlation length\n",
    "            # geostruct = pyemu.geostats.GeoStruct(\n",
    "            #     variograms=v)\n",
    "\n",
    "        self.logger.log(\n",
    "            \"adding {0} type {1} style parameters for file(s) {2}\".format(\n",
    "                par_type, par_style, [str(f) for f in filenames]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if rebuild_pst:  # may want to just update pst and rebuild\n",
    "            # (with new relations)\n",
    "            if self.pst is not None:\n",
    "                self.logger.log(\"Adding pars to control file \" \"and rewriting pst\")\n",
    "                self.build_pst(filename=self.pst.filename, update=\"pars\")\n",
    "            else:\n",
    "                self.build_pst(filename=self.pst.filename, update=False)\n",
    "                self.logger.warn(\n",
    "                    \"pst object not available, \" \"new control file will be written\"\n",
    "                )\n",
    "        return df\n",
    "\n",
    "    def _load_listtype_file(\n",
    "        self, filename, index_cols, use_cols, fmt=None, sep=None, skip=None, c_char=None\n",
    "    ):\n",
    "        if isinstance(filename, list):\n",
    "            assert len(filename) == 1\n",
    "            filename = filename[0]  # should only ever be one passed\n",
    "        if isinstance(fmt, list):\n",
    "            assert len(fmt) == 1\n",
    "            fmt = fmt[0]\n",
    "        if isinstance(sep, list):\n",
    "            assert len(sep) == 1\n",
    "            sep = sep[0]\n",
    "        if isinstance(skip, list):\n",
    "            assert len(skip) == 1\n",
    "            skip = skip[0]\n",
    "\n",
    "        # trying to use use_cols and index_cols to work out whether to\n",
    "        # read header from csv.\n",
    "        # either use_cols or index_cols could still be None\n",
    "        # -- case of both being None should already have been caught\n",
    "        # but index_cols could still be None...\n",
    "\n",
    "        check_args = [a for a in [index_cols, use_cols] if a is not None]\n",
    "        # `a` should be list if it is not None\n",
    "        if all(isinstance(a[0], str) for a in check_args):\n",
    "            # index_cols can be from header str\n",
    "            header = 0  # will need to read a header\n",
    "        elif all(isinstance(a[0], (int, np.integer)) for a in check_args):\n",
    "            # index_cols are column numbers in input file\n",
    "            header = None\n",
    "        else:\n",
    "            if len(check_args) > 1:\n",
    "                #  implies neither are None but they either both are not str,int\n",
    "                #  or are different\n",
    "                self.logger.lraise(\n",
    "                    \"unrecognized type for index_cols or use_cols \"\n",
    "                    \"should be str or int and both should be of the \"\n",
    "                    \"same type, not {0} or {1}\".format(\n",
    "                        *[str(type(a[0])) for a in check_args]\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # implies not correct type\n",
    "                self.logger.lraise(\n",
    "                    \"unrecognized type for either index_cols or use_cols \"\n",
    "                    \"should be str or int, not {0}\".format(type(check_args[0][0]))\n",
    "                )\n",
    "\n",
    "        # checking no overlap between index_cols and use_cols\n",
    "        if len(check_args) > 1:\n",
    "            si = set(index_cols)\n",
    "            su = set(use_cols)\n",
    "\n",
    "            i = si.intersection(su)\n",
    "            if len(i) > 0:\n",
    "                self.logger.lraise(f\"use_cols also listed in index_cols: {str(i)}\")\n",
    "\n",
    "        file_path = self.new_d / filename\n",
    "        if not os.path.exists(file_path):\n",
    "            self.logger.lraise(f\"par/obs filename '{file_path}' not found \")\n",
    "        self.logger.log(f\"reading list-style file: {file_path}\")\n",
    "        if fmt.lower() == \"free\":\n",
    "            if sep is None:\n",
    "                sep = r\"\\s+\"\n",
    "                if Path(filename).suffix == \".csv\":\n",
    "                    sep = \",\"\n",
    "        else:\n",
    "            # TODO support reading fixed-format\n",
    "            #  (based on value of fmt passed)\n",
    "            #  ... or not?\n",
    "            self.logger.warn(\n",
    "                \"0) Only reading free format list-style par \"\n",
    "                \"files currently supported.\"\n",
    "            )\n",
    "            self.logger.warn(\"1) Assuming safe to read as whitespace delim.\")\n",
    "            self.logger.warn(\"2) Desired format string will still be passed through\")\n",
    "            sep = r\"\\s+\"\n",
    "        try:\n",
    "            # read each input file\n",
    "            if skip > 0 or c_char is not None:\n",
    "                if c_char is None:\n",
    "                    with open(file_path, \"r\") as fp:\n",
    "                        storehead = {\n",
    "                            lp: line for lp, line in enumerate(fp) if lp < skip\n",
    "                        }\n",
    "                else:\n",
    "                    with open(file_path, \"r\") as fp:\n",
    "                        storehead = {\n",
    "                            lp: line\n",
    "                            for lp, line in enumerate(fp)\n",
    "                            if lp < skip or line.strip().startswith(c_char)\n",
    "                        }\n",
    "            else:\n",
    "                storehead = {}\n",
    "        except TypeError:\n",
    "            c_char = skip\n",
    "            skip = None\n",
    "            with open(file_path, \"r\") as fp:\n",
    "                storehead = {\n",
    "                    lp: line\n",
    "                    for lp, line in enumerate(fp)\n",
    "                    if line.strip().startswith(c_char)\n",
    "                }\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            comment=c_char,\n",
    "            sep=sep,\n",
    "            skiprows=skip,\n",
    "            header=header,\n",
    "            low_memory=False,\n",
    "            dtype='object'\n",
    "        )\n",
    "        self.logger.log(f\"reading list-style file: {file_path}\")\n",
    "        # ensure that column ids from index_col is in input file\n",
    "        missing = []\n",
    "        for index_col in index_cols:\n",
    "            if index_col not in df.columns:\n",
    "                missing.append(index_col)\n",
    "            # df.loc[:, index_col] = df.loc[:, index_col].astype(np.int64) # TODO int? why?\n",
    "        if len(missing) > 0:\n",
    "            self.logger.lraise(\n",
    "                \"the following index_cols were not \"\n",
    "                \"found in file '{0}':{1}\"\n",
    "                \"\".format(file_path, str(missing))\n",
    "            )\n",
    "        # ensure requested use_cols are in input file\n",
    "        if use_cols is not None:\n",
    "            for use_col in use_cols:\n",
    "                if use_col not in df.columns:\n",
    "                    missing.append(use_cols)\n",
    "        if len(missing) > 0:\n",
    "            self.logger.lraise(\n",
    "                \"the following use_cols were not found \"\n",
    "                \"in file '{0}':{1}\"\n",
    "                \"\".format(file_path, str(missing))\n",
    "            )\n",
    "        return df, storehead, sep\n",
    "\n",
    "    def _prep_arg_list_lengths(\n",
    "        self,\n",
    "        filenames,\n",
    "        fmts=None,\n",
    "        seps=None,\n",
    "        skip_rows=None,\n",
    "        index_cols=None,\n",
    "        use_cols=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Private wrapper function to align filenames, formats, delimiters,\n",
    "        reading options and setup columns for passing sequentially to\n",
    "        load_listtype\n",
    "        Args:\n",
    "            filenames (`str`) or (`list`): names for files ot eventually read\n",
    "            fmts (`str`) or (`list`): of column formaters for input file.\n",
    "                If `None`, free-formatting is assumed\n",
    "            seps (`str`) or (`list`): column separator free formatter files.\n",
    "                If `None`, a list of `None`s is returned and the delimiter\n",
    "                is eventually governed by the file extension (`,` for .csv)\n",
    "            skip_rows (`str`) or (`list`): Number of rows in file header to not\n",
    "                form part of the dataframe\n",
    "            index_cols (`int`) or (`list`): Columns in tabular file to use as indicies\n",
    "            use_cols (`int`) or (`list`): Columns in tabular file to\n",
    "                use as par or obs cols\n",
    "        Returns:\n",
    "            algined lists of:\n",
    "            filenames, fmts, seps, skip_rows, index_cols, use_cols\n",
    "            for squentially passing to `_load_listtype_file()`\n",
    "\n",
    "        \"\"\"\n",
    "        if not isinstance(filenames, list):\n",
    "            filenames = [filenames]\n",
    "        if fmts is None:\n",
    "            fmts = [\"free\" for _ in filenames]\n",
    "        if not isinstance(fmts, list):\n",
    "            fmts = [fmts]\n",
    "        if len(fmts) != len(filenames):\n",
    "            self.logger.warn(\n",
    "                \"Discrepancy between number of filenames ({0}) \"\n",
    "                \"and number of formatter strings ({1}). \"\n",
    "                \"Will repeat first ({2})\"\n",
    "                \"\".format(len(filenames), len(fmts), fmts[0])\n",
    "            )\n",
    "            fmts = [fmts[0] for _ in filenames]\n",
    "        fmts = [\"free\" if fmt is None else fmt for fmt in fmts]\n",
    "        if seps is None:\n",
    "            seps = [None for _ in filenames]\n",
    "        if not isinstance(seps, list):\n",
    "            seps = [seps]\n",
    "        if len(seps) != len(filenames):\n",
    "            self.logger.warn(\n",
    "                \"Discrepancy between number of filenames ({0}) \"\n",
    "                \"and number of seps defined ({1}). \"\n",
    "                \"Will repeat first ({2})\"\n",
    "                \"\".format(len(filenames), len(seps), seps[0])\n",
    "            )\n",
    "            seps = [seps[0] for _ in filenames]\n",
    "        if skip_rows is None:\n",
    "            skip_rows = [None for _ in filenames]\n",
    "        if not isinstance(skip_rows, list):\n",
    "            skip_rows = [skip_rows]\n",
    "        if len(skip_rows) != len(filenames):\n",
    "            self.logger.warn(\n",
    "                \"Discrepancy between number of filenames ({0}) \"\n",
    "                \"and number of skip_rows defined ({1}). \"\n",
    "                \"Will repeat first ({2})\"\n",
    "                \"\".format(len(filenames), len(skip_rows), skip_rows[0])\n",
    "            )\n",
    "            skip_rows = [skip_rows[0] for _ in filenames]\n",
    "        skip_rows = [0 if s is None else s for s in skip_rows]\n",
    "\n",
    "        if index_cols is None and use_cols is not None:\n",
    "            self.logger.lraise(\n",
    "                \"index_cols is None, but use_cols is not ({0})\" \"\".format(str(use_cols))\n",
    "            )\n",
    "\n",
    "        if index_cols is not None:\n",
    "            if not isinstance(index_cols, list):\n",
    "                index_cols = [index_cols]\n",
    "        if use_cols is not None:\n",
    "            if not isinstance(use_cols, list):\n",
    "                use_cols = [use_cols]\n",
    "        return filenames, fmts, seps, skip_rows, index_cols, use_cols\n",
    "\n",
    "\n",
    "def write_list_tpl(\n",
    "    filenames,\n",
    "    dfs,\n",
    "    name,\n",
    "    tpl_filename,\n",
    "    index_cols,\n",
    "    par_type,\n",
    "    use_cols=None,\n",
    "    use_rows=None,\n",
    "    suffix=\"\",\n",
    "    zone_array=None,\n",
    "    gpname=None,\n",
    "    get_xy=None,\n",
    "    ij_in_idx=None,\n",
    "    xy_in_idx=None,\n",
    "    zero_based=True,\n",
    "    input_filename=None,\n",
    "    par_style=\"m\",\n",
    "    headerlines=None,\n",
    "    fill_value=1.0,\n",
    "    logger=None,\n",
    "):\n",
    "    \"\"\"Write template files for a list style input.\n",
    "\n",
    "    Args:\n",
    "        filenames (`str` of `container` of `str`): original input filenames\n",
    "        dfs (`pandas.DataFrame` or `container` of pandas.DataFrames): pandas\n",
    "            representations of input file.\n",
    "        name (`str` or container of str): parameter name prefixes.\n",
    "            If more that one column to be parameterised, must be a container\n",
    "            of strings providing the prefix for the parameters in the\n",
    "            different columns.\n",
    "        tpl_filename (`str`): Path (from current execution directory)\n",
    "            for desired template file\n",
    "        index_cols (`list`): column names to use as indices in tabular\n",
    "            input dataframe\n",
    "        par_type (`str`): 'constant','zone', or 'grid' used in parname\n",
    "            generation. If `constant`, one par is set up for each `use_cols`.\n",
    "            If `zone`, one par is set up for each zone for each `use_cols`.\n",
    "            If `grid`, one par is set up for every unique index combination\n",
    "            (from `index_cols`) for each `use_cols`.\n",
    "        use_cols (`list`): Columns in tabular input file to paramerterise.\n",
    "            If None, pars are set up for all columns apart from index cols.\n",
    "        use_rows (`list` of `int` or `tuple`): Setup parameters for only\n",
    "            specific rows in list-style model input file.\n",
    "            If list of `int` -- assumed to be a row index selction (zero-based).\n",
    "            If list of `tuple` -- assumed to be selection based `index_cols`\n",
    "                values. e.g. [(3,5,6)] would attempt to set parameters where the\n",
    "                model file values for 3 `index_cols` are 3,5,6. N.B. values in\n",
    "                tuple are actual model file entry values. For use_rows with a\n",
    "                single 'index_cols' use [(3,),(5,),(6,)] to set parameters for\n",
    "                rows with model file index entries of 3,5,6.\n",
    "            If no rows in the model input file match `use_rows` -- parameters\n",
    "                will be set up for all rows.\n",
    "            Only valid/effective if index_cols is not None.\n",
    "            Default is None -- setup parameters for all rows.\n",
    "        suffix (`str`): Optional par name suffix\n",
    "        zone_array (`np.ndarray`): Array defining zone divisions.\n",
    "            If not None and `par_type` is `grid` or `zone` it is expected that\n",
    "            `index_cols` provide the indicies for\n",
    "            querying `zone_array`. Therefore, array dimension should equal\n",
    "            `len(index_cols)`.\n",
    "        get_xy (`pyemu.PstFrom` method): Can be specified to get real-world xy\n",
    "            from `index_cols` passed (to assist correlation definition)\n",
    "        ij_in_idx (`list` or `array`): defining which `index_cols` contain i,j\n",
    "        xy_in_idx (`list` or `array`): defining which `index_cols` contain x,y\n",
    "        zero_based (`boolean`): IMPORTANT - pass as False if `index_cols`\n",
    "            are NOT zero-based indicies (e.g. MODFLOW row/cols).\n",
    "            If False 1 with be subtracted from `index_cols`.\n",
    "        input_filename (`str`): Path to input file (paired with tpl file)\n",
    "        par_style (`str`): either 'd','a', or 'm'\n",
    "        headerlines ([`str`]): optional header lines in the original model file, used for\n",
    "            direct style parameters\n",
    "    Returns:\n",
    "        `pandas.DataFrame`: dataframe with info for the new parameters\n",
    "\n",
    "    Note:\n",
    "        This function is called by `PstFrom` programmatically\n",
    "\n",
    "    \"\"\"\n",
    "    # get dataframe with autogenerated parnames based on `name`, `index_cols`,\n",
    "    # `use_cols`, `suffix` and `par_type`\n",
    "    if par_style == \"d\":\n",
    "        df_tpl, nxs = _write_direct_df_tpl(\n",
    "            filenames[0],\n",
    "            tpl_filename,\n",
    "            dfs[0],\n",
    "            name,\n",
    "            index_cols,\n",
    "            par_type,\n",
    "            use_cols=use_cols,\n",
    "            use_rows=use_rows,\n",
    "            suffix=suffix,\n",
    "            gpname=gpname,\n",
    "            zone_array=zone_array,\n",
    "            get_xy=get_xy,\n",
    "            ij_in_idx=ij_in_idx,\n",
    "            xy_in_idx=xy_in_idx,\n",
    "            zero_based=zero_based,\n",
    "            headerlines=headerlines,\n",
    "            logger=logger,\n",
    "        )\n",
    "    else:\n",
    "        df_tpl = _get_tpl_or_ins_df(\n",
    "            dfs,\n",
    "            name,\n",
    "            index_cols,\n",
    "            par_type,\n",
    "            use_cols=use_cols,\n",
    "            suffix=suffix,\n",
    "            gpname=gpname,\n",
    "            zone_array=zone_array,\n",
    "            get_xy=get_xy,\n",
    "            ij_in_idx=ij_in_idx,\n",
    "            xy_in_idx=xy_in_idx,\n",
    "            zero_based=zero_based,\n",
    "            par_fill_value=fill_value,\n",
    "            par_style=par_style,\n",
    "        )\n",
    "        idxs = [[tuple(s) for s in df.loc[:, index_cols].values] for df in dfs]\n",
    "        use_rows, nxs = _get_use_rows(\n",
    "            df_tpl, idxs, use_rows, zero_based, tpl_filename, logger=logger\n",
    "        )\n",
    "        df_tpl = df_tpl.loc[use_rows, :]  # direct pars done in direct function\n",
    "        # can we just slice df_tpl here\n",
    "    for col in use_cols:  # corellations flagged using pargp\n",
    "        df_tpl[\"covgp{0}\".format(col)] = df_tpl.loc[:, \"pargp{0}\".format(col)].values\n",
    "    # needs modifying if colocated pars in same group\n",
    "    if par_type == \"grid\" and \"x\" in df_tpl.columns:\n",
    "        if df_tpl.duplicated([\"x\", \"y\"]).any():\n",
    "            # may need to use a different grouping for parameter correlations\n",
    "            # where parameter x and y values are the same but pars are not\n",
    "            # correlated (e.g. 2d correlation but different layers)\n",
    "            # - this will only work if `index_cols` contains a third dimension.\n",
    "            if len(index_cols) > 2:\n",
    "                third_d = index_cols.copy()\n",
    "                if xy_in_idx is not None:\n",
    "                    for idx in xy_in_idx:\n",
    "                        third_d.remove(index_cols[idx])\n",
    "                elif ij_in_idx is not None:\n",
    "                    for idx in ij_in_idx:\n",
    "                        third_d.remove(index_cols[idx])\n",
    "                else:  # if xy_in_idx and ij_in_idx ar None\n",
    "                    # then parse_kij assumes that i is at idx[-2] and j at idx[-1]\n",
    "                    third_d.pop()  # pops -1\n",
    "                    third_d.pop()  # pops -2\n",
    "                msg = (\n",
    "                    \"Coincidently located pars in list-style file, \"\n",
    "                    \"attempting to separate pars based on `index_cols` \"\n",
    "                    f\"passed - using index_col[{third_d[-1]}] \"\n",
    "                    f\"for third dimension\"\n",
    "                )\n",
    "                if logger is not None:\n",
    "                    logger.warn(msg)\n",
    "                else:\n",
    "                    PyemuWarning(msg)\n",
    "                for col in use_cols:\n",
    "                    df_tpl[\"covgp{0}\".format(col)] = df_tpl.loc[\n",
    "                        :, \"covgp{0}\".format(col)\n",
    "                    ].str.cat(\n",
    "                        pd.DataFrame(df_tpl.sidx.to_list()).iloc[:, 0].astype(str),\n",
    "                        \"_cov\",\n",
    "                    )\n",
    "            else:\n",
    "                msg = (\n",
    "                    \"Coincidently located pars in list-style file. \"\n",
    "                    \"Likely to cause issues building par cov or \"\n",
    "                    \"drawing par ensemble. Can be resolved by passing \"\n",
    "                    \"an additional `index_col` as a basis for \"\n",
    "                    \"splitting colocated correlations (e.g. Layer)\"\n",
    "                )\n",
    "                if logger is not None:\n",
    "                    logger.warn(msg)\n",
    "                else:\n",
    "                    PyemuWarning(msg)\n",
    "    # pull out par details where multiple `use_cols` are requested\n",
    "    parnme = list(df_tpl.loc[:, use_cols].values.flatten())\n",
    "    pargp = list(\n",
    "        df_tpl.loc[:, [\"pargp{0}\".format(col) for col in use_cols]].values.flatten()\n",
    "    )\n",
    "\n",
    "    covgp = list(\n",
    "        df_tpl.loc[:, [\"covgp{0}\".format(col) for col in use_cols]].values.flatten()\n",
    "    )\n",
    "    df_tpl = df_tpl.drop(\n",
    "        [col for col in df_tpl.columns if str(col).startswith(\"covgp\")], axis=1\n",
    "    )\n",
    "    df_par = pd.DataFrame(\n",
    "        {\"parnme\": parnme, \"pargp\": pargp, \"covgp\": covgp}, index=parnme\n",
    "    )\n",
    "\n",
    "    parval_cols = [c for c in df_tpl.columns if \"parval1\" in str(c)]\n",
    "    parval = df_tpl.loc[:, parval_cols].values.flatten().tolist()\n",
    "\n",
    "    if (\n",
    "        par_type == \"grid\" and \"x\" in df_tpl.columns\n",
    "    ):  # TODO work out if x,y needed for constant and zone pars too\n",
    "        df_par[\"x\"], df_par[\"y\"] = np.concatenate(\n",
    "            df_tpl.apply(lambda r: [[r.x, r.y] for _ in use_cols], axis=1).values\n",
    "        ).T\n",
    "    for use_col in use_cols:\n",
    "        df_tpl.loc[:, use_col] = df_tpl.loc[:, use_col].apply(\n",
    "            lambda x: \"~  {0}  ~\".format(x)\n",
    "        )\n",
    "    if par_style in [\"m\", \"a\"]:\n",
    "        pyemu.helpers._write_df_tpl(\n",
    "            filename=tpl_filename, df=df_tpl, sep=\",\", tpl_marker=\"~\"\n",
    "        )\n",
    "\n",
    "        if input_filename is not None:\n",
    "            df_in = df_tpl.copy()\n",
    "            df_in.loc[:, use_cols] = fill_value\n",
    "            df_in.to_csv(input_filename)\n",
    "    df_par.loc[:, \"tpl_filename\"] = tpl_filename\n",
    "    df_par.loc[:, \"input_filename\"] = input_filename\n",
    "    df_par.loc[:, \"parval1\"] = parval\n",
    "    return df_par, nxs\n",
    "\n",
    "\n",
    "def _write_direct_df_tpl(\n",
    "    in_filename,\n",
    "    tpl_filename,\n",
    "    df,\n",
    "    name,\n",
    "    index_cols,\n",
    "    typ,\n",
    "    use_cols=None,\n",
    "    use_rows=None,\n",
    "    suffix=\"\",\n",
    "    zone_array=None,\n",
    "    get_xy=None,\n",
    "    ij_in_idx=None,\n",
    "    xy_in_idx=None,\n",
    "    zero_based=True,\n",
    "    gpname=None,\n",
    "    headerlines=None,\n",
    "    logger=None,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Private method to auto-generate parameter or obs names from tabular\n",
    "    model files (input or output) read into pandas dataframes\n",
    "    Args:\n",
    "        tpl_filename (`str` ): template filename\n",
    "        df (`pandas.DataFrame`): DataFrame of list-style input file\n",
    "        name (`str`): Parameter name prefix\n",
    "        index_cols (`str` or `list`): columns of dataframes to use as indicies\n",
    "        typ (`str`): 'constant','zone', or 'grid' used in parname generation.\n",
    "            If `constant`, one par is set up for each `use_cols`.\n",
    "            If `zone`, one par is set up for each zone for each `use_cols`.\n",
    "            If `grid`, one par is set up for every unique index combination\n",
    "            (from `index_cols`) for each `use_cols`.\n",
    "        use_cols (`list`): Columns to parameterise. If None, pars are set up\n",
    "            for all columns apart from index cols\n",
    "        suffix (`str`): Optional par name suffix.\n",
    "        zone_array (`np.ndarray`): Array defining zone divisions.\n",
    "            If not None and `par_type` is `grid` or `zone` it is expected that\n",
    "            `index_cols` provide the indicies for querying `zone_array`.\n",
    "            Therefore, array dimension should equal `len(index_cols)`.\n",
    "        get_xy (`pyemu.PstFrom` method): Can be specified to get real-world xy\n",
    "            from `index_cols` passed (to include in obs/par name)\n",
    "        ij_in_idx (`list` or `array`): defining which `index_cols` contain i,j\n",
    "        xy_in_idx (`list` or `array`): defining which `index_cols` contain x,y\n",
    "        zero_based (`boolean`): IMPORTANT - pass as False if `index_cols`\n",
    "            are NOT zero-based indicies (e.g. MODFLOW row/cols).\n",
    "            If False 1 with be subtracted from `index_cols`.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame with paranme and pargroup define for each `use_col`\n",
    "\n",
    "    Note:\n",
    "        This function is called by `PstFrom` programmatically\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO much of this duplicates what is in _get_tpl_or_ins_df() -- could posssibly be consolidated\n",
    "    # work out the union of indices across all dfs\n",
    "\n",
    "    sidx = []\n",
    "\n",
    "    didx = df.loc[:, index_cols].apply(tuple, axis=1)\n",
    "    sidx.extend(didx)\n",
    "\n",
    "    df_ti = pd.DataFrame({\"sidx\": sidx}, columns=[\"sidx\"])\n",
    "    inames, fmt = _get_index_strfmt(index_cols)\n",
    "    df_ti = _get_index_strings(df_ti, fmt, zero_based)\n",
    "    df_ti = _getxy_from_idx(df_ti, get_xy, xy_in_idx, ij_in_idx)\n",
    "\n",
    "    df_ti, direct_tpl_df = _build_parnames(\n",
    "        df_ti,\n",
    "        typ,\n",
    "        zone_array,\n",
    "        index_cols,\n",
    "        use_cols,\n",
    "        name,\n",
    "        gpname,\n",
    "        suffix,\n",
    "        par_style=\"d\",\n",
    "        init_df=df,\n",
    "        init_fname=in_filename,\n",
    "    )\n",
    "    idxs = [tuple(s) for s in df.loc[:, index_cols].values]\n",
    "    use_rows, nxs = _get_use_rows(\n",
    "        df_ti, [idxs], use_rows, zero_based, tpl_filename, logger=logger\n",
    "    )\n",
    "    df_ti = df_ti.loc[use_rows]\n",
    "    not_rows = ~direct_tpl_df.index.isin(use_rows)\n",
    "    direct_tpl_df.loc[not_rows] = df.loc[not_rows, direct_tpl_df.columns]\n",
    "    if isinstance(direct_tpl_df.columns[0], str):\n",
    "        header = True\n",
    "    else:\n",
    "        header = False\n",
    "    pyemu.helpers._write_df_tpl(\n",
    "        tpl_filename, direct_tpl_df, index=False, header=header, headerlines=headerlines\n",
    "    )\n",
    "    return df_ti, nxs\n",
    "\n",
    "\n",
    "def _get_use_rows(tpldf, idxcolvals, use_rows, zero_based, fnme, logger=None):\n",
    "    \"\"\"\n",
    "    private function to get use_rows index within df based on passed use_rows\n",
    "    option, which could be in various forms...\n",
    "    Args:\n",
    "        tpldf:\n",
    "        idxcolvals:\n",
    "        use_rows:\n",
    "        zero_based:\n",
    "        fname:\n",
    "        logger:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    if use_rows is None:\n",
    "        use_rows = tpldf.index\n",
    "        nxs = [len(set(idx)) for idx in idxcolvals]\n",
    "        return use_rows, nxs\n",
    "    if np.ndim(use_rows) == 0:\n",
    "        use_rows = [use_rows]\n",
    "    if np.ndim(use_rows) == 1:  # assume we have collection of int that describe iloc\n",
    "        use_rows = [idx[i] for i in use_rows for idx in idxcolvals]\n",
    "    else:\n",
    "        use_rows = [tuple(r) for r in use_rows]\n",
    "    nxs = [len(set(use_rows).intersection(idx)) for idx in idxcolvals]\n",
    "    orig_use_rows = use_rows.copy()\n",
    "    if not zero_based:  # assume passed indicies are 1 based\n",
    "        use_rows = [\n",
    "            tuple([i - 1 if isinstance(i, (int, np.integer)) else i for i in r])\n",
    "            if not isinstance(r, str)\n",
    "            else r\n",
    "            for r in use_rows\n",
    "        ]\n",
    "    use_rows = set(use_rows)\n",
    "    sel = tpldf.sidx.isin(use_rows) | tpldf.idx_strs.isin(use_rows)\n",
    "    if not sel.any():  # use_rows must be ints\n",
    "        inidx = list(use_rows.intersection(tpldf.index))\n",
    "        missing = use_rows.difference(tpldf.index)\n",
    "        use_rows = tpldf.iloc[inidx].index.unique()\n",
    "    else:\n",
    "        missing = set(use_rows).difference(tpldf.sidx, tpldf.idx_strs)\n",
    "        use_rows = tpldf.loc[sel].index.unique()\n",
    "    if len(missing) > 0:\n",
    "        msg = (\n",
    "            \"write_list_tpl: Requested rows missing from parameter file, \"\n",
    "            f\"rows: {missing}, file: {fnme}.\"\n",
    "        )\n",
    "        if logger is not None:\n",
    "            logger.warn(msg)\n",
    "        else:\n",
    "            warnings.warn(msg, PyemuWarning)\n",
    "    if len(use_rows) == 0:\n",
    "        msg = (\n",
    "            \"write_list_tpl: None of request rows found in parameter file, \"\n",
    "            f\"rows: {orig_use_rows}, file: {fnme}. \"\n",
    "            \"Will set up pars for all rows.\"\n",
    "        )\n",
    "        if logger is not None:\n",
    "            logger.warn(msg)\n",
    "        else:\n",
    "            warnings.warn(msg, PyemuWarning)\n",
    "        use_rows = tpldf.index\n",
    "    return use_rows, nxs\n",
    "\n",
    "\n",
    "def _get_index_strfmt(index_cols):\n",
    "    # get some index strings for naming\n",
    "    j = \"_\"\n",
    "    # fmt = \"{0}|{1}\"\n",
    "    if isinstance(index_cols[0], str):\n",
    "        inames = index_cols\n",
    "    else:\n",
    "        inames = [\"idx{0}\".format(i) for i in range(len(index_cols))]\n",
    "    # full formatter string\n",
    "    fmt = j.join([f\"{iname}|{{{i}}}\" for i, iname in enumerate(inames)])\n",
    "    # else:\n",
    "    #     # fmt = \"{1:3}\"\n",
    "    #     j = \"\"\n",
    "    #     if isinstance(index_cols[0], str):\n",
    "    #         inames = index_cols\n",
    "    #     else:\n",
    "    #         inames = [\"{0}\".format(i) for i in range(len(index_cols))]\n",
    "    #     # full formatter string\n",
    "    #     fmt = j.join([f\"{{{i}:3}}\" for i, iname in enumerate(inames)])\n",
    "    return inames, fmt\n",
    "\n",
    "\n",
    "def _get_index_strings(df, fmt, zero_based):\n",
    "    if not zero_based:\n",
    "        # only if indices are ints (trying to support strings as par ids)\n",
    "        df.loc[:, \"sidx\"] = df.sidx.apply(\n",
    "            lambda x: tuple(xx - 1 if isinstance(xx, (int, np.integer)) else xx for xx in x)\n",
    "        )\n",
    "\n",
    "    df.loc[:, \"idx_strs\"] = df.sidx.apply(lambda x: fmt.format(*x)).str.replace(\" \", \"\")\n",
    "    df.loc[:, \"idx_strs\"] = df.idx_strs.str.replace(\":\", \"\", regex=False).str.lower()\n",
    "    df.loc[:, \"idx_strs\"] = df.idx_strs.str.replace(\"|\", \":\", regex=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _getxy_from_idx(df, get_xy, xy_in_idx, ij_in_idx):\n",
    "    if get_xy is None:\n",
    "        return df\n",
    "    if xy_in_idx is not None:\n",
    "        df[[\"x\", \"y\"]] = pd.DataFrame(df.sidx.to_list()).iloc[:, xy_in_idx]\n",
    "        return df\n",
    "\n",
    "    df.loc[:, \"xy\"] = df.sidx.apply(get_xy, ij_id=ij_in_idx)\n",
    "    df.loc[:, \"x\"] = df.xy.apply(lambda x: x[0])\n",
    "    df.loc[:, \"y\"] = df.xy.apply(lambda x: x[1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def _build_parnames(\n",
    "    df,\n",
    "    typ,\n",
    "    zone_array,\n",
    "    index_cols,\n",
    "    use_cols,\n",
    "    basename,\n",
    "    gpname,\n",
    "    suffix,\n",
    "    par_style,\n",
    "    init_df=None,\n",
    "    init_fname=None,\n",
    "    fill_value=1.0,\n",
    "):\n",
    "    if par_style == \"d\":\n",
    "        assert init_df is not None\n",
    "        direct_tpl_df = init_df.copy()\n",
    "        if typ == \"constant\":\n",
    "            assert init_fname is not None\n",
    "    if use_cols is None:\n",
    "        use_cols = [c for c in df.columns if c not in index_cols]\n",
    "        # if direct, we have more to deal with...\n",
    "    for iuc, use_col in enumerate(use_cols):\n",
    "        if not isinstance(basename, str):\n",
    "            nname = basename[iuc]\n",
    "            # if zone type, find the zones for each index position\n",
    "        else:\n",
    "            nname = basename\n",
    "        if zone_array is not None and typ in [\"zone\", \"grid\"]:\n",
    "            if zone_array.ndim != len(index_cols):\n",
    "                raise Exception(\n",
    "                    \"get_tpl_or_ins_df() error: \"\n",
    "                    \"zone_array.ndim \"\n",
    "                    \"({0}) != len(index_cols)({1})\"\n",
    "                    \"\".format(zone_array.ndim, len(index_cols))\n",
    "                )\n",
    "            df.loc[:, \"zval\"] = df.sidx.apply(lambda x: zone_array[x])\n",
    "\n",
    "        if gpname is None or gpname[iuc] is None:\n",
    "            ngpname = nname\n",
    "        else:\n",
    "            if not isinstance(gpname, str):\n",
    "                ngpname = gpname[iuc]\n",
    "            else:\n",
    "                ngpname = gpname\n",
    "        df.loc[:, \"pargp{}\".format(use_col)] = ngpname\n",
    "        df.loc[:, \"parval1_{0}\".format(use_col)] = fill_value\n",
    "        if typ == \"constant\":\n",
    "            # one par for entire use_col column\n",
    "            fmtr = \"pname:{0}_ptype:cn_usecol:{1}\"\n",
    "            fmtr += \"_pstyle:{0}\".format(par_style)\n",
    "            if suffix != \"\":\n",
    "                fmtr += f\"_{suffix}\"\n",
    "            # else:\n",
    "            #     fmtr = \"{0}{1}\"\n",
    "            #     if suffix != \"\":\n",
    "            #         fmtr += suffix\n",
    "            df.loc[:, use_col] = fmtr.format(nname, use_col)\n",
    "            if par_style == \"d\":\n",
    "                _check_diff(init_df.loc[:, use_col].values, init_fname)\n",
    "                df.loc[:, \"parval1_{0}\".format(use_col)] = init_df.loc[:, use_col][0]\n",
    "        elif typ == \"zone\":\n",
    "            # one par for each zone\n",
    "            fmtr = \"pname:{0}_ptype:zn_usecol:{1}\"\n",
    "            if par_style == \"d\":\n",
    "                # todo\n",
    "                raise NotImplementedError(\n",
    "                    \"list-style direct zone-type parameters not implemented\"\n",
    "                )\n",
    "                fmtr += \"_pstyle:d\"\n",
    "            else:\n",
    "                fmtr += \"_pstyle:m\"\n",
    "            if zone_array is not None:\n",
    "                fmtr += \"_zone:{2}\"\n",
    "                # df.loc[:, use_col] += df.zval.apply(\n",
    "                #     lambda x: \"_zone:{0}\".format(x)\n",
    "                # )\n",
    "            if suffix != \"\":\n",
    "                fmtr += f\"_{suffix}\"\n",
    "                # df.loc[:, use_col] += \"_{0}\".format(suffix)\n",
    "            # else:\n",
    "            #     fmtr = \"{0}{1}\"\n",
    "            #     if zone_array is not None:\n",
    "            #         fmtr += \"z{2}\"\n",
    "            #     if suffix != \"\":\n",
    "            #         fmtr += suffix\n",
    "            if zone_array is not None:\n",
    "                df.loc[:, use_col] = df.zval.apply(\n",
    "                    lambda x: fmtr.format(nname, use_col, x)\n",
    "                )\n",
    "            else:\n",
    "                df.loc[:, use_col] = fmtr.format(nname, use_col)\n",
    "            # todo:  Direct pars:\n",
    "            #  check that values are constant within zones and assign parval1\n",
    "\n",
    "        elif typ == \"grid\":\n",
    "            # one par for each index\n",
    "            fmtr = \"pname:{0}_ptype:gr_usecol:{1}\"\n",
    "            fmtr += \"_pstyle:{0}\".format(par_style)\n",
    "            if zone_array is not None:\n",
    "                fmtr += \"_zone:{2}_{3}\"\n",
    "            else:\n",
    "                fmtr += \"_{2}\"\n",
    "            if suffix != \"\":\n",
    "                fmtr += f\"_{suffix}\"\n",
    "            if zone_array is not None:\n",
    "                df.loc[:, use_col] = df.apply(\n",
    "                    lambda x: fmtr.format(nname, use_col, x.zval, x.idx_strs), axis=1\n",
    "                )\n",
    "            else:\n",
    "                df.loc[:, use_col] = df.idx_strs.apply(\n",
    "                    lambda x: fmtr.format(nname, use_col, x)\n",
    "                )\n",
    "            if par_style == \"d\":\n",
    "                df.loc[:, f\"parval1_{use_col}\"] = init_df.loc[:, use_col].values\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"get_tpl_or_ins_df() error: \"\n",
    "                \"unrecognized 'typ', if not 'obs', \"\n",
    "                \"should be 'constant','zone', \"\n",
    "                \"or 'grid', not '{0}'\".format(typ)\n",
    "            )\n",
    "\n",
    "        if par_style == \"d\":\n",
    "            direct_tpl_df[use_col] = (\n",
    "                df.loc[:, use_col].apply(lambda x: \"~ {0} ~\".format(x)).values\n",
    "            )\n",
    "    if par_style == \"d\":\n",
    "        return df, direct_tpl_df\n",
    "    return df\n",
    "\n",
    "\n",
    "def _get_tpl_or_ins_df(\n",
    "    dfs,\n",
    "    name,\n",
    "    index_cols,\n",
    "    typ,\n",
    "    use_cols=None,\n",
    "    suffix=\"\",\n",
    "    zone_array=None,\n",
    "    get_xy=None,\n",
    "    ij_in_idx=None,\n",
    "    xy_in_idx=None,\n",
    "    zero_based=True,\n",
    "    gpname=None,\n",
    "    par_fill_value=1.0,\n",
    "    par_style=\"m\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Private method to auto-generate parameter or obs names from tabular\n",
    "    model files (input or output) read into pandas dataframes\n",
    "    Args:\n",
    "        dfs (`pandas.DataFrame` or `list`): DataFrames (can be list of DataFrames)\n",
    "            to set up parameters or observations\n",
    "        name (`str`): Parameter name or Observation name prefix\n",
    "        index_cols (`str` or `list`): columns of dataframes to use as indicies\n",
    "        typ (`str`): 'obs' to set up observation names or,\n",
    "            'constant','zone', or 'grid' used in parname generation.\n",
    "            If `constant`, one par is set up for each `use_cols`.\n",
    "            If `zone`, one par is set up for each zone for each `use_cols`.\n",
    "            If `grid`, one par is set up for every unique index combination\n",
    "            (from `index_cols`) for each `use_cols`.\n",
    "        use_cols (`list`): Columns to parameterise. If None, pars are set up\n",
    "            for all columns apart from index cols. Not used if `typ`==`obs`.\n",
    "        suffix (`str`): Optional par name suffix. Not used if `typ`==`obs`.\n",
    "        zone_array (`np.ndarray`): Only used for paremeters (`typ` != `obs`).\n",
    "            Array defining zone divisions.\n",
    "            If not None and `par_type` is `grid` or `zone` it is expected that\n",
    "            `index_cols` provide the indicies for querying `zone_array`.\n",
    "            Therefore, array dimension should equal `len(index_cols)`.\n",
    "        get_xy (`pyemu.PstFrom` method): Can be specified to get real-world xy\n",
    "            from `index_cols` passed (to include in obs/par name)\n",
    "        ij_in_idx (`list` or `array`): defining which `index_cols` contain i,j\n",
    "        xy_in_idx (`list` or `array`): defining which `index_cols` contain x,y\n",
    "        zero_based (`boolean`): IMPORTANT - pass as False if `index_cols`\n",
    "            are NOT zero-based indicies (e.g. MODFLOW row/cols).\n",
    "            If False 1 with be subtracted from `index_cols`.=\n",
    "        par_fill_value (float): value to use as `parval1`,Default is 1.0\n",
    "\n",
    "    Returns:\n",
    "        if `typ`==`obs`: pandas.DataFrame with index strings for setting up obs\n",
    "        names when passing through to\n",
    "        pyemu.pst_utils.csv_to_ins_file(df.set_index('idx_str')\n",
    "\n",
    "        else: pandas.DataFrame with paranme and pargroup define for each `use_col`\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(dfs, pd.DataFrame):\n",
    "        dfs = [dfs]\n",
    "    if not isinstance(dfs, list):\n",
    "        dfs = list(dfs)\n",
    "\n",
    "    # work out the union of indices across all dfs\n",
    "    # order matters for obs\n",
    "    sidx = []\n",
    "    for df in dfs:\n",
    "        # avoiding df.values to prevent conversion to same type\n",
    "        didx = zip(*[df[col] for col in index_cols])\n",
    "        aidx = [i for i in didx if i not in sidx]\n",
    "        sidx.extend(aidx)\n",
    "\n",
    "    df_ti = pd.DataFrame({\"sidx\": sidx}, columns=[\"sidx\"])\n",
    "    inames, fmt = _get_index_strfmt(index_cols)\n",
    "    df_ti = _get_index_strings(df_ti, fmt, zero_based)\n",
    "    df_ti = _getxy_from_idx(df_ti, get_xy, xy_in_idx, ij_in_idx)\n",
    "\n",
    "    if typ == \"obs\":\n",
    "        return df_ti  #################### RETURN if OBS\n",
    "    df_ti = _build_parnames(\n",
    "        df_ti,\n",
    "        typ,\n",
    "        zone_array,\n",
    "        index_cols,\n",
    "        use_cols,\n",
    "        name,\n",
    "        gpname,\n",
    "        suffix,\n",
    "        par_style,\n",
    "        fill_value=par_fill_value,\n",
    "    )\n",
    "    return df_ti\n",
    "\n",
    "\n",
    "def write_array_tpl(\n",
    "    name,\n",
    "    tpl_filename,\n",
    "    suffix,\n",
    "    par_type,\n",
    "    zone_array=None,\n",
    "    gpname=None,\n",
    "    shape=None,\n",
    "    fill_value=1.0,\n",
    "    get_xy=None,\n",
    "    input_filename=None,\n",
    "    par_style=\"m\",\n",
    "):\n",
    "    \"\"\"\n",
    "    write a template file for a 2D array.\n",
    "\n",
    "     Args:\n",
    "        name (`str`): the base parameter name\n",
    "        tpl_filename (`str`): the template file to write - include path\n",
    "        suffix (`str`): suffix to append to par names\n",
    "        par_type (`str`): type of parameter\n",
    "        zone_array (`numpy.ndarray`): an array used to skip inactive cells. Values less than 1 are\n",
    "            not parameterized and are assigned a value of fill_value. Default is None.\n",
    "        gpname (`str`): pargp filed in dataframe\n",
    "        shape (`tuple`): dimensions of array to write\n",
    "        fill_value:\n",
    "        get_xy:\n",
    "        input_filename:\n",
    "        par_style (`str`): either 'd','a', or 'm'\n",
    "\n",
    "    Returns:\n",
    "        df (`pandas.DataFrame`): a dataframe with parameter information\n",
    "\n",
    "    Note:\n",
    "        This function is called by `PstFrom` programmatically\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if shape is None and zone_array is None:\n",
    "        raise Exception(\n",
    "            \"write_array_tpl() error: must pass either zone_array \" \"or shape\"\n",
    "        )\n",
    "    elif shape is not None and zone_array is not None:\n",
    "        if shape != zone_array.shape:\n",
    "            raise Exception(\n",
    "                \"write_array_tpl() error: passed \"\n",
    "                \"shape {0} != zone_array.shape {1}\".format(shape, zone_array.shape)\n",
    "            )\n",
    "    elif shape is None:\n",
    "        shape = zone_array.shape\n",
    "    if len(shape) != 2:\n",
    "        raise Exception(\n",
    "            \"write_array_tpl() error: shape '{0}' not 2D\" \"\".format(str(shape))\n",
    "        )\n",
    "\n",
    "    par_style = par_style.lower()\n",
    "    if par_style == \"d\":\n",
    "        if not os.path.exists(input_filename):\n",
    "            raise Exception(\n",
    "                \"write_array_tpl() error: couldn't find input file \"\n",
    "                + \" {0}, which is required for 'direct' par_style\".format(\n",
    "                    input_filename\n",
    "                )\n",
    "            )\n",
    "        org_arr = np.loadtxt(input_filename, ndmin=2)\n",
    "        if par_type == \"grid\":\n",
    "            pass\n",
    "        elif par_type == \"constant\":\n",
    "            _check_diff(org_arr, input_filename)\n",
    "        elif par_type == \"zone\":\n",
    "            for zval in np.unique(zone_array):\n",
    "                if zval < 1:\n",
    "                    continue\n",
    "                zone_org_arr = org_arr.copy()\n",
    "                zone_org_arr[zone_array != zval] = np.NaN\n",
    "                _check_diff(zone_org_arr, input_filename, zval)\n",
    "    elif par_style == \"m\":\n",
    "        org_arr = np.ones(shape)\n",
    "    elif par_style == \"a\":\n",
    "        org_arr = np.zeros(shape)\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"write_array_tpl() error: unrecognized 'par_style' {0} \".format(par_style)\n",
    "            + \"should be 'd','a', or 'm'\"\n",
    "        )\n",
    "\n",
    "    def constant_namer(i, j):\n",
    "        pname = \"pname:{1}_ptype:cn_pstyle:{0}\".format(par_style, name)\n",
    "        if suffix != \"\":\n",
    "            pname += \"_{0}\".format(suffix)\n",
    "        return pname\n",
    "\n",
    "    def zone_namer(i, j):\n",
    "        zval = 1\n",
    "        if zone_array is not None:\n",
    "            zval = zone_array[i, j]\n",
    "        pname = \"pname:{1}_ptype:zn_pstyle:{0}_zone:{2}\".format(\n",
    "            par_style, name, zval\n",
    "        )\n",
    "        if suffix != \"\":\n",
    "            pname += \"_{0}\".format(suffix)\n",
    "        return pname\n",
    "\n",
    "    def grid_namer(i, j):\n",
    "        pname = \"pname:{1}_ptype:gr_pstyle:{0}_i:{2}_j:{3}\".format(\n",
    "            par_style, name, i, j\n",
    "        )\n",
    "        if get_xy is not None:\n",
    "            pname += \"_x:{0:0.2f}_y:{1:0.2f}\".format(\n",
    "                *get_xy([i, j], ij_id=[0, 1]))\n",
    "        if zone_array is not None:\n",
    "            pname += \"_zone:{0}\".format(zone_array[i, j])\n",
    "        if suffix != \"\":\n",
    "            pname += \"_{0}\".format(suffix)\n",
    "        return pname\n",
    "\n",
    "    if par_type == \"constant\":\n",
    "        namer = constant_namer\n",
    "    elif par_type == \"zone\":\n",
    "        namer = zone_namer\n",
    "    elif par_type == \"grid\":\n",
    "        namer = grid_namer\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"write_array_tpl() error: unsupported par_type\"\n",
    "            \", options are 'constant', 'zone', or 'grid', not\"\n",
    "            \"'{0}'\".format(par_type)\n",
    "        )\n",
    "\n",
    "    parnme = []\n",
    "    org_par_val_dict = {}\n",
    "    xx, yy, ii, jj = [], [], [], []\n",
    "    with open(tpl_filename, \"w\") as f:\n",
    "        f.write(\"ptf ~\\n\")\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                if zone_array is not None and zone_array[i, j] < 1:\n",
    "                    pname = \" {0} \".format(fill_value)\n",
    "                else:\n",
    "                    if get_xy is not None:\n",
    "                        x, y = get_xy([i, j], ij_id=[0, 1])\n",
    "                        xx.append(x)\n",
    "                        yy.append(y)\n",
    "                    ii.append(i)\n",
    "                    jj.append(j)\n",
    "\n",
    "                    pname = namer(i, j)\n",
    "                    parnme.append(pname)\n",
    "                    org_par_val_dict[pname] = org_arr[i, j]\n",
    "                    pname = \" ~   {0}    ~\".format(pname)\n",
    "\n",
    "                f.write(pname)\n",
    "            f.write(\"\\n\")\n",
    "    df = pd.DataFrame({\"parnme\": parnme}, index=parnme)\n",
    "    df.loc[:, \"parval1\"] = df.parnme.apply(lambda x: org_par_val_dict[x])\n",
    "    if par_type == \"grid\":\n",
    "        df.loc[:, \"i\"] = ii\n",
    "        df.loc[:, \"j\"] = jj\n",
    "        if get_xy is not None:\n",
    "            df.loc[:, \"x\"] = xx\n",
    "            df.loc[:, \"y\"] = yy\n",
    "    if gpname is None:\n",
    "        gpname = name\n",
    "    df.loc[:, \"pargp\"] = \"{0}_{1}\".format(gpname, suffix.replace(\"_\", \"\")).rstrip(\"_\")\n",
    "    df.loc[:, \"tpl_filename\"] = tpl_filename\n",
    "    df.loc[:, \"input_filename\"] = input_filename\n",
    "    if input_filename is not None:\n",
    "        if par_style in [\"m\", \"d\"]:\n",
    "            arr = np.ones(shape)\n",
    "        elif par_style == \"a\":\n",
    "            arr = np.zeros(shape)\n",
    "        np.savetxt(input_filename, arr, fmt=\"%2.1f\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _check_diff(org_arr, input_filename, zval=None):\n",
    "    percent_diff = 100.0 * np.abs(\n",
    "        (np.nanmax(org_arr) - np.nanmin(org_arr)) / np.nanmean(org_arr)\n",
    "    )\n",
    "\n",
    "    if percent_diff > DIRECT_PAR_PERCENT_DIFF_TOL:\n",
    "        message = \"_check_diff() error: direct par for file '{0}'\".format(\n",
    "            input_filename\n",
    "        ) + \"exceeds tolerance for percent difference: {0} > {1}\".format(\n",
    "            percent_diff, DIRECT_PAR_PERCENT_DIFF_TOL\n",
    "        )\n",
    "        if zval is not None:\n",
    "            message += \" in zone {0}\".format(zval)\n",
    "        raise Exception(message)\n",
    "\n",
    "\n",
    "def get_filepath(folder, filename):\n",
    "    \"\"\"Return a path to a file within a folder,\n",
    "    without repeating the folder in the output path,\n",
    "    if the input filename (path) already contains the folder.\"\"\"\n",
    "    filename = Path(filename)\n",
    "    folder = Path(folder)\n",
    "    if folder not in filename.parents:\n",
    "        filename = folder / filename\n",
    "    return filename\n",
    "\n",
    "\n",
    "def get_relative_filepath(folder, filename):\n",
    "    \"\"\"Like :func:`~pyemu.utils.pst_from.get_filepath`, except\n",
    "    return path for filename relative to folder.\n",
    "    \"\"\"\n",
    "    return get_filepath(folder, filename).relative_to(folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
