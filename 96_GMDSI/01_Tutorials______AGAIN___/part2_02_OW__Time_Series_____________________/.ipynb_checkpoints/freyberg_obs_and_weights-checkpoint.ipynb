{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "import matplotlib\n",
    "\n",
    "import sys\n",
    "import pyemu\n",
    "import flopy\n",
    "# assert \"dependencies\" in flopy.__file__\n",
    "# assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = os.path.join('freyberg6_template')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "you need to run the '/part2_01_pstfrom_pest_setup/freyberg_pstfrom_pest_setup.ipynb' notebook",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mrmtree(t_d)\n\u001b[0;32m      5\u001b[0m org_t_d \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart2_01_pstfrom_pest_setup\u001b[39m\u001b[38;5;124m\"\u001b[39m,t_d)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(org_t_d),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou need to run the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/part2_01_pstfrom_pest_setup/freyberg_pstfrom_pest_setup.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m notebook\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing files at \u001b[39m\u001b[38;5;124m\"\u001b[39m,org_t_d)\n\u001b[0;32m      8\u001b[0m shutil\u001b[38;5;241m.\u001b[39mcopytree(org_t_d,t_d)\n",
      "\u001b[1;31mAssertionError\u001b[0m: you need to run the '/part2_01_pstfrom_pest_setup/freyberg_pstfrom_pest_setup.ipynb' notebook"
     ]
    }
   ],
   "source": [
    "# use the conveninece function to get the pre-preprepared PEST dataset;\n",
    "# this is the same dataset constructed in the \"pstfrom\" tutorial\n",
    "if os.path.exists(t_d):\n",
    "        shutil.rmtree(t_d)\n",
    "org_t_d = os.path.join(\"..\",\"part2_01_pstfrom_pest_setup\",t_d)\n",
    "assert os.path.exists(org_t_d),\"you need to run the '/part2_01_pstfrom_pest_setup/freyberg_pstfrom_pest_setup.ipynb' notebook\"\n",
    "print(\"using files at \",org_t_d)\n",
    "shutil.copytree(org_t_d,t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the `Pst` control file we constructed during the \"pstfrom\" tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_file = \"freyberg_mf6.pst\"\n",
    "pst = pyemu.Pst(os.path.join(t_d, pst_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we constructed the PEST dataset (in the \"pstfrom pest setup\" tutorial) we simply identified what model outputs we wanted PEST to \"observe\". In doing so, `pyemu.PstFrom` assigned observation target values that it found in the existing model output files. (Which conveniently allowed us to test whether out PEST setup was working correctly). All observation weights were assigned a default value of 1.0. \n",
    "\n",
    "As a reminder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we need to do several things:\n",
    " - replace observation target values (the `obsval` column) with corresponding values from \"measured data\";\n",
    " - assign meaningful weights to history matching target observations (the `weight` column);\n",
    " - assign zero weight to observations that should not affect history matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off with the basics. First set all weights to zero. We will then go through and assign meaningful weights only to relevant target observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for nonzero weights\n",
    "obs.weight.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign all weight zero\n",
    "obs.loc[:, 'weight'] = 0\n",
    "\n",
    "# check for non zero weights\n",
    "obs.weight.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measured Data\n",
    "\n",
    "In most data assimilation contexts you will have some relevant measured data (e.g. water levels, river flow rates, etc.) which correspond to simulated model outputs. These will probably not coincide exactly with your model outputs. Are the wells at the same coordinate as the center of the model cell? Do measurement times line up nicely with model output times? Doubt it. And if they do, are single measurements that match model output times biased? And so on... \n",
    "\n",
    "A modeller needs to ensure that the observation values assigned in the PEST control file are aligned with simulated model outputs. This will usually require some case-specific pre-processing. Here we are going to demonstrate __an example__ - but remember, every case is different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's access our dataset of \"measured\" observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_csv = os.path.join('..', '..', 'models', 'daily_freyberg_mf6_truth',\"obs_data.csv\")\n",
    "assert os.path.exists(obs_csv)\n",
    "obs_data = pd.read_csv(obs_csv)\n",
    "obs_data.set_index('site', inplace=True)\n",
    "obs_data.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have measured data at daily intervals. But our model simulates monthly stress periods. So what observation value do we use? \n",
    "\n",
    "One option is to simply sample measured values from the data closest to our simulated output. The next cell does this, with a few checks along the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just pick the nearest to the sp end\n",
    "model_times = pst.observation_data.time.dropna().astype(float).unique()\n",
    "# get the list of osb names for which we have data\n",
    "obs_sites =  obs_data.index.unique().tolist()\n",
    "\n",
    "# restructure the obsevration data \n",
    "es_obs_data = []\n",
    "for site in obs_sites:\n",
    "    site_obs_data = obs_data.loc[site,:].copy()\n",
    "    if isinstance(site_obs_data, pd.Series):\n",
    "        site_obs_data.loc[\"site\"] = site_obs_data.index.values\n",
    "    elif isinstance(site_obs_data, pd.DataFrame):\n",
    "        site_obs_data.loc[:,\"site\"] = site_obs_data.index.values\n",
    "        site_obs_data.index = site_obs_data.time\n",
    "        site_obs_data = site_obs_data.reindex(model_times,method=\"nearest\")\n",
    "\n",
    "    if site_obs_data.shape != site_obs_data.dropna().shape:\n",
    "        print(\"broke\",site)\n",
    "    es_obs_data.append(site_obs_data)\n",
    "es_obs_data = pd.concat(es_obs_data,axis=0,ignore_index=True)\n",
    "es_obs_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right then...let's plot our down-sampled measurement data and compare it to the original high-frequency time series.\n",
    "\n",
    "The next cell generates plots for each time series of measured data. Blue lines are the original high-frequency data. The marked red line is the down-sampled data. What do you think? Does sampling to the \"closest date\" capture the behaviour of the time series? Doesn't look too good...It does not seem to capture the general trend very well.\n",
    "\n",
    "Let's try something else instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in obs_sites:\n",
    "    #print(site)\n",
    "    site_obs_data = obs_data.loc[site,:]\n",
    "    es_site_obs_data = es_obs_data.loc[es_obs_data.site==site,:].copy()\n",
    "    es_site_obs_data.sort_values(by=\"time\",inplace=True)\n",
    "    #print(site,site_obs_data.shape)\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    ax.plot(site_obs_data.time,site_obs_data.value,\"b-\",lw=0.5)\n",
    "    #ax.plot(es_site_obs_data.datetime,es_site_obs_data.value,'r-',lw=2)\n",
    "    ax.plot(es_site_obs_data.time,es_site_obs_data.value,'r-',lw=1,marker='.',ms=10)\n",
    "    ax.set_title(site)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's try using a moving-average instead. Effectively this is applying a low-pass filter to the time-series, smooting out some of the spiky noise. \n",
    "\n",
    "The next cell re-samples the data and then plots it. Measured data sampled using a low-pass filter is shown by the marked green line. What do you think? Better? It certainly does a better job at capturing the trends in the original data! Let's go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_obs_data = {}\n",
    "for site in obs_sites:\n",
    "    #print(site)\n",
    "    site_obs_data = obs_data.loc[site,:].copy()\n",
    "    if isinstance(site_obs_data, pd.Series):\n",
    "        site_obs_data.loc[\"site\"] = site_obs_data.index.values\n",
    "    if isinstance(site_obs_data, pd.DataFrame):\n",
    "        site_obs_data.loc[:,\"site\"] = site_obs_data.index.values\n",
    "        site_obs_data.index = site_obs_data.time\n",
    "        sm = site_obs_data.value.rolling(window=20,center=True,min_periods=1).mean()\n",
    "        sm_site_obs_data = sm.reindex(model_times,method=\"nearest\")\n",
    "    #ess_obs_data.append(pd.DataFrame9sm_site_obs_data)\n",
    "    ess_obs_data[site] = sm_site_obs_data\n",
    "    \n",
    "    es_site_obs_data = es_obs_data.loc[es_obs_data.site==site,:].copy()\n",
    "    es_site_obs_data.sort_values(by=\"time\",inplace=True)\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    ax.plot(site_obs_data.time,site_obs_data.value,\"b-\",lw=0.25)\n",
    "    ax.plot(es_site_obs_data.time,es_site_obs_data.value,'r-',lw=1,marker='.',ms=10)\n",
    "    ax.plot(sm_site_obs_data.index,sm_site_obs_data.values,'g-',lw=0.5,marker='.',ms=10)\n",
    "    ax.set_title(site)\n",
    "plt.show()\n",
    "ess_obs_data = pd.DataFrame(ess_obs_data)\n",
    "ess_obs_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Target Observation Values in the Control File\n",
    "\n",
    "Right then - so, these are our smoothed-sampled observation values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_obs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are confronted with the task of getting these _processed_ measured observation values into the `Pst` control file. Once again, how you do this will end up being somewhat case-specific and will depend on how your obsveration names were constructed. For example, in our case we can use the following function (we made it a function because we are going to repeat it a few times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pst_obsvals(obs_names, obs_data):\n",
    "    \"\"\"obs_names: list of selected obs names\n",
    "       obs_data: dataframe with obs values to use in pst\"\"\"\n",
    "    # for checking\n",
    "    org_nnzobs = pst.nnz_obs\n",
    "    # get list of times for obs name sufixes\n",
    "    time_str = obs_data.index.map(lambda x: f\"time:{x}\").values\n",
    "    # empyt list to keep track of misssing observation names\n",
    "    missing=[]\n",
    "    for col in obs_data.columns:\n",
    "        # get obs list sufix for each column of data\n",
    "        obs_sufix = col.lower()+\"_\"+time_str\n",
    "        for string, oval, time in zip(obs_sufix,obs_data.loc[:,col].values, obs_data.index.values):\n",
    "                \n",
    "                if not any(string in obsnme for obsnme in obs_names):\n",
    "                    if string.startswith(\"trgw-2\"):\n",
    "                        pass\n",
    "                    else:\n",
    "                        missing.append(string)\n",
    "                # if not, then update the pst.observation_data\n",
    "                else:\n",
    "                    # get a list of obsnames\n",
    "                    obsnme = [ks for ks in obs_names if string in ks] \n",
    "                    assert len(obsnme) == 1,string\n",
    "                    obsnme = obsnme[0]\n",
    "                    # assign the obsvals\n",
    "                    obs.loc[obsnme,\"obsval\"] = oval\n",
    "                    # assign a generic weight\n",
    "                    if time > 3652.5 and time <=4018.5:\n",
    "                        obs.loc[obsnme,\"weight\"] = 1.0\n",
    "    # checks\n",
    "    #if (pst.nnz_obs-org_nnzobs)!=0:\n",
    "    #    assert (pst.nnz_obs-org_nnzobs)==obs_data.count().sum()\n",
    "    if len(missing)==0:\n",
    "        print('All good.')\n",
    "        print('Number of new nonzero obs:' ,pst.nnz_obs-org_nnzobs) \n",
    "        print('Number of nonzero obs:' ,pst.nnz_obs)  \n",
    "    else:\n",
    "        raise ValueError('The following obs are missing:\\n',missing)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.nnz_obs_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subselection of observaton names; this is because several groups share the same obs name sufix\n",
    "obs_names = obs.loc[obs.oname.isin(['hds', 'sfr']), 'obsnme']\n",
    "\n",
    "# run the function\n",
    "update_pst_obsvals(obs_names, ess_obs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.nnz_obs_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.oname.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that has sorted out the absolute observation groups. But remember the 'sfrtd' and 'hdstd' observation groups? Yeah thats right, we also added in a bunch of other \"secondary observations\" (the time difference between obsevrations) as well as postprocessing functions to get them from model outputs. We need to get target values for these observations into our control file as well!\n",
    "\n",
    "Let's start by calculating the secondary values from the absolute measured values. In our case, the easiest is to populate the model output files with measured values and then call our postprocessing function.\n",
    "\n",
    "Let's first read in the SFR model output file, just so we can see what is happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_sfr = pd.read_csv(os.path.join(t_d,\"sfr.csv\"),\n",
    "                    index_col=0)\n",
    "\n",
    "obs_sfr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now update the model output csv files with the smooth-sampled measured values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_obs_csv(obs_csv):\n",
    "    obsdf = pd.read_csv(obs_csv, index_col=0)\n",
    "    check = obsdf.copy()\n",
    "    # update values in reelvant cols\n",
    "    for col in ess_obs_data.columns:\n",
    "        if col in obsdf.columns:\n",
    "            obsdf.loc[:,col] = ess_obs_data.loc[:,col]\n",
    "    # keep only measured data columns; helps for vdiff and tdiff obs later on\n",
    "    #obsdf = obsdf.loc[:,[col for col in ess_obs_data.columns if col in obsdf.columns]]\n",
    "    # rewrite the model output file\n",
    "    obsdf.to_csv(obs_csv)\n",
    "    # check \n",
    "    obsdf = pd.read_csv(obs_csv, index_col=0)\n",
    "    assert (obsdf.index==check.index).all()\n",
    "    return obsdf\n",
    "\n",
    "# update the SFR obs csv\n",
    "obs_srf = update_obs_csv(os.path.join(t_d,\"sfr.csv\"))\n",
    "# update the heads obs csv\n",
    "obs_hds = update_obs_csv(os.path.join(t_d,\"heads.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK...now we can run the postprocessing function to update the \"tdiff\" model output csv's. Copy across the `helpers.py` we used during the `PstFrom` tutorial. Then import it and run the `process_secondary_obs()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy2(os.path.join('..','part2_01_pstfrom_pest_setup', 'helpers.py'),\n",
    "            os.path.join('helpers.py'))\n",
    "\n",
    "import helpers\n",
    "helpers.process_secondary_obs(ws=t_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the oname column in the pst.observation_data provides a useful way to select observations in this case\n",
    "obs.oname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_nnzobs = pst.nnz_obs\n",
    "    #if (pst.nnz_obs-org_nnzobs)!=0:\n",
    "    #    assert (pst.nnz_obs-org_nnzobs)==obs_data.count().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of nonzero obs:', pst.nnz_obs)\n",
    "\n",
    "diff_obsdict = {'sfrtd': \"sfr.tdiff.csv\", \n",
    "                'hdstd': \"heads.tdiff.csv\",\n",
    "                }\n",
    "\n",
    "for keys, value in diff_obsdict.items():\n",
    "    print(keys)\n",
    "    # get subselct of obs names\n",
    "    obs_names = obs.loc[obs.oname.isin([keys]), 'obsnme']\n",
    "    # get df\n",
    "    obs_csv = pd.read_csv(os.path.join(t_d,value),index_col=0)\n",
    "    # specify cols to use; make use of info recorded in pst.observation_data to only select cols with measured data\n",
    "    usecols = list(set((map(str.upper, obs.loc[pst.nnz_obs_names,'usecol'].unique()))) & set(obs_csv.columns.tolist()))\n",
    "    obs_csv = obs_csv.loc[:, usecols]\n",
    "    # for checking\n",
    "    org_nnz_obs_names = pst.nnz_obs_names\n",
    "    # run the function\n",
    "    update_pst_obsvals(obs_names,\n",
    "                        obs_csv)\n",
    "    # verify num of new nnz obs\n",
    "    print(pst.nnz_obs)\n",
    "    print(len(org_nnz_obs_names))\n",
    "    print(len(usecols))\n",
    "    assert (pst.nnz_obs-len(org_nnz_obs_names))==12*len(usecols), [i for i in pst.nnz_obs_names if i not in org_nnz_obs_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.nnz_obs_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.nnz_obs_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell does some sneaky things in the background to populate `obsvals` for forecast observations just so that we can keep track of the truth. In real-world applications you might assign values that reflect decision-criteria (such as limits at which \"bad things\" happen, for example) simply as a convenience. For the purposes of history matching, these values have no impact because they are assigned zero weight. They can play a role in specifying constraints when undertaking optimisation problems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.loc[pst.forecast_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbd.prep_forecasts(pst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.loc[pst.forecast_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Weights\n",
    "\n",
    "Of all the issues that we have seen over the years, none is greater than (in)appropriate weighting strategies.  It is a critical and fundamental component of any inverse problem, but is especially important in settings where the model is imperfect simulator and the observation data are noisy and there are diverse types of data.  Goundwater modeling anyone? \n",
    "\n",
    "In essence the weights will change the shape of the objective function surface in parameter space, moving the minimum around and altering the path to the minimum (this can be seen visually in the response surface notebooks).  Given the important role weights play in the outcome of a history-matching/data assimilation analysis, rarely is a weighting strategy \"one and done\", instead it is continuously revisited during a modeling analysis, based on what happened during the previous history-matching attempt.  \n",
    "\n",
    "We are going to start off by taking a look at our current objective function value and the relative contributions from the various observation groups - these relative contributions are a function of the residuals and weights in each group. Recall that this is the objective function value with **initial parameter values** and the default observations weights.\n",
    "\n",
    "First off, we need to get PEST to run the model once so that the objective function can be calculated. Let's do that now. Start by reading the control file and checking that NOPTMAX is set to zero:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check noptmax\n",
    "pst.control_data.noptmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You got a zero? Alrighty then! Let's write the uprated control file and run PEST again and see what that has done to our Phi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(t_d,pst_file),version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run(\"pestpp-ies.exe {0}\".format(pst_file),cwd=t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to reload the `Pst` control file so that the residuals are updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(t_d, pst_file))\n",
    "pst.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeepers - that's large! Before we race off and start running PEST to lower it we should compare simualted and measured values and take a look at the components of Phi. \n",
    "\n",
    "Let's start with taking a closer look. The `pst.phi_components` attribute returns a dictionary of the observation group names and their contribution to the overal value of Phi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.phi_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unfortunately, in this case we have too many observation groups to easily display (we assigned each individual time series to its own observation group; this is a default setting in `pyemu.PstFrom`). \n",
    "\n",
    "So let's use `Pandas` to help us sumamrize this information (note: `pyemu.plot_utils.res_phi_pie()` does the same thing, but it looks a bit ugly because of the large number of observation groups). To make it easier, we are going to just look at the nonzero observation groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnz_phi_components = {k:pst.phi_components[k] for k in pst.nnz_obs_groups} # that's a dictionary comprehension there y'all\n",
    "nnz_phi_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And while we are at it, plot these in a pie chart. \n",
    "\n",
    "If you wish, try displaying this with `pyemu.plot_utils.res_phi_pie()` instead. Because of the large number of columns it's not going to be pretty, but it gets the job done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phicomp = pd.Series(nnz_phi_components)\n",
    "plt.pie(phicomp, labels=phicomp.index.values);\n",
    "#pyemu.plot_utils.res_phi_pie(pst,);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that is certainly not ideal - phi is dominated by the SFR observation groups. Why? Because the magnitude of these observation values are much larger than groundwater-level based observations, so we can expect the residuals in SFR observations to be yuge compared to groundwater level residuals...and we assigned the same weight to all of them...\n",
    "\n",
    "Now we have some choices to make.  In many settings, there are certain observations (or observation groups) that are of increased importance, whether its for predictive reasons (like some data are more similar to the predictive outputs from the modeling) or political - \"show me obs vs sim for well XXX\"...If this is the case, then it is probably important to give those observations a larger portion of the composite objective function so that the results of the history matching better reproduce those important observations.  \n",
    "\n",
    "In this set of notebooks, we will use another very common approach: give all observations groups an equal portion of the composite objective function.  This basically says \"all of the different observation groups are important, so do your best with all of them\"\n",
    "\n",
    "The `Pst.adjust_weights()` method provides a mechanism to fine tune observation weights according to their contribution to the objective function. (*Side note: the PWTADJ1 utility from the PEST-suite automates this same process of \"weighting for visibility\".*) \n",
    "\n",
    "We start by creating a dictionary of non-zero weighted observation group names and their respective contributions to the objective function. Herein, we will use the existing composite phi value as the target composite phi...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of group names and weights\n",
    "contrib_per_group = pst.phi / float(len(pst.nnz_obs_groups))\n",
    "balanced_groups = {grp:contrib_per_group for grp in pst.nnz_obs_groups}\n",
    "balanced_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all non-zero weighted groups have a contribution of 100.0\n",
    "pst.adjust_weights(obsgrp_dict=balanced_groups,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how that has affected the contributions to Phi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "phicomp = pd.Series({k:pst.phi_components[k] for k in pst.nnz_obs_groups})\n",
    "plt.pie(phicomp, labels=phicomp.index.values);\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better! Now each observation group contributes equally to the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell adds in a column to the `pst.observation_data` for checking purposes in subsequent tutorials. In practice, when you have lots of model outputs treated as \"obserations\" in the pest control file, setting a flag to indicate exactly which observation quantities correspond to actual real-world information can be important for tracking things through your workflow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.loc[pst.nnz_obs_names,'observed'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Observation Weights and Measurement Noise\n",
    "\n",
    "Let's have a look at what weight values were assigned to our observation groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "for group in pst.nnz_obs_groups:\n",
    "    print(group,obs.loc[obs.obgnme==group,\"weight\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, some variability there, and, as expected, the sfr flowout observations have been given a very low weight and the groundwater level obs have been given a very high weight - this is simply to overcome the difference in magnitudes between these two different data types.  All good...or is it?\n",
    "\n",
    "In standard deterministic parameter estimation, only the relative difference between the weights matters, so we are fine there...but in uncertainty analysis, we often want to account for the contribution from measurement noise and we havent told any of the pest++ tools not to use the inverse of the weights to approximate measurement noise, and this is a problem because those weights we assigned have no relation to measurement noise!  This can cause massive problems later, especially is you are using explicit noise realizations in uncertainty analysis - Imagine how much SFR flow noise is implied by that tiny weight?  It's easy to see how negative SFR flow noise values might be drawn with that small of a weight (high of a standard deviation) #badtimes.   \n",
    "\n",
    "So what can we do?  Well there are options.  An easy way is to simply supply a \"standard_deviation\" column in the `pst.observation_data` dataframe that will cause these values to be used to represent measurement noise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.loc[:,\"standard_deviation\"] = np.nan\n",
    "hds_obs = [o for o in pst.nnz_obs_names if \"oname:hds_\" in o]\n",
    "assert len(hds_obs) > 0\n",
    "obs.loc[hds_obs,\"standard_deviation\"] = 0.5\n",
    "hdstd_obs = [o for o in pst.nnz_obs_names if \"oname:hdstd_\" in o]\n",
    "assert len(hdstd_obs) > 0\n",
    "obs.loc[hdstd_obs,\"standard_deviation\"] = 0.01\n",
    "\n",
    "sfr_obs = [o for o in pst.nnz_obs_names if \"oname:sfr_\" in o]\n",
    "assert len(sfr_obs) > 0\n",
    "# here we will used noise that is a function of the observed flow value so that \n",
    "# when flow is high, noise is high.\n",
    "obs.loc[sfr_obs,\"standard_deviation\"] = abs(obs.loc[sfr_obs,\"obsval\"] * 0.1)\n",
    "sfrtd_obs = [o for o in pst.nnz_obs_names if \"oname:sfrtd_\" in o]\n",
    "assert len(sfrtd_obs) > 0\n",
    "obs.loc[sfrtd_obs,\"standard_deviation\"] = abs(obs.loc[sfrtd_obs,\"obsval\"] * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(t_d,pst_file),version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some **caution** is required here. Observation weights and how these pertain to history-matching *versus* how they pertain to generating an observation ensemble for use with `pestpp-ies` or FOSM is a frequent source of confusion.\n",
    "\n",
    " - when **history-matching**, observation weights listed in the control file determine their contribution to the objective function, and therefore to the parameter estimation process. Here, observation weights may be assigned to reflect observation uncertainty, the balance required for equal \"visibility\", or other modeller-defined (and perhaps subjective...) measures of observation worth.  \n",
    " - when undertaking **uncertainty analysis**, weights should reflect ideally the inverse of observation error (or the standard deviation of measurement noise). Keep this in mind when using `pestpp-glm` or `pestpp-ies`. If the observations in the control file do not have error-based weighting then (1) care is required if using `pestpp-glm` for FOSM and (2) make sure to provide an adequately prepared observation ensemble to `pestpp-ies`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made it this far? Congratulations! Have a cookie :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
