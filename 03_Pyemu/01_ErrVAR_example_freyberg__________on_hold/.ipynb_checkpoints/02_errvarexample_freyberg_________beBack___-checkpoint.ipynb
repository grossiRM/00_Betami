{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ` PAR_ID  `        __Advanced__ __subject__ `   for the time being     `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Rectangle as rect\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   could not parse xul in E:\\15_REPOS\\00_BETAMI\\03_pyEMU\\01_ErrVAR_example_freyberg\\Freyberg\\extra_crispy\\freyberg.nam\n"
     ]
    }
   ],
   "source": [
    "import flopy\n",
    "model_ws = os.path.join(\"Freyberg\",\"extra_crispy\")\n",
    "ml = flopy.modflow.Modflow.load(\"freyberg.nam\",model_ws=model_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyemu\n",
    "sr = pyemu.helpers.SpatialReference.from_namfile(os.path.join(model_ws, ml.namefile), delc=ml.dis.delc, delr=ml.dis.delr)\n",
    "ml.modelgrid.set_coord_info(xoff=sr.xll,yoff=sr.yll,angrot=sr.rotation,crs=sr.proj4_str,merge_coord_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations, parameters found in jacobian: (704, 747)\n"
     ]
    }
   ],
   "source": [
    "jco = os.path.join(\"Freyberg\",\"freyberg.jcb\")\n",
    "pst = pyemu.Pst(jco.replace(\"jcb\",\"pst\"))\n",
    "omitted = [pname for pname in pst.par_names if \\\n",
    "           pname.startswith(\"wf\") or pname.startswith(\"rch\")]\n",
    "forecasts = pst.pestpp_options[\"forecasts\"].split(',')\n",
    "la = pyemu.ErrVar(jco=jco,verbose=\"errvar_freyberg.log\",\n",
    "                  omitted_parameters=omitted)\n",
    "print(\"observations, parameters found in jacobian:\",la.jco.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Parameter__ __identifiability__  `    stoped here     `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = la.qhalfx.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "figure = plt.figure(figsize=(10, 5))\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(s.x)\n",
    "ax.set_title(\"singular spectrum\")\n",
    "ax.set_ylabel(\"power\")\n",
    "ax.set_xlabel(\"singular value\")\n",
    "ax.set_xlim(0,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the singular spectrum decays rapidly (not uncommon) and that we can really only support about 12 right singular vectors even though we have 700+ parameters in the inverse problem.  \n",
    "\n",
    "Let's get the identifiability dataframe at 12 singular vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the method is passed the number of singular vectors to include in V_1\n",
    "ident_df = la.get_identifiability_dataframe(12) \n",
    "ident_df.sort_values(by=\"ident\",ascending=False).iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the indentifiability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ident_df.sort_values(by=\"ident\",ascending=False).iloc[0:20].\\\n",
    "     loc[:,\"ident\"].plot(kind=\"bar\",figsize=(10,10))\n",
    "ax.set_ylabel(\"identifiability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast error variance \n",
    "\n",
    "Now let's explore the error variance of the forecasts we are interested in.  We will use an extended version of the forecast error variance equation:   \n",
    "\n",
    "$\\sigma_{s - \\hat{s}}^2 = \\underbrace{\\textbf{y}_i^T({\\bf{I}} - {\\textbf{R}})\\boldsymbol{\\Sigma}_{{\\boldsymbol{\\theta}}_i}({\\textbf{I}} - {\\textbf{R}})^T\\textbf{y}_i}_{1} + \\underbrace{{\\textbf{y}}_i^T{\\bf{G}}\\boldsymbol{\\Sigma}_{\\mathbf{\\epsilon}}{\\textbf{G}}^T{\\textbf{y}}_i}_{2} + \\underbrace{{\\bf{p}}\\boldsymbol{\\Sigma}_{{\\boldsymbol{\\theta}}_o}{\\bf{p}}^T}_{3}$\n",
    "\n",
    "Where term 1 is the null-space contribution, term 2 is the solution space contribution and term 3 is the model error term (the penalty for not adjusting uncertain parameters).  Remember the well flux and recharge parameters that we marked as omitted?  The consequences of that action can now be explicitly evaluated.  See Moore and Doherty (2005) and White and other (2014) for more explanation of these terms.  Note that if you don't have any `omitted_parameters`, the only terms 1 and 2 contribute to error variance\n",
    "\n",
    "First we need to create a list (or numpy ndarray) of the singular values we want to test.  Since we have 12 data, we only need to test up to $13$ singular values because that is where the action is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vals = np.arange(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ErrVar` derived type exposes a method to get a multi-index pandas dataframe with each of the terms of the error variance equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errvar_df = la.get_errvar_dataframe(sing_vals)\n",
    "errvar_df.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errvar_df[[\"first\"]].to_latex(\"sw_gw_0.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the error variance components for each forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\"first\": 'g', \"second\": 'b', \"third\": 'c'}\n",
    "max_idx = 19\n",
    "idx = sing_vals[:max_idx]\n",
    "for ipred, pred in enumerate(forecasts):\n",
    "    pred = pred.lower()\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_title(pred)\n",
    "    first = errvar_df[(\"first\", pred)][:max_idx]\n",
    "    second = errvar_df[(\"second\", pred)][:max_idx]\n",
    "    third = errvar_df[(\"third\", pred)][:max_idx]\n",
    "    ax.bar(idx, first, width=1.0, edgecolor=\"none\", \n",
    "           facecolor=colors[\"first\"], label=\"first\",bottom=0.0)\n",
    "    ax.bar(idx, second, width=1.0, edgecolor=\"none\", \n",
    "           facecolor=colors[\"second\"], label=\"second\", bottom=first)\n",
    "    ax.bar(idx, third, width=1.0, edgecolor=\"none\", \n",
    "           facecolor=colors[\"third\"], label=\"third\", \n",
    "           bottom=second+first)\n",
    "    ax.set_xlim(-1,max_idx+1)\n",
    "    ax.set_xticks(idx+0.5)\n",
    "    ax.set_xticklabels(idx)\n",
    "    #if ipred == 2:\n",
    "    ax.set_xlabel(\"singular value\")\n",
    "    ax.set_ylabel(\"error variance\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the trade off between getting a good fit to push down the null-space (1st) term and the penalty for overfitting (the rise of the solution space (2nd) term)).  The sum of the first two terms in the \"apparent\" error variance (e.g. the uncertainty that standard analyses would yield) without considering the contribution from the omitted parameters.  You can verify this by checking prior uncertainty from the Schur's complement notebook against the zero singular value result using only terms 1 and 2. Note that the top of the green bar is the limit of traditional uncertainty/error variance analysis: accounting for parameter and observation\n",
    "\n",
    "We also see the added penalty for not adjusting the well flux and recharge parameters.  For the water level at the end of the calibration period forecast (``or28c05_0``), the fact the we have left parameters out doesn't matter - the parameter compensation associated with fixing uncertain model inputs can be \"calibrated out\" beyond 2 singular values.  For the water level forecast during forecast period (``or28c05_1``), the penalty for fixed parameters persists -it s nearly constant over the range of singular values.  \n",
    "\n",
    "For ``sw_gw_0``, the situation is much worse: not only are we greatly underestimating uncertainty by omitting parameters, worse, calibration increases the uncertainty for this forecast because the adjustable parameters are compensating for the omitted, uncertaint parameters in ways that are damanaging to the forecast. \n",
    "\n",
    "For the forecast period sw-gw exchange (``sw_gw_1``), calibration doesn't help or hurt - this forecast depend entirely on null space parameter components.  But treating the recharge and well pumpage as \"fixed\" (omitted) results in greatly underestimated uncertainty.     \n",
    "\n",
    "Let's check the ```errvar``` results against the results from ```schur```. This is simple with ```pyemu```, we simply  cast the ```errvar``` type to a ```schur``` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schur = la.get(astype=pyemu.Schur)\n",
    "schur_prior = schur.prior_forecast\n",
    "schur_post = schur.posterior_forecast\n",
    "print(\"{0:10s} {1:>12s} {2:>12s} {3:>12s} {4:>12s}\"\n",
    "      .format(\"forecast\",\"errvar prior\",\"errvar min\",\n",
    "              \"schur prior\", \"schur post\"))\n",
    "for ipred, pred in enumerate(forecasts):\n",
    "    first = errvar_df[(\"first\", pred)][:max_idx]\n",
    "    second = errvar_df[(\"second\", pred)][:max_idx]  \n",
    "    min_ev = np.min(first + second)\n",
    "    prior_ev = first[0] + second[0]\n",
    "    prior_sh = schur_prior[pred]\n",
    "    post_sh = schur_post[pred]\n",
    "    print(\"{0:12s} {1:12.6f} {2:12.6f} {3:12.6} {4:12.6f}\"\n",
    "          .format(pred,prior_ev,min_ev,prior_sh,post_sh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the prior from ```schur``` class matches the two-term ```errvar``` result at zero singular values.  We also see, as expected, the posterior from ```schur``` is slightly lower than the two-term ```errvar``` result.  This shows us that the \"apparent\" uncertainty in these predictions, as found through application of Bayes equation, is being under estimated because if the ill effects of the omitted parameters."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
